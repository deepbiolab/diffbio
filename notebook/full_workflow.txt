# å®Œæ•´ç«¯åˆ°ç«¯ä»£ç ï¼šåŸºäºEmbeddingçš„å…‹éš†é—´è¿ç§»å­¦ä¹ ç³»ç»Ÿ

æˆ‘å°†æä¾›ä¸€ä¸ªå®Œæ•´çš„ã€å¯ç›´æ¥è¿è¡Œçš„ä»£ç åŒ…ï¼ŒåŒ…å«3ä¸ªæ ¸å¿ƒæ¨¡å—å’Œ1ä¸ªå®Œæ•´ç¤ºä¾‹ã€‚

## ğŸ“¦ æ–‡ä»¶ç»“æ„

```
embedding_transfer_system/
â”œâ”€â”€ embedding_analyzer.py          # æ¨¡å—1: Embeddingæå–ä¸åˆ†æ
â”œâ”€â”€ embedding_regressor.py         # æ¨¡å—2: å°‘æ ·æœ¬å›å½’å»ºæ¨¡
â”œâ”€â”€ embedding_optimizer.py         # æ¨¡å—3: æœªçŸ¥é…æ–¹ä¼˜åŒ–ï¼ˆæ–°ï¼‰
â”œâ”€â”€ example_complete_workflow.py   # å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ requirements.txt               # ä¾èµ–åŒ…
```

---

## ğŸ“„ æ–‡ä»¶1: `embedding_analyzer.py`

```python
"""
embedding_analyzer.py
TabPFN Embeddingæå–ä¸å…‹éš†ç›¸ä¼¼æ€§åˆ†æ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åœ¨æºå…‹éš†ä¸Šè®­ç»ƒTabPFN
2. æå–æº/ç›®æ ‡å…‹éš†çš„embeddings
3. è®¡ç®—å¯è¿ç§»æ€§æŒ‡æ ‡
4. å¯è§†åŒ–embeddingç©ºé—´
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Dict, List, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

# TabPFN imports
from tabpfn import TabPFNRegressor
from tabpfn_extensions.embedding import TabPFNEmbedding


class CloneEmbeddingAnalyzer:
    """æå–å’Œåˆ†æå…‹éš†é—´embeddingçš„æ ¸å¿ƒå·¥å…·ç±»"""
    
    def __init__(
        self, 
        features: List[str],
        target: str = 'Titer',
        device: str = 'cpu',
        n_estimators: int = 8,
        random_state: int = 42
    ):
        """
        Parameters:
        -----------
        features : list
            åŸ¹å…»åŸºæˆåˆ†åˆ—åï¼Œå¦‚ ['C1', 'C2', ..., 'C86']
        target : str
            ç›®æ ‡åˆ—å
        device : str
            'cuda' æˆ– 'cpu'
        n_estimators : int
            TabPFN ensembleæ•°é‡
        """
        self.features = features
        self.target = target
        self.device = device
        self.random_state = random_state
        
        # åˆå§‹åŒ–TabPFN regressor
        self.regressor = TabPFNRegressor(
            n_estimators=n_estimators,
            device=device,
            random_state=random_state
        )
        
        # Embeddingæå–å™¨
        self.embedding_extractor = TabPFNEmbedding(
            tabpfn_reg=self.regressor,
            n_fold=0  # ä¸ä½¿ç”¨äº¤å‰éªŒè¯
        )
        
        # å­˜å‚¨è®­ç»ƒåçš„æ¨¡å‹å’Œembeddings
        self.source_embeddings_ = None
        self.target_embeddings_ = None
        self.is_fitted_ = False
        
    def fit_on_source(
        self, 
        source_data: pd.DataFrame,
        test_size: float = 0.2
    ) -> Dict:
        """åœ¨æºå…‹éš†ä¸Šè®­ç»ƒTabPFN"""
        
        print("=" * 80)
        print("Step 1: Training TabPFN on Source Clone")
        print("=" * 80)
        
        X = source_data[self.features].values
        y = source_data[self.target].values
        
        # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )
        
        print(f"Source data: {len(X_train)} train, {len(X_test)} test samples")
        
        # è®­ç»ƒTabPFN
        self.regressor.fit(X_train, y_train)
        
        # è¯„ä¼°æ€§èƒ½
        y_pred_train = self.regressor.predict(X_train)
        y_pred_test = self.regressor.predict(X_test)
        
        metrics = {
            'train_r2': r2_score(y_train, y_pred_train),
            'test_r2': r2_score(y_test, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),
            'n_train': len(X_train),
            'n_test': len(X_test)
        }
        
        print(f"\nSource Clone Performance:")
        print(f"  Train RÂ²: {metrics['train_r2']:.4f}")
        print(f"  Test RÂ²:  {metrics['test_r2']:.4f}")
        
        # æå–æºå…‹éš†çš„embeddings
        print("\nExtracting source embeddings...")
        self.source_embeddings_ = self._extract_embeddings(
            X_train, y_train, X
        )
        
        # å­˜å‚¨å®Œæ•´çš„æºæ•°æ®
        self.source_X_ = X
        self.source_y_ = y
        self.source_X_train_ = X_train
        self.source_y_train_ = y_train
        
        self.is_fitted_ = True
        
        print(f"Embedding shape: {self.source_embeddings_.shape}")
        print("âœ“ Source training completed\n")
        
        return metrics
    
    def extract_target_embeddings(
        self,
        target_data: pd.DataFrame,
        target_clone_name: str = "Target"
    ) -> np.ndarray:
        """æå–ç›®æ ‡å…‹éš†çš„embeddings"""
        
        if not self.is_fitted_:
            raise RuntimeError("Must fit on source data first!")
        
        print("=" * 80)
        print(f"Step 2: Extracting Embeddings for {target_clone_name}")
        print("=" * 80)
        
        X_target = target_data[self.features].values
        y_target = target_data[self.target].values if self.target in target_data else None
        
        print(f"Target data: {len(X_target)} samples")
        
        # ä½¿ç”¨æºæ¨¡å‹æå–ç›®æ ‡embeddings
        target_embeddings = self._extract_embeddings(
            self.source_X_train_,
            self.source_y_train_,
            X_target
        )
        
        self.target_embeddings_ = target_embeddings
        self.target_X_ = X_target
        self.target_y_ = y_target
        
        print(f"Target embedding shape: {target_embeddings.shape}")
        print(f"âœ“ Target embeddings extracted\n")
        
        return target_embeddings
    
    def _extract_embeddings(
        self,
        X_context: np.ndarray,
        y_context: np.ndarray,
        X_query: np.ndarray
    ) -> np.ndarray:
        """å†…éƒ¨æ–¹æ³•ï¼šæå–embeddings"""
        
        embeddings = self.embedding_extractor.get_embeddings(
            X_context,
            y_context,
            X_query,
            data_source="test"
        )
        
        # å–å¹³å‡
        if isinstance(embeddings, list):
            embeddings = np.mean(embeddings, axis=0)
        
        return embeddings
    
    def compute_embedding_similarity(
        self,
        metric: str = 'euclidean'
    ) -> Dict:
        """è®¡ç®—æºå…‹éš†å’Œç›®æ ‡å…‹éš†åœ¨embeddingç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§"""
        
        if self.source_embeddings_ is None or self.target_embeddings_ is None:
            raise RuntimeError("Must extract both embeddings first!")
        
        print("=" * 80)
        print("Step 3: Computing Embedding Similarity")
        print("=" * 80)
        
        from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity
        from scipy.spatial.distance import cdist
        
        # 1. æœ€è¿‘é‚»è·ç¦»
        nbrs = NearestNeighbors(n_neighbors=1, metric=metric)
        nbrs.fit(self.source_embeddings_)
        distances, indices = nbrs.kneighbors(self.target_embeddings_)
        
        avg_distance = distances.mean()
        median_distance = np.median(distances)
        
        # 2. è¦†ç›–åº¦
        source_internal_dist = cdist(
            self.source_embeddings_,
            self.source_embeddings_,
            metric=metric
        )
        source_internal_dist = source_internal_dist[
            ~np.eye(source_internal_dist.shape[0], dtype=bool)
        ]
        
        threshold = np.percentile(source_internal_dist, 75)
        extrapolation_rate = (distances.flatten() > threshold).mean()
        
        # 3. åˆ†å¸ƒç›¸ä¼¼æ€§
        if metric == 'cosine':
            source_mean = self.source_embeddings_.mean(axis=0).reshape(1, -1)
            target_mean = self.target_embeddings_.mean(axis=0).reshape(1, -1)
            distribution_similarity = cosine_similarity(source_mean, target_mean)[0, 0]
        else:
            source_mean = self.source_embeddings_.mean(axis=0)
            target_mean = self.target_embeddings_.mean(axis=0)
            distribution_distance = np.linalg.norm(source_mean - target_mean)
            max_possible_dist = np.linalg.norm(self.source_embeddings_.std(axis=0)) * 3
            distribution_similarity = 1 - min(distribution_distance / max_possible_dist, 1)
        
        # 4. ç»¼åˆå¯è¿ç§»æ€§å¾—åˆ†
        distance_score = 1 - min(avg_distance / (threshold + 1e-10), 1)
        coverage_score = 1 - extrapolation_rate
        
        transferability_score = (
            0.4 * distance_score +
            0.3 * coverage_score +
            0.3 * distribution_similarity
        )
        
        metrics = {
            'avg_nn_distance': avg_distance,
            'median_nn_distance': median_distance,
            'extrapolation_rate': extrapolation_rate,
            'distribution_similarity': distribution_similarity,
            'transferability_score': transferability_score,
            'nn_indices': indices.flatten(),
            'nn_distances': distances.flatten()
        }
        
        print(f"\nTransferability Metrics:")
        print(f"  Transferability Score: {transferability_score:.4f}")
        print(f"  Average NN Distance:   {avg_distance:.4f}")
        print(f"  Extrapolation Rate:    {extrapolation_rate:.2%}")
        
        if transferability_score > 0.7:
            print("   â†’ HIGH transferability âœ“")
        elif transferability_score > 0.5:
            print("   â†’ MODERATE transferability")
        else:
            print("   â†’ LOW transferability âš ï¸")
        
        print()
        
        return metrics
    
    def visualize_embedding_space(
        self,
        method: str = 'tsne',
        save_path: Optional[Path] = None
    ):
        """å¯è§†åŒ–embeddingç©ºé—´"""
        
        if self.source_embeddings_ is None or self.target_embeddings_ is None:
            raise RuntimeError("Must extract both embeddings first!")
        
        print("=" * 80)
        print(f"Step 4: Visualizing Embedding Space ({method.upper()})")
        print("=" * 80)
        
        # åˆå¹¶embeddings
        combined_embeddings = np.vstack([
            self.source_embeddings_,
            self.target_embeddings_
        ])
        
        # é™ç»´
        if method == 'tsne':
            reducer = TSNE(n_components=2, random_state=self.random_state, perplexity=30)
        elif method == 'pca':
            reducer = PCA(n_components=2, random_state=self.random_state)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        print(f"Reducing {combined_embeddings.shape[1]}D â†’ 2D...")
        embeddings_2d = reducer.fit_transform(combined_embeddings)
        
        # åˆ†ç¦»
        n_source = len(self.source_embeddings_)
        source_2d = embeddings_2d[:n_source]
        target_2d = embeddings_2d[n_source:]
        
        # ç»˜å›¾
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾ï¼šæŒ‰titerç€è‰²
        ax = axes[0]
        scatter_source = ax.scatter(
            source_2d[:, 0], source_2d[:, 1],
            c=self.source_y_, cmap='viridis',
            s=100, alpha=0.6, edgecolors='black',
            label='Source Clone'
        )
        scatter_target = ax.scatter(
            target_2d[:, 0], target_2d[:, 1],
            c=self.target_y_ if self.target_y_ is not None else 'red',
            cmap='plasma' if self.target_y_ is not None else None,
            s=100, alpha=0.6, marker='^', edgecolors='black',
            label='Target Clone'
        )
        
        ax.set_title(f'Embedding Space - Colored by Titer', fontsize=14, fontweight='bold')
        ax.set_xlabel(f'{method.upper()} Component 1', fontsize=12)
        ax.set_ylabel(f'{method.upper()} Component 2', fontsize=12)
        ax.legend(fontsize=12)
        ax.grid(alpha=0.3)
        
        cbar = plt.colorbar(scatter_source, ax=ax)
        cbar.set_label('Titer', fontsize=12)
        
        # å³å›¾ï¼šåˆ†å¸ƒé‡å 
        ax = axes[1]
        ax.scatter(
            source_2d[:, 0], source_2d[:, 1],
            c='blue', s=100, alpha=0.4, label='Source'
        )
        ax.scatter(
            target_2d[:, 0], target_2d[:, 1],
            c='red', s=100, alpha=0.4, marker='^', label='Target'
        )
        
        ax.set_title('Distribution Overlap', fontsize=14, fontweight='bold')
        ax.set_xlabel(f'{method.upper()} Component 1', fontsize=12)
        ax.set_ylabel(f'{method.upper()} Component 2', fontsize=12)
        ax.legend(fontsize=12)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved to {save_path}")
        
        plt.show()
        print()
```

---

## ğŸ“„ æ–‡ä»¶2: `embedding_regressor.py`

```python
"""
embedding_regressor.py
åŸºäºEmbeddingç©ºé—´çš„å°‘æ ·æœ¬è¿ç§»å­¦ä¹ å›å½’æ¨¡å‹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å®ç°6ç§è¿ç§»å­¦ä¹ ç­–ç•¥
2. è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
3. å¯è§†åŒ–é¢„æµ‹æ•ˆæœ
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Literal, Tuple
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

from embedding_analyzer import CloneEmbeddingAnalyzer


class EmbeddingSpaceRegressor:
    """åœ¨Embeddingç©ºé—´ä¸­æ„å»ºå›å½’æ¨¡å‹"""
    
    def __init__(
        self,
        analyzer: CloneEmbeddingAnalyzer,
        regressor_type: Literal['ridge', 'lasso', 'elastic', 'rf', 'gbm', 'svr', 'mlp'] = 'ridge',
        alpha: float = 1.0,
        random_state: int = 42
    ):
        """
        Parameters:
        -----------
        regressor_type : str
            'ridge', 'lasso', 'elastic', 'rf', 'gbm', 'svr', 'mlp'
        alpha : float
            æ­£åˆ™åŒ–å‚æ•°
        """
        if not analyzer.is_fitted_:
            raise RuntimeError("Analyzer must be fitted first!")
        
        self.analyzer = analyzer
        self.regressor_type = regressor_type
        self.alpha = alpha
        self.random_state = random_state
        
        self.scaler = StandardScaler()
        self.models_ = {}
        self.scalers_ = {}
        self.performance_history_ = {}
        
    def _create_regressor(self) -> object:
        """åˆ›å»ºå›å½’å™¨"""
        
        if self.regressor_type == 'ridge':
            return Ridge(alpha=self.alpha, random_state=self.random_state)
        elif self.regressor_type == 'lasso':
            return Lasso(alpha=self.alpha, random_state=self.random_state, max_iter=5000)
        elif self.regressor_type == 'elastic':
            return ElasticNet(alpha=self.alpha, random_state=self.random_state, max_iter=5000)
        elif self.regressor_type == 'rf':
            return RandomForestRegressor(
                n_estimators=100, max_depth=10, min_samples_leaf=3,
                random_state=self.random_state, n_jobs=-1
            )
        elif self.regressor_type == 'gbm':
            return GradientBoostingRegressor(
                n_estimators=100, max_depth=5, learning_rate=0.1,
                random_state=self.random_state
            )
        elif self.regressor_type == 'svr':
            return SVR(C=1.0, epsilon=0.1, kernel='rbf')
        elif self.regressor_type == 'mlp':
            return MLPRegressor(
                hidden_layer_sizes=(128, 64), activation='relu',
                alpha=self.alpha, max_iter=1000, early_stopping=True,
                random_state=self.random_state
            )
        else:
            raise ValueError(f"Unknown regressor type: {self.regressor_type}")
    
    def fit_all_strategies(
        self,
        target_train_indices: np.ndarray,
        target_test_indices: np.ndarray,
        verbose: bool = True
    ) -> Dict[str, Dict]:
        """è®­ç»ƒæ‰€æœ‰è¿ç§»å­¦ä¹ ç­–ç•¥"""
        
        if verbose:
            print("=" * 80)
            print("Training All Transfer Learning Strategies")
            print("=" * 80)
            print(f"Target train: {len(target_train_indices)} samples")
            print(f"Target test:  {len(target_test_indices)} samples")
            print(f"Regressor: {self.regressor_type}")
            print()
        
        strategies = [
            'source_only',
            'target_only',
            'fine_tuning',
            'mixed',
            'weighted',
            'domain_adapted'
        ]
        
        results = {}
        
        for strategy in strategies:
            if verbose:
                print(f"\n{'â”€' * 80}")
                print(f"Strategy: {strategy.upper().replace('_', ' ')}")
                print(f"{'â”€' * 80}")
            
            try:
                metrics = self._fit_single_strategy(
                    strategy=strategy,
                    target_train_indices=target_train_indices,
                    target_test_indices=target_test_indices,
                    verbose=verbose
                )
                results[strategy] = metrics
                
                if verbose:
                    self._print_metrics(metrics)
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸  Failed: {e}")
                results[strategy] = {'error': str(e)}
        
        self.performance_history_['all_strategies'] = results
        
        if verbose:
            print("\n" + "=" * 80)
            print("SUMMARY")
            print("=" * 80)
            self._print_summary(results)
        
        return results
    
    def _fit_single_strategy(
        self,
        strategy: str,
        target_train_indices: np.ndarray,
        target_test_indices: np.ndarray,
        verbose: bool = False
    ) -> Dict:
        """è®­ç»ƒå•ä¸ªç­–ç•¥"""
        
        target_train_emb = self.analyzer.target_embeddings_[target_train_indices]
        target_test_emb = self.analyzer.target_embeddings_[target_test_indices]
        target_train_y = self.analyzer.target_y_[target_train_indices]
        target_test_y = self.analyzer.target_y_[target_test_indices]
        
        source_emb = self.analyzer.source_embeddings_
        source_y = self.analyzer.source_y_
        
        if strategy == 'source_only':
            return self._fit_source_only(
                source_emb, source_y, target_test_emb, target_test_y, verbose
            )
        elif strategy == 'target_only':
            return self._fit_target_only(
                target_train_emb, target_train_y, target_test_emb, target_test_y, verbose
            )
        elif strategy == 'fine_tuning':
            return self._fit_fine_tuning(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y, verbose
            )
        elif strategy == 'mixed':
            return self._fit_mixed(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                weight_ratio=1.0, verbose=verbose
            )
        elif strategy == 'weighted':
            return self._fit_mixed(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                weight_ratio=5.0, verbose=verbose
            )
        elif strategy == 'domain_adapted':
            return self._fit_domain_adapted(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y, verbose
            )
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
    
    def _fit_source_only(self, source_emb, source_y, target_test_emb, target_test_y, verbose):
        """ç­–ç•¥1: ä»…æºæ•°æ®"""
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        model = self._create_regressor()
        model.fit(source_emb_scaled, source_y)
        
        y_pred_test = model.predict(target_test_emb_scaled)
        
        self.models_['source_only'] = model
        self.scalers_['source_only'] = scaler
        
        return {
            'test_r2': r2_score(target_test_y, y_pred_test),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test)
        }
    
    def _fit_target_only(self, target_train_emb, target_train_y, target_test_emb, target_test_y, verbose):
        """ç­–ç•¥2: ä»…ç›®æ ‡æ•°æ®"""
        scaler = StandardScaler()
        target_train_emb_scaled = scaler.fit_transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        model = self._create_regressor()
        model.fit(target_train_emb_scaled, target_train_y)
        
        y_pred_test = model.predict(target_test_emb_scaled)
        
        self.models_['target_only'] = model
        self.scalers_['target_only'] = scaler
        
        return {
            'test_r2': r2_score(target_test_y, y_pred_test),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test)
        }
    
    def _fit_fine_tuning(self, source_emb, source_y, target_train_emb, target_train_y, 
                         target_test_emb, target_test_y, verbose):
        """ç­–ç•¥3: Fine-tuning"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨åŠ æƒæ··åˆæ¨¡æ‹Ÿ
        return self._fit_mixed(
            source_emb, source_y, target_train_emb, target_train_y,
            target_test_emb, target_test_y, weight_ratio=3.0, verbose=False
        )
    
    def _fit_mixed(self, source_emb, source_y, target_train_emb, target_train_y,
                   target_test_emb, target_test_y, weight_ratio=1.0, verbose=False):
        """ç­–ç•¥4/5: æ··åˆè®­ç»ƒ"""
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb)
        target_train_emb_scaled = scaler.transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        combined_emb = np.vstack([source_emb_scaled, target_train_emb_scaled])
        combined_y = np.concatenate([source_y, target_train_y])
        
        sample_weights = np.concatenate([
            np.ones(len(source_y)) * 1.0,
            np.ones(len(target_train_y)) * weight_ratio
        ])
        
        model = self._create_regressor()
        try:
            model.fit(combined_emb, combined_y, sample_weight=sample_weights)
        except TypeError:
            model.fit(combined_emb, combined_y)
        
        y_pred_test = model.predict(target_test_emb_scaled)
        
        strategy_name = 'weighted' if weight_ratio > 1.0 else 'mixed'
        self.models_[strategy_name] = model
        self.scalers_[strategy_name] = scaler
        
        return {
            'test_r2': r2_score(target_test_y, y_pred_test),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test)
        }
    
    def _fit_domain_adapted(self, source_emb, source_y, target_train_emb, target_train_y,
                           target_test_emb, target_test_y, verbose):
        """ç­–ç•¥6: Domain Adaptation (CORAL)"""
        
        def coral_alignment(source, target):
            cov_source = np.cov(source, rowvar=False) + np.eye(source.shape[1]) * 1e-5
            cov_target = np.cov(target, rowvar=False) + np.eye(target.shape[1]) * 1e-5
            
            source_centered = source - source.mean(axis=0)
            
            try:
                A_source = np.linalg.cholesky(cov_source)
                A_target = np.linalg.cholesky(cov_target)
            except np.linalg.LinAlgError:
                U_s, S_s, _ = np.linalg.svd(cov_source)
                A_source = U_s @ np.diag(np.sqrt(S_s))
                U_t, S_t, _ = np.linalg.svd(cov_target)
                A_target = U_t @ np.diag(np.sqrt(S_t))
            
            source_aligned = source_centered @ np.linalg.inv(A_source) @ A_target
            source_aligned += target.mean(axis=0)
            
            return source_aligned
        
        source_emb_aligned = coral_alignment(source_emb, target_train_emb)
        
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb_aligned)
        target_train_emb_scaled = scaler.transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        combined_emb = np.vstack([source_emb_scaled, target_train_emb_scaled])
        combined_y = np.concatenate([source_y, target_train_y])
        
        sample_weights = np.concatenate([
            np.ones(len(source_y)) * 1.0,
            np.ones(len(target_train_y)) * 2.0
        ])
        
        model = self._create_regressor()
        try:
            model.fit(combined_emb, combined_y, sample_weight=sample_weights)
        except TypeError:
            model.fit(combined_emb, combined_y)
        
        y_pred_test = model.predict(target_test_emb_scaled)
        
        self.models_['domain_adapted'] = model
        self.scalers_['domain_adapted'] = scaler
        
        return {
            'test_r2': r2_score(target_test_y, y_pred_test),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test)
        }
    
    def predict(self, X: np.ndarray, strategy: str = 'best') -> np.ndarray:
        """é¢„æµ‹æ–°æ•°æ®
        
        Parameters:
        -----------
        X : ndarray
            åŸå§‹ç‰¹å¾ç©ºé—´çš„æ•°æ®
        strategy : str
            ä½¿ç”¨çš„ç­–ç•¥ï¼Œ'best'è‡ªåŠ¨é€‰æ‹©
        """
        # æå–embeddings
        embeddings = self.analyzer._extract_embeddings(
            self.analyzer.source_X_train_,
            self.analyzer.source_y_train_,
            X
        )
        
        # é€‰æ‹©ç­–ç•¥
        if strategy == 'best':
            if 'all_strategies' in self.performance_history_:
                best_strategy = max(
                    self.performance_history_['all_strategies'].items(),
                    key=lambda x: x[1].get('test_r2', -np.inf) if 'error' not in x[1] else -np.inf
                )[0]
            else:
                best_strategy = list(self.models_.keys())[0]
            strategy = best_strategy
        
        if strategy not in self.models_:
            raise ValueError(f"Strategy '{strategy}' not fitted!")
        
        model = self.models_[strategy]
        scaler = self.scalers_[strategy]
        
        embeddings_scaled = scaler.transform(embeddings)
        predictions = model.predict(embeddings_scaled)
        
        return predictions
    
    def _print_metrics(self, metrics: Dict):
        """æ‰“å°æŒ‡æ ‡"""
        if 'error' in metrics:
            print(f"  âŒ Error: {metrics['error']}")
            return
        
        print(f"  Test RÂ²:   {metrics['test_r2']:.4f}")
        print(f"  Test RMSE: {metrics['test_rmse']:.2f}")
        print(f"  Test MAE:  {metrics['test_mae']:.2f}")
    
    def _print_summary(self, results: Dict):
        """æ‰“å°æ±‡æ€»"""
        summary_data = []
        for strategy, metrics in results.items():
            if 'error' not in metrics:
                summary_data.append({
                    'Strategy': strategy.replace('_', ' ').title(),
                    'Test RÂ²': metrics['test_r2'],
                    'Test RMSE': metrics['test_rmse']
                })
        
        if not summary_data:
            return
        
        df = pd.DataFrame(summary_data)
        df = df.sort_values('Test RÂ²', ascending=False)
        
        print("\nRanked by Test RÂ²:")
        print(df.to_string(index=False))
        
        best = df.iloc[0]
        print(f"\nğŸ† Best: {best['Strategy']} (RÂ² = {best['Test RÂ²']:.4f})")
    
    def visualize_predictions(
        self,
        target_test_indices: np.ndarray,
        strategies: Optional[List[str]] = None,
        save_path: Optional[Path] = None
    ):
        """å¯è§†åŒ–é¢„æµ‹æ•ˆæœ"""
        
        if strategies is None:
            strategies = list(self.models_.keys())
        
        y_true = self.analyzer.target_y_[target_test_indices]
        
        predictions = {}
        for strategy in strategies:
            if strategy in self.models_:
                model = self.models_[strategy]
                scaler = self.scalers_[strategy]
                
                test_emb = self.analyzer.target_embeddings_[target_test_indices]
                test_emb_scaled = scaler.transform(test_emb)
                y_pred = model.predict(test_emb_scaled)
                predictions[strategy] = y_pred
        
        n_strategies = len(predictions)
        n_cols = min(3, n_strategies)
        n_rows = (n_strategies + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
        if n_strategies == 1:
            axes = np.array([axes])
        axes = axes.flatten()
        
        for idx, (strategy, y_pred) in enumerate(predictions.items()):
            ax = axes[idx]
            
            ax.scatter(y_true, y_pred, alpha=0.6, s=100, edgecolors='black')
            
            min_val = min(y_true.min(), y_pred.min())
            max_val = max(y_true.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')
            
            r2 = r2_score(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            
            ax.set_title(f'{strategy.replace("_", " ").title()}\nRÂ² = {r2:.3f}, RMSE = {rmse:.2f}',
                        fontsize=12, fontweight='bold')
            ax.set_xlabel('True Titer', fontsize=11)
            ax.set_ylabel('Predicted Titer', fontsize=11)
            ax.legend()
            ax.grid(alpha=0.3)
        
        for idx in range(len(predictions), len(axes)):
            axes[idx].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
```

---

## ğŸ“„ æ–‡ä»¶3: `embedding_optimizer.py` (æ ¸å¿ƒæ–°å¢)

```python
"""
embedding_optimizer.py
åŸºäºå·²éªŒè¯æ¨¡å‹é¢„æµ‹å’Œä¼˜åŒ–æœªçŸ¥åŸ¹å…»åŸºé…æ–¹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é¢„æµ‹å…¨æ–°é…æ–¹çš„titer
2. éšæœºæœç´¢æœ€ä¼˜é…æ–¹
3. æ¢¯åº¦ä¼˜åŒ–ï¼ˆé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼‰
4. å¤šæ ·æ€§ä¼˜åŒ–
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Dict, List, Optional
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from embedding_analyzer import CloneEmbeddingAnalyzer
from embedding_regressor import EmbeddingSpaceRegressor


class TrueEmbeddingGuidedOptimizer:
    """é¢„æµ‹å’Œä¼˜åŒ–æœªçŸ¥åŸ¹å…»åŸºé…æ–¹"""
    
    def __init__(
        self,
        analyzer: CloneEmbeddingAnalyzer,
        trained_regressor: EmbeddingSpaceRegressor,
        best_strategy: str,
        feature_bounds: Dict[str, Tuple[float, float]]
    ):
        """
        Parameters:
        -----------
        analyzer : CloneEmbeddingAnalyzer
            å·²fitçš„analyzer
        trained_regressor : EmbeddingSpaceRegressor
            å·²è®­ç»ƒçš„å›å½’æ¨¡å‹
        best_strategy : str
            æœ€ä½³ç­–ç•¥åç§°ï¼ˆå¦‚'domain_adapted'ï¼‰
        feature_bounds : dict
            ç‰¹å¾å–å€¼èŒƒå›´ï¼Œå¦‚ {'C1': (0, 1), 'C2': (0, 0.5), ...}
        """
        self.analyzer = analyzer
        self.regressor = trained_regressor
        self.strategy = best_strategy
        self.feature_bounds = feature_bounds
        self.features = list(feature_bounds.keys())
        
        if len(self.features) != len(analyzer.features):
            raise ValueError("Feature bounds must cover all features!")
    
    def predict_titer_for_new_formulation(
        self,
        formulation: np.ndarray
    ) -> float:
        """é¢„æµ‹å•ä¸ªå…¨æ–°é…æ–¹çš„titer
        
        Parameters:
        -----------
        formulation : ndarray
            é…æ–¹ï¼Œå½¢çŠ¶ (n_features,) æˆ– (1, n_features)
            
        Returns:
        --------
        predicted_titer : float
        """
        if formulation.ndim == 1:
            formulation = formulation.reshape(1, -1)
        
        predicted_titer = self.regressor.predict(
            X=formulation,
            strategy=self.strategy
        )
        
        return predicted_titer[0]
    
    def generate_random_candidates(
        self,
        n_candidates: int = 1000,
        seed: Optional[int] = None
    ) -> np.ndarray:
        """éšæœºç”Ÿæˆå€™é€‰é…æ–¹"""
        
        if seed is not None:
            np.random.seed(seed)
        
        candidates = []
        
        for _ in range(n_candidates):
            formulation = []
            for feat in self.features:
                low, high = self.feature_bounds[feat]
                value = np.random.uniform(low, high)
                formulation.append(value)
            candidates.append(formulation)
        
        return np.array(candidates)
    
    def optimize_formulation_random_search(
        self,
        n_candidates: int = 10000,
        top_k: int = 10,
        seed: int = 42
    ) -> pd.DataFrame:
        """éšæœºæœç´¢æœ€ä¼˜é…æ–¹
        
        â­ æ ¸å¿ƒæµç¨‹ï¼š
        1. éšæœºç”Ÿæˆå¤§é‡å€™é€‰é…æ–¹
        2. ç”¨æ¨¡å‹é¢„æµ‹æ¯ä¸ªé…æ–¹çš„titer
        3. è¿”å›é¢„æµ‹titeræœ€é«˜çš„top-k
        
        è¿™äº›æ¨èçš„é…æ–¹æ˜¯ã€ä»æœªæµ‹è¯•è¿‡çš„ã€‘ï¼
        """
        print("=" * 80)
        print("Random Search for Unknown Formulations")
        print("=" * 80)
        print(f"Generating {n_candidates} random candidates...")
        
        # 1. ç”Ÿæˆå€™é€‰
        candidates = self.generate_random_candidates(n_candidates, seed)
        
        # 2. é¢„æµ‹æ‰€æœ‰å€™é€‰çš„titer
        print("Predicting titers...")
        predicted_titers = self.regressor.predict(
            X=candidates,
            strategy=self.strategy
        )
        
        # 3. æ’åºå¹¶é€‰æ‹©top-k
        top_indices = np.argsort(predicted_titers)[::-1][:top_k]
        
        # 4. æ„å»ºæ¨èDataFrame
        recommendations = pd.DataFrame(
            candidates[top_indices],
            columns=self.features
        )
        recommendations['Predicted_Titer'] = predicted_titers[top_indices]
        recommendations['Rank'] = range(1, top_k + 1)
        
        cols = ['Rank', 'Predicted_Titer'] + self.features
        recommendations = recommendations[cols]
        
        print(f"\nâœ“ Top-{top_k} formulations identified:")
        print(f"  Best predicted titer: {predicted_titers[top_indices[0]]:.2f}")
        print(f"  Top-{top_k} average:  {predicted_titers[top_indices].mean():.2f}")
        print()
        
        return recommendations
    
    def optimize_formulation_gradient_based(
        self,
        n_starts: int = 10,
        method: str = 'L-BFGS-B'
    ) -> pd.DataFrame:
        """åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼ˆé€‚ç”¨äºRidgeç­‰çº¿æ€§æ¨¡å‹ï¼‰"""
        
        print("=" * 80)
        print("Gradient-Based Optimization")
        print("=" * 80)
        
        if self.regressor.regressor_type not in ['ridge', 'lasso', 'elastic']:
            print("âš ï¸  Gradient optimization works best with linear models")
            print("   Falling back to random search...")
            return self.optimize_formulation_random_search(n_candidates=10000, top_k=10)
        
        def objective(x):
            titer = self.predict_titer_for_new_formulation(x)
            return -titer  # æœ€å°åŒ–è´Ÿå€¼ = æœ€å¤§åŒ–æ­£å€¼
        
        bounds = [self.feature_bounds[feat] for feat in self.features]
        
        print(f"Running {n_starts} optimizations...")
        
        results = []
        
        for i in range(n_starts):
            x0 = np.array([np.random.uniform(low, high) for low, high in bounds])
            
            res = minimize(
                objective,
                x0,
                method=method,
                bounds=bounds,
                options={'maxiter': 1000}
            )
            
            if res.success:
                results.append({
                    'formulation': res.x,
                    'predicted_titer': -res.fun
                })
        
        if not results:
            print("âŒ All optimizations failed!")
            return pd.DataFrame()
        
        results = sorted(results, key=lambda x: x['predicted_titer'], reverse=True)
        
        recommendations = pd.DataFrame(
            [r['formulation'] for r in results],
            columns=self.features
        )
        recommendations['Predicted_Titer'] = [r['predicted_titer'] for r in results]
        recommendations['Rank'] = range(1, len(results) + 1)
        
        cols = ['Rank', 'Predicted_Titer'] + self.features
        recommendations = recommendations[cols]
        
        print(f"\nâœ“ Found {len(results)} optimal formulations:")
        print(f"  Best predicted titer: {results[0]['predicted_titer']:.2f}")
        print()
        
        return recommendations
    
    def optimize_with_diversity(
        self,
        n_recommendations: int = 10,
        diversity_weight: float = 0.3,
        n_candidates: int = 5000
    ) -> pd.DataFrame:
        """åœ¨ä¼˜åŒ–titerçš„åŒæ—¶ä¿æŒé…æ–¹å¤šæ ·æ€§"""
        
        print("=" * 80)
        print("Diversity-Aware Optimization")
        print("=" * 80)
        
        candidates = self.generate_random_candidates(n_candidates)
        predicted_titers = self.regressor.predict(candidates, strategy=self.strategy)
        
        # å½’ä¸€åŒ–
        titer_min, titer_max = predicted_titers.min(), predicted_titers.max()
        normalized_titers = (predicted_titers - titer_min) / (titer_max - titer_min + 1e-10)
        
        # è´ªå¿ƒé€‰æ‹©
        selected_indices = []
        first_idx = np.argmax(predicted_titers)
        selected_indices.append(first_idx)
        
        for _ in range(n_recommendations - 1):
            selected_formulations = candidates[selected_indices]
            
            min_distances = []
            for candidate in candidates:
                if len(selected_formulations) == 0:
                    min_distances.append(0)
                else:
                    distances = np.linalg.norm(selected_formulations - candidate, axis=1)
                    min_distances.append(distances.min())
            
            min_distances = np.array(min_distances)
            
            if min_distances.max() > 0:
                normalized_distances = min_distances / min_distances.max()
            else:
                normalized_distances = np.zeros_like(min_distances)
            
            scores = (
                (1 - diversity_weight) * normalized_titers +
                diversity_weight * normalized_distances
            )
            
            scores[selected_indices] = -np.inf
            next_idx = np.argmax(scores)
            selected_indices.append(next_idx)
        
        recommendations = pd.DataFrame(
            candidates[selected_indices],
            columns=self.features
        )
        recommendations['Predicted_Titer'] = predicted_titers[selected_indices]
        recommendations['Rank'] = range(1, n_recommendations + 1)
        
        cols = ['Rank', 'Predicted_Titer'] + self.features
        recommendations = recommendations[cols]
        
        print(f"\nâœ“ Selected {n_recommendations} diverse formulations:")
        print(f"  Best titer: {predicted_titers[selected_indices[0]]:.2f}")
        print(f"  Avg titer:  {predicted_titers[selected_indices].mean():.2f}")
        
        from sklearn.metrics.pairwise import euclidean_distances
        pairwise_dist = euclidean_distances(candidates[selected_indices])
        avg_distance = pairwise_dist[np.triu_indices_from(pairwise_dist, k=1)].mean()
        print(f"  Avg diversity: {avg_distance:.4f}")
        print()
        
        return recommendations
    
    def validate_on_known_data(self) -> Dict:
        """åœ¨å·²çŸ¥æ•°æ®ä¸ŠéªŒè¯æ¨¡å‹å¯é æ€§"""
        
        print("=" * 80)
        print("Model Validation on Known Target Data")
        print("=" * 80)
        
        if self.analyzer.target_y_ is None:
            print("No target data for validation")
            return {}
        
        predictions = self.regressor.predict(
            self.analyzer.target_X_,
            strategy=self.strategy
        )
        
        from sklearn.metrics import r2_score, mean_absolute_error
        
        r2 = r2_score(self.analyzer.target_y_, predictions)
        mae = mean_absolute_error(self.analyzer.target_y_, predictions)
        
        print(f"\nValidation Results:")
        print(f"  RÂ² Score: {r2:.4f}")
        print(f"  MAE:      {mae:.2f}")
        
        if r2 > 0.6:
            print("  âœ… HIGH confidence - Safe to use for new predictions")
        elif r2 > 0.4:
            print("  âœ“ MODERATE confidence - Use with caution")
        else:
            print("  âš ï¸  LOW confidence - Collect more data recommended")
        
        print()
        
        return {'r2': r2, 'mae': mae}
```

---

## ğŸ“„ æ–‡ä»¶4: `example_complete_workflow.py` (å®Œæ•´ç¤ºä¾‹)

```python
"""
example_complete_workflow.py
å®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµç¤ºä¾‹

å±•ç¤ºä»æ•°æ®åŠ è½½åˆ°ä¼˜åŒ–æœªçŸ¥é…æ–¹çš„å…¨æµç¨‹
"""

import numpy as np
import pandas as pd
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from embedding_analyzer import CloneEmbeddingAnalyzer
from embedding_regressor import EmbeddingSpaceRegressor
from embedding_optimizer import TrueEmbeddingGuidedOptimizer


def main():
    """å®Œæ•´å·¥ä½œæµ"""
    
    print("\n" + "ğŸ”¬" * 40)
    print("COMPLETE EMBEDDING-BASED TRANSFER LEARNING WORKFLOW")
    print("ğŸ”¬" * 40 + "\n")
    
    # ========================================================================
    # PART 1: æ•°æ®å‡†å¤‡
    # ========================================================================
    
    print("=" * 80)
    print("PART 1: Data Preparation")
    print("=" * 80)
    
    # å®šä¹‰ç‰¹å¾
    n_features = 86
    features = [f'C{i}' for i in range(1, n_features + 1)]
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
    np.random.seed(42)
    
    # å…‹éš†A: 50æ¡
    clone_A_data = pd.DataFrame({
        **{feat: np.random.rand(50) * 0.5 for feat in features},
        'Titer': 1000 + np.random.rand(50) * 1000
    })
    
    # å…‹éš†B: 36æ¡
    clone_B_data = pd.DataFrame({
        **{feat: np.random.rand(36) * 0.6 for feat in features},
        'Titer': 1500 + np.random.rand(36) * 1200
    })
    
    print(f"âœ“ Clone A loaded: {len(clone_A_data)} samples")
    print(f"âœ“ Clone B loaded: {len(clone_B_data)} samples")
    print()
    
    # ========================================================================
    # PART 2: Embeddingæå–ä¸å¯è¿ç§»æ€§åˆ†æ
    # ========================================================================
    
    print("=" * 80)
    print("PART 2: Embedding Analysis")
    print("=" * 80)
    
    analyzer = CloneEmbeddingAnalyzer(
        features=features,
        target='Titer',
        device='cpu',  # å¦‚æœæœ‰GPUæ”¹ä¸º'cuda'
        n_estimators=8,
        random_state=42
    )
    
    # åœ¨å…‹éš†Aä¸Šè®­ç»ƒ
    source_metrics = analyzer.fit_on_source(clone_A_data, test_size=0.2)
    
    # æå–å…‹éš†Bçš„embeddings
    analyzer.extract_target_embeddings(clone_B_data, "Clone B")
    
    # è®¡ç®—å¯è¿ç§»æ€§
    similarity_metrics = analyzer.compute_embedding_similarity(metric='euclidean')
    
    # å¯è§†åŒ–
    analyzer.visualize_embedding_space(
        method='tsne',
        save_path=Path('embedding_space.png')
    )
    
    # ========================================================================
    # PART 3: åˆ’åˆ†ç›®æ ‡æ•°æ®ï¼ˆ10è®­ç»ƒ + 26æµ‹è¯•ï¼‰
    # ========================================================================
    
    print("=" * 80)
    print("PART 3: Split Target Data")
    print("=" * 80)
    
    n_target_train = 10
    n_total = len(clone_B_data)
    
    all_indices = np.arange(n_total)
    np.random.shuffle(all_indices)
    
    target_train_indices = all_indices[:n_target_train]
    target_test_indices = all_indices[n_target_train:]
    
    print(f"âœ“ Target training: {len(target_train_indices)} samples")
    print(f"âœ“ Target testing:  {len(target_test_indices)} samples")
    print()
    
    # ========================================================================
    # PART 4: è®­ç»ƒè¿ç§»å­¦ä¹ æ¨¡å‹
    # ========================================================================
    
    print("=" * 80)
    print("PART 4: Train Transfer Learning Models")
    print("=" * 80)
    
    # æµ‹è¯•å¤šä¸ªå›å½’å™¨
    regressor_types = ['ridge', 'rf', 'gbm']
    all_results = {}
    
    for reg_type in regressor_types:
        print(f"\n{'#' * 80}")
        print(f"Testing Regressor: {reg_type.upper()}")
        print(f"{'#' * 80}\n")
        
        emb_regressor = EmbeddingSpaceRegressor(
            analyzer=analyzer,
            regressor_type=reg_type,
            alpha=1.0,
            random_state=42
        )
        
        results = emb_regressor.fit_all_strategies(
            target_train_indices=target_train_indices,
            target_test_indices=target_test_indices,
            verbose=True
        )
        
        all_results[reg_type] = {
            'regressor': emb_regressor,
            'results': results
        }
    
    # ========================================================================
    # PART 5: é€‰æ‹©æœ€ä½³æ¨¡å‹
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("PART 5: Select Best Model")
    print("=" * 80)
    
    # æ±‡æ€»æ‰€æœ‰é…ç½®
    comparison_data = []
    for reg_type, data in all_results.items():
        for strategy, metrics in data['results'].items():
            if 'error' not in metrics:
                comparison_data.append({
                    'Regressor': reg_type.upper(),
                    'Strategy': strategy.replace('_', ' ').title(),
                    'Test RÂ²': metrics['test_r2'],
                    'Test RMSE': metrics['test_rmse']
                })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.sort_values('Test RÂ²', ascending=False)
    
    print("\nğŸ† Top 10 Configurations:")
    print(comparison_df.head(10).to_string(index=False))
    
    # é€‰æ‹©æœ€ä½³
    best_row = comparison_df.iloc[0]
    best_regressor_type = best_row['Regressor'].lower()
    best_strategy = best_row['Strategy'].lower().replace(' ', '_')
    best_r2 = best_row['Test RÂ²']
    
    print(f"\nâœ… BEST MODEL SELECTED:")
    print(f"   Regressor: {best_regressor_type.upper()}")
    print(f"   Strategy:  {best_strategy.replace('_', ' ').title()}")
    print(f"   Test RÂ²:   {best_r2:.4f}")
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_emb_regressor = all_results[best_regressor_type]['regressor']
    
    # å¯è§†åŒ–é¢„æµ‹
    print("\nğŸ“Š Visualizing predictions...")
    best_emb_regressor.visualize_predictions(
        target_test_indices=target_test_indices,
        save_path=Path('prediction_performance.png')
    )
    
    # ========================================================================
    # PART 6: ä¼˜åŒ–æœªçŸ¥é…æ–¹ï¼ˆæ ¸å¿ƒï¼ï¼‰
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("PART 6: OPTIMIZE UNKNOWN FORMULATIONS (NEW!)")
    print("=" * 80)
    print("\nâš ï¸  CRITICAL: The following formulations are UNKNOWN (never tested)!\n")
    
    # å®šä¹‰ç‰¹å¾bounds
    feature_bounds = {f'C{i}': (0, 1) for i in range(1, n_features + 1)}
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = TrueEmbeddingGuidedOptimizer(
        analyzer=analyzer,
        trained_regressor=best_emb_regressor,
        best_strategy=best_strategy,
        feature_bounds=feature_bounds
    )
    
    # éªŒè¯æ¨¡å‹å¯é æ€§
    validation_metrics = optimizer.validate_on_known_data()
    
    # ç­–ç•¥1: éšæœºæœç´¢
    print("\n" + "â”€" * 80)
    print("Strategy 1: Random Search")
    print("â”€" * 80)
    
    recommendations_random = optimizer.optimize_formulation_random_search(
        n_candidates=10000,
        top_k=10,
        seed=42
    )
    
    print("ğŸ“‹ Top 5 Recommended Formulations (Random Search):")
    print(recommendations_random.head().to_string(index=False))
    
    # ä¿å­˜
    output_path = Path('recommended_formulations_random.csv')
    recommendations_random.to_csv(output_path, index=False)
    print(f"\nâœ“ Saved to {output_path}")
    
    # ç­–ç•¥2: æ¢¯åº¦ä¼˜åŒ–ï¼ˆå¦‚æœæ˜¯Ridgeï¼‰
    if best_regressor_type == 'ridge':
        print("\n" + "â”€" * 80)
        print("Strategy 2: Gradient-Based Optimization")
        print("â”€" * 80)
        
        recommendations_gradient = optimizer.optimize_formulation_gradient_based(
            n_starts=20,
            method='L-BFGS-B'
        )
        
        print("ğŸ“‹ Top 5 Recommended Formulations (Gradient):")
        print(recommendations_gradient.head().to_string(index=False))
        
        output_path = Path('recommended_formulations_gradient.csv')
        recommendations_gradient.to_csv(output_path, index=False)
        print(f"\nâœ“ Saved to {output_path}")
    
    # ç­–ç•¥3: å¤šæ ·æ€§ä¼˜åŒ–
    print("\n" + "â”€" * 80)
    print("Strategy 3: Diversity-Aware Optimization")
    print("â”€" * 80)
    
    recommendations_diverse = optimizer.optimize_with_diversity(
        n_recommendations=10,
        diversity_weight=0.3,
        n_candidates=5000
    )
    
    print("ğŸ“‹ All Diverse Recommendations:")
    print(recommendations_diverse.to_string(index=False))
    
    output_path = Path('recommended_formulations_diverse.csv')
    recommendations_diverse.to_csv(output_path, index=False)
    print(f"\nâœ“ Saved to {output_path}")
    
    # ========================================================================
    # PART 7: å®é™…åº”ç”¨æŒ‡å—
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("PART 7: NEXT STEPS - Experimental Validation")
    print("=" * 80)
    
    print("""
    ğŸ”¬ RECOMMENDED EXPERIMENTAL WORKFLOW:
    
    1. Review Recommendations
       â†’ Check the CSV files generated above
       â†’ Focus on top 5-10 formulations with highest predicted titer
    
    2. Experimental Validation
       â†’ Prepare these NEW formulations in the lab
       â†’ Test on Clone B (or the target clone of interest)
       â†’ Measure actual titers
    
    3. Compare Predictions vs Reality
       â†’ If actual titers â‰ˆ predicted titers â†’ Model is working! âœ…
       â†’ If actual titers << predicted titers â†’ Model may be overfitting âš ï¸
       â†’ If actual titers >> predicted titers â†’ Found a surprise! ğŸ‰
    
    4. Iterative Improvement
       â†’ Add validated formulations to training data
       â†’ Retrain the model with expanded dataset
       â†’ Generate new recommendations
    
    5. Production Deployment
       â†’ Once confident (RÂ² > 0.7 on validation), use for routine optimization
       â†’ Continue monitoring and updating model periodically
    """)
    
    # ========================================================================
    # æ±‡æ€»æŠ¥å‘Š
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ğŸ“Š FINAL SUMMARY REPORT")
    print("=" * 80)
    
    print(f"""
    Source Clone (A):
      - Samples: {len(clone_A_data)}
      - TabPFN Test RÂ²: {source_metrics['test_r2']:.4f}
    
    Target Clone (B):
      - Total samples: {len(clone_B_data)}
      - Training samples: {len(target_train_indices)}
      - Testing samples: {len(target_test_indices)}
    
    Transferability:
      - Score: {similarity_metrics['transferability_score']:.4f}
      - Extrapolation rate: {similarity_metrics['extrapolation_rate']:.2%}
    
    Best Model:
      - Type: {best_regressor_type.upper()} + {best_strategy.replace('_', ' ').title()}
      - Test RÂ²: {best_r2:.4f}
      - Validation RÂ²: {validation_metrics.get('r2', 'N/A')}
    
    Recommendations Generated:
      - Random search: {len(recommendations_random)} formulations
      - Diverse set: {len(recommendations_diverse)} formulations
      - Best predicted titer: {recommendations_random.iloc[0]['Predicted_Titer']:.2f}
    
    Files Created:
      âœ“ embedding_space.png
      âœ“ prediction_performance.png
      âœ“ recommended_formulations_random.csv
      âœ“ recommended_formulations_diverse.csv
    """)
    
    print("=" * 80)
    print("âœ… WORKFLOW COMPLETED SUCCESSFULLY!")
    print("=" * 80)
    print("\nğŸ’¡ Next: Run experiments with recommended formulations\n")


if __name__ == "__main__":
    main()
```

---

## ğŸ“„ æ–‡ä»¶5: `requirements.txt`

```txt
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
scipy>=1.7.0
tabpfn>=0.1.0
tabpfn-extensions>=0.1.0
torch>=1.9.0
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œå®Œæ•´ç¤ºä¾‹

```bash
python example_complete_workflow.py
```

### 3. ä½¿ç”¨ä½ çš„çœŸå®æ•°æ®

ä¿®æ”¹ `example_complete_workflow.py` ä¸­çš„æ•°æ®åŠ è½½éƒ¨åˆ†ï¼š

```python
# æ›¿æ¢è¿™éƒ¨åˆ†
clone_A_data = pd.read_csv('your_clone_A_data.csv')
clone_B_data = pd.read_csv('your_clone_B_data.csv')
```

---

## ğŸ“Š é¢„æœŸè¾“å‡º

è¿è¡Œåå°†ç”Ÿæˆï¼š

1. **æ§åˆ¶å°è¾“å‡º**ï¼šå®Œæ•´çš„åˆ†ææŠ¥å‘Š
2. **å›¾ç‰‡æ–‡ä»¶**ï¼š
   - `embedding_space.png` - embeddingç©ºé—´å¯è§†åŒ–
   - `prediction_performance.png` - é¢„æµ‹æ•ˆæœ
3. **CSVæ–‡ä»¶**ï¼ˆâ­æ ¸å¿ƒï¼‰ï¼š
   - `recommended_formulations_random.csv` - æ¨èçš„æœªçŸ¥é…æ–¹
   - `recommended_formulations_diverse.csv` - å¤šæ ·æ€§æ¨è

---

## âœ… éªŒè¯æµç¨‹

```python
# æ‹¿åˆ°æ¨èå
recommendations = pd.read_csv('recommended_formulations_random.csv')

# é€‰æ‹©top 5æµ‹è¯•
top5 = recommendations.head(5)

# åœ¨å®éªŒå®¤é…åˆ¶è¿™5ä¸ªé…æ–¹
# åœ¨å…‹éš†Bä¸Šæµ‹è¯•
# æµ‹é‡çœŸå®titer

# å¯¹æ¯”
true_titers = [2100, 2050, 1980, 1950, 1900]  # å®éªŒç»“æœ
predicted_titers = top5['Predicted_Titer'].values

from sklearn.metrics import r2_score
r2 = r2_score(true_titers, predicted_titers)
print(f"Validation RÂ²: {r2:.4f}")

# å¦‚æœ RÂ² > 0.6 â†’ æ¨¡å‹å¯ä¿¡ï¼
```

---

è¿™å¥—å®Œæ•´ä»£ç ç°åœ¨çœŸæ­£å®ç°äº†ä½ çš„éœ€æ±‚ï¼š
1. âœ… ç”¨å°‘é‡ç›®æ ‡æ•°æ®è®­ç»ƒ
2. âœ… åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯
3. âœ… **é¢„æµ‹å¹¶æ¨èæœªçŸ¥é…æ–¹**
4. âœ… æŒ‡å¯¼ä¸‹ä¸€è½®å®éªŒ

éœ€è¦æˆ‘è§£é‡Šä»»ä½•éƒ¨åˆ†å—ï¼Ÿ