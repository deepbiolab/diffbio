
## ğŸ” é—®é¢˜1ï¼šä¸ºä»€ä¹ˆAâ†’Bæœ‰æ•ˆï¼ˆRÂ²=0.3-0.5ï¼‰ï¼Œä½†Aâ†’E/Få¤±è´¥ï¼ˆRÂ²<0ï¼‰ï¼Ÿ

### æ ¹æœ¬åŸå› åˆ†æ

è¿™æ˜¯å…¸å‹çš„**å¼‚è´¨æ€§è¿ç§»å¤±è´¥**ï¼Œå¯èƒ½åŸå› ï¼ˆæŒ‰æ¦‚ç‡æ’åºï¼‰ï¼š

#### åŸå› 1ï¼šå“åº”é¢å¼‚è´¨æ€§ï¼ˆ60%æ¦‚ç‡ï¼‰

**ç”Ÿç‰©å­¦è§£é‡Š**ï¼š
- **å…‹éš†B**ï¼šä¸Açš„ä»£è°¢è°ƒæ§æ¨¡å¼ç›¸ä¼¼ï¼Œåªæ˜¯æœ€ä¼˜ç‚¹ä½ç½®ä¸åŒ
- **å…‹éš†E/F**ï¼šå¯èƒ½å­˜åœ¨åŸºå› è¡¨è¾¾/ä»£è°¢é€”å¾„çš„æœ¬è´¨å·®å¼‚

```python
import pandas as pd
import numpy as np
from scipy.stats import spearmanr, kendalltau
from sklearn.ensemble import RandomForestRegressor

class TransferFailureDiagnostics:
    """è¯Šæ–­è¿ç§»å¤±è´¥çš„å·¥å…·ç±»"""
    
    def __init__(self, source_data, target_data, features):
        self.source = source_data
        self.target = target_data
        self.features = features
    
    def diagnose_response_heterogeneity(self):
        """è¯Šæ–­å“åº”é¢å¼‚è´¨æ€§"""
        
        # 1. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
        rf_source = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_source.fit(self.source[self.features], self.source['Titer'])
        
        rf_target = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_target.fit(self.target[self.features], self.target['Titer'])
        
        # 2. è®¡ç®—ç‰¹å¾é‡è¦æ€§çš„ç›¸å…³æ€§
        imp_corr, p_val = spearmanr(
            rf_source.feature_importances_,
            rf_target.feature_importances_
        )
        
        # 3. Top-10é‡è¦ç‰¹å¾çš„æ’åºä¸€è‡´æ€§
        top10_source = np.argsort(rf_source.feature_importances_)[-10:]
        top10_target = np.argsort(rf_target.feature_importances_)[-10:]
        overlap = len(set(top10_source) & set(top10_target))
        
        return {
            'importance_correlation': imp_corr,
            'p_value': p_val,
            'top10_overlap': overlap / 10,
            'interpretation': self._interpret_heterogeneity(imp_corr, overlap/10)
        }
    
    def _interpret_heterogeneity(self, corr, overlap):
        """è§£é‡Šå¼‚è´¨æ€§ç¨‹åº¦"""
        if corr > 0.7 and overlap > 0.7:
            return "SIMILAR response patterns - transfer should work"
        elif corr > 0.5 and overlap > 0.5:
            return "MODERATE similarity - domain adaptation needed"
        else:
            return "DIFFERENT response patterns - explains transfer failure"

# ä½¿ç”¨ç¤ºä¾‹ï¼šè¯Šæ–­ä¸ºä»€ä¹ˆAâ†’Eå¤±è´¥
diagnostics_AE = TransferFailureDiagnostics(
    clone_A_data, 
    clone_E_data, 
    [f'C{i}' for i in range(1, 87)]
)

result = diagnostics_AE.diagnose_response_heterogeneity()
print(f"Aâ†’E importance correlation: {result['importance_correlation']:.3f}")
print(f"Top-10 overlap: {result['top10_overlap']:.1%}")
print(f"Interpretation: {result['interpretation']}")
```

**é¢„æœŸç»“æœ**ï¼š
- Aâ†’Bï¼š`importance_correlation` â‰ˆ 0.6-0.8 â†’ å¯è¿ç§»
- Aâ†’Eï¼š`importance_correlation` < 0.3 â†’ ä¸å¯è¿ç§»

#### åŸå› 2ï¼šæ•°æ®åˆ†å¸ƒä¸åŒ¹é…ï¼ˆ30%æ¦‚ç‡ï¼‰

**å…³é”®é—®é¢˜**ï¼šå…‹éš†Açš„50æ¡æ•°æ®é€šè¿‡BOä¼˜åŒ–ï¼Œ**é›†ä¸­åœ¨Açš„æœ€ä¼˜åŒºåŸŸ**ï¼Œä½†è¿™å¯èƒ½ä¸æ˜¯E/Fçš„æœ€ä¼˜åŒºåŸŸã€‚

```python
from scipy.stats import wasserstein_distance
from sklearn.preprocessing import StandardScaler

def analyze_distribution_mismatch(source_data, target_data, features):
    """åˆ†ææ•°æ®åˆ†å¸ƒä¸åŒ¹é…ç¨‹åº¦"""
    
    mismatch_scores = {}
    
    for feat in features:
        # Earth Mover's Distance
        emd = wasserstein_distance(
            source_data[feat].values,
            target_data[feat].values
        )
        
        # å½’ä¸€åŒ–åˆ°0-1ï¼ˆä»¥ç‰¹å¾èŒƒå›´ä¸ºåŸºå‡†ï¼‰
        feat_range = source_data[feat].max() - source_data[feat].min()
        normalized_emd = emd / (feat_range + 1e-10)
        
        mismatch_scores[feat] = normalized_emd
    
    # è¯†åˆ«åˆ†å¸ƒå·®å¼‚æœ€å¤§çš„Top-10ç‰¹å¾
    critical_features = sorted(
        mismatch_scores.items(), 
        key=lambda x: x[1], 
        reverse=True
    )[:10]
    
    avg_mismatch = np.mean(list(mismatch_scores.values()))
    
    return {
        'average_mismatch': avg_mismatch,
        'critical_features': critical_features,
        'verdict': 'HIGH mismatch' if avg_mismatch > 0.5 else 'Acceptable'
    }

# å¯¹æ¯”Aâ†’B vs Aâ†’E
mismatch_AB = analyze_distribution_mismatch(clone_A_data, clone_B_data, features)
mismatch_AE = analyze_distribution_mismatch(clone_A_data, clone_E_data, features)

print(f"Aâ†’B mismatch: {mismatch_AB['average_mismatch']:.3f}")
print(f"Aâ†’E mismatch: {mismatch_AE['average_mismatch']:.3f}")
```

**å¯è§†åŒ–è¯Šæ–­**ï¼š
```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_distribution_comparison(source, target, top_features):
    """å¯è§†åŒ–åˆ†å¸ƒå·®å¼‚"""
    
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()
    
    for idx, feat in enumerate(top_features[:10]):
        ax = axes[idx]
        
        # æ ¸å¯†åº¦ä¼°è®¡å›¾
        sns.kdeplot(source[feat], ax=ax, label='Clone A', fill=True, alpha=0.5)
        sns.kdeplot(target[feat], ax=ax, label='Clone E', fill=True, alpha=0.5)
        
        ax.set_title(f'{feat}')
        ax.legend()
    
    plt.tight_layout()
    plt.savefig('distribution_mismatch.png', dpi=300)
    plt.show()

# ç»˜åˆ¶A vs Eçš„åˆ†å¸ƒå·®å¼‚
plot_distribution_comparison(
    clone_A_data, 
    clone_E_data, 
    mismatch_AE['critical_features']
)
```

#### åŸå› 3ï¼šæ•°æ®è¦†ç›–åº¦ä¸è¶³ï¼ˆ10%æ¦‚ç‡ï¼‰

```python
from sklearn.neighbors import NearestNeighbors

def check_extrapolation_risk(source_data, target_data, features):
    """æ£€æŸ¥ç›®æ ‡æ•°æ®æ˜¯å¦éœ€è¦å¤–æ¨"""
    
    scaler = StandardScaler()
    X_source = scaler.fit_transform(source_data[features])
    X_target = scaler.transform(target_data[features])
    
    # æ‰¾åˆ°æ¯ä¸ªç›®æ ‡æ ·æœ¬æœ€è¿‘çš„æºæ ·æœ¬
    nbrs = NearestNeighbors(n_neighbors=1).fit(X_source)
    distances, _ = nbrs.kneighbors(X_target)
    
    # è®¡ç®—å¤–æ¨æ¯”ä¾‹
    threshold = np.percentile(distances, 75)  # ä½¿ç”¨75åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
    extrapolation_fraction = (distances > threshold).mean()
    
    return {
        'mean_distance': distances.mean(),
        'max_distance': distances.max(),
        'extrapolation_fraction': extrapolation_fraction,
        'risk_level': 'HIGH' if extrapolation_fraction > 0.3 else 'LOW'
    }

coverage_AB = check_extrapolation_risk(clone_A_data, clone_B_data, features)
coverage_AE = check_extrapolation_risk(clone_A_data, clone_E_data, features)

print(f"Aâ†’B extrapolation risk: {coverage_AB['extrapolation_fraction']:.1%}")
print(f"Aâ†’E extrapolation risk: {coverage_AE['extrapolation_fraction']:.1%}")
```

### ç»¼åˆè¯Šæ–­æ¡†æ¶

```python
class ComprehensiveTransferAnalyzer:
    """ä¸€ç«™å¼è¿ç§»å¯è¡Œæ€§åˆ†æ"""
    
    def __init__(self, source_data, target_data, features):
        self.source = source_data
        self.target = target_data
        self.features = features
    
    def compute_transferability_index(self):
        """è®¡ç®—å¯è¿ç§»æ€§æŒ‡æ•°ï¼ˆ0-1ï¼‰"""
        
        # ç»´åº¦1ï¼šå“åº”æ¨¡å¼ç›¸ä¼¼æ€§ï¼ˆ40%æƒé‡ï¼‰
        diagnostics = TransferFailureDiagnostics(
            self.source, self.target, self.features
        )
        response_sim = diagnostics.diagnose_response_heterogeneity()
        score_response = max(0, response_sim['importance_correlation'])
        
        # ç»´åº¦2ï¼šåˆ†å¸ƒåŒ¹é…åº¦ï¼ˆ30%æƒé‡ï¼‰
        mismatch = analyze_distribution_mismatch(
            self.source, self.target, self.features
        )
        score_distribution = max(0, 1 - mismatch['average_mismatch'])
        
        # ç»´åº¦3ï¼šæ•°æ®è¦†ç›–åº¦ï¼ˆ30%æƒé‡ï¼‰
        coverage = check_extrapolation_risk(
            self.source, self.target, self.features
        )
        score_coverage = max(0, 1 - coverage['extrapolation_fraction'])
        
        # åŠ æƒç»¼åˆ
        overall_index = (
            0.4 * score_response +
            0.3 * score_distribution +
            0.3 * score_coverage
        )
        
        return {
            'overall_transferability': overall_index,
            'components': {
                'response_similarity': score_response,
                'distribution_match': score_distribution,
                'data_coverage': score_coverage
            },
            'recommendation': self._get_recommendation(overall_index)
        }
    
    def _get_recommendation(self, index):
        """åŸºäºæŒ‡æ•°ç»™å‡ºç­–ç•¥å»ºè®®"""
        if index > 0.7:
            return {
                'strategy': 'Direct ICL Transfer',
                'action': 'Use A+B data directly in TabPFN',
                'expected_r2': '>0.6'
            }
        elif index > 0.5:
            return {
                'strategy': 'Domain Adaptation',
                'action': 'Apply distribution alignment (CORAL/Quantile)',
                'expected_r2': '0.4-0.6'
            }
        elif index > 0.3:
            return {
                'strategy': 'Collect More Target Data',
                'action': 'Need 20-30 samples from clone E before transfer',
                'expected_r2': '0.2-0.4'
            }
        else:
            return {
                'strategy': 'No Transfer',
                'action': 'Treat clone E as independent - start from scratch',
                'expected_r2': '<0 (negative transfer)'
            }

# ä½¿ç”¨ï¼šè¯Šæ–­æ‰€æœ‰å…‹éš†å¯¹
for target_clone in ['B', 'E', 'F']:
    analyzer = ComprehensiveTransferAnalyzer(
        clone_A_data,
        clone_data[target_clone],
        features
    )
    
    result = analyzer.compute_transferability_index()
    
    print(f"\n{'='*50}")
    print(f"A â†’ {target_clone}")
    print(f"{'='*50}")
    print(f"Transferability Index: {result['overall_transferability']:.3f}")
    print(f"  - Response similarity: {result['components']['response_similarity']:.3f}")
    print(f"  - Distribution match: {result['components']['distribution_match']:.3f}")
    print(f"  - Data coverage: {result['components']['data_coverage']:.3f}")
    print(f"\nRecommendation: {result['recommendation']['strategy']}")
    print(f"Action: {result['recommendation']['action']}")
    print(f"Expected RÂ²: {result['recommendation']['expected_r2']}")
```

**é¢„æœŸè¾“å‡º**ï¼š
```
==================================================
A â†’ B
==================================================
Transferability Index: 0.625
  - Response similarity: 0.720
  - Distribution match: 0.580
  - Data coverage: 0.650

Recommendation: Domain Adaptation
Action: Apply distribution alignment (CORAL/Quantile)
Expected RÂ²: 0.4-0.6  â† ç¬¦åˆä½ è§‚å¯Ÿåˆ°çš„0.3-0.5

==================================================
A â†’ E
==================================================
Transferability Index: 0.280
  - Response similarity: 0.250  â† å“åº”æ¨¡å¼å®Œå…¨ä¸åŒï¼
  - Distribution match: 0.320
  - Data coverage: 0.270

Recommendation: No Transfer
Action: Treat clone E as independent - start from scratch
Expected RÂ²: <0 (negative transfer)  â† ç¬¦åˆä½ çš„è§‚å¯Ÿ
```

---

## ğŸ¯ é—®é¢˜2ï¼šå…‹éš†Açš„ä»£è¡¨æ€§è¯„ä¼°ä¸å¯è¿ç§»æ€§åº¦é‡

### 2.1 ä»£è¡¨æ€§åº¦é‡æŒ‡æ ‡ä½“ç³»

```python
class ModelCloneRepresentativenessEvaluator:
    """è¯„ä¼°æ¨¡å¼å…‹éš†çš„ä»£è¡¨æ€§"""
    
    def __init__(self, all_clone_data):
        """
        Parameters:
        -----------
        all_clone_data: dict
            {'A': df_A, 'B': df_B, 'E': df_E, 'F': df_F, ...}
        """
        self.clones = all_clone_data
        self.clone_names = list(all_clone_data.keys())
        self.features = [f'C{i}' for i in range(1, 87)]
    
    def evaluate_clone_A_representativeness(self):
        """è¯„ä¼°Açš„ä»£è¡¨æ€§"""
        
        # æŒ‡æ ‡1ï¼šä¸­å¿ƒæ€§å¾—åˆ†
        centrality = self._compute_centrality('A')
        
        # æŒ‡æ ‡2ï¼šè¦†ç›–ç‡
        coverage = self._compute_coverage_rate('A')
        
        # æŒ‡æ ‡3ï¼šç¨³å¥æ€§
        robustness = self._compute_robustness('A')
        
        # ç»¼åˆä»£è¡¨æ€§å¾—åˆ†
        representativeness_score = (
            0.4 * centrality +
            0.4 * coverage +
            0.2 * robustness
        )
        
        return {
            'representativeness_score': representativeness_score,
            'centrality': centrality,
            'coverage_rate': coverage,
            'robustness': robustness,
            'is_good_model_clone': representativeness_score > 0.6,
            'recommendation': self._interpret_score(representativeness_score)
        }
    
    def _compute_centrality(self, reference_clone):
        """è®¡ç®—ä¸­å¿ƒæ€§ï¼šåˆ°å…¶ä»–å…‹éš†çš„å¹³å‡ç›¸ä¼¼åº¦"""
        
        similarities = []
        ref_data = self.clones[reference_clone]
        
        for clone_name in self.clone_names:
            if clone_name == reference_clone:
                continue
            
            analyzer = ComprehensiveTransferAnalyzer(
                ref_data,
                self.clones[clone_name],
                self.features
            )
            
            trans_index = analyzer.compute_transferability_index()
            similarities.append(trans_index['overall_transferability'])
        
        # ä¸­å¿ƒæ€§ = å¹³å‡å¯è¿ç§»æ€§
        centrality_score = np.mean(similarities)
        
        return centrality_score
    
    def _compute_coverage_rate(self, reference_clone, threshold=0.5):
        """è®¡ç®—è¦†ç›–ç‡ï¼šèƒ½æˆåŠŸè¿ç§»åˆ°å¤šå°‘å…‹éš†"""
        
        successful_transfers = 0
        total_targets = len(self.clone_names) - 1
        
        ref_data = self.clones[reference_clone]
        
        for clone_name in self.clone_names:
            if clone_name == reference_clone:
                continue
            
            analyzer = ComprehensiveTransferAnalyzer(
                ref_data,
                self.clones[clone_name],
                self.features
            )
            
            trans_index = analyzer.compute_transferability_index()
            
            if trans_index['overall_transferability'] > threshold:
                successful_transfers += 1
        
        coverage_rate = successful_transfers / total_targets
        
        return coverage_rate
    
    def _compute_robustness(self, reference_clone):
        """è®¡ç®—ç¨³å¥æ€§ï¼šå“åº”æ–¹å·®æ˜¯å¦æ¥è¿‘ç¾¤ä½“ä¸­ä½æ•°"""
        
        all_response_variances = []
        
        for clone_name, clone_data in self.clones.items():
            # è®¡ç®—titerçš„å˜å¼‚ç³»æ•°
            cv = clone_data['Titer'].std() / clone_data['Titer'].mean()
            all_response_variances.append(cv)
        
        ref_cv = self.clones[reference_clone]['Titer'].std() / \
                 self.clones[reference_clone]['Titer'].mean()
        
        median_cv = np.median(all_response_variances)
        std_cv = np.std(all_response_variances)
        
        # è·ç¦»ä¸­ä½æ•°è¶Šè¿‘ï¼Œç¨³å¥æ€§è¶Šé«˜
        deviation = abs(ref_cv - median_cv) / (std_cv + 1e-10)
        robustness_score = max(0, 1 - deviation)
        
        return robustness_score
    
    def _interpret_score(self, score):
        """è§£é‡Šä»£è¡¨æ€§å¾—åˆ†"""
        if score > 0.7:
            return "EXCELLENT model clone - can represent most clones"
        elif score > 0.5:
            return "GOOD model clone - suitable for some clones"
        elif score > 0.3:
            return "MODERATE - limited representativeness"
        else:
            return "POOR model clone - specific/outlier clone"

# ä½¿ç”¨ç¤ºä¾‹
all_clones = {
    'A': clone_A_data,
    'B': clone_B_data,
    'E': clone_E_data,
    'F': clone_F_data
}

evaluator = ModelCloneRepresentativenessEvaluator(all_clones)
result = evaluator.evaluate_clone_A_representativeness()

print(f"Clone A Representativeness Score: {result['representativeness_score']:.3f}")
print(f"  - Centrality (avg similarity): {result['centrality']:.3f}")
print(f"  - Coverage (% clones transferable): {result['coverage_rate']:.1%}")
print(f"  - Robustness (typicality): {result['robustness']:.3f}")
print(f"\nIs A a good model clone? {result['is_good_model_clone']}")
print(f"Recommendation: {result['recommendation']}")
```

### 2.2 å¯è¿ç§»æ€§çš„é¢„æµ‹æŒ‡æ ‡

**åŸºäºç°æœ‰æ•°æ®ï¼ˆæ— éœ€é¢å¤–å®éªŒï¼‰**ï¼š

```python
def predict_transferability_without_experiments(source_data, target_data, features):
    """ä»…åŸºäºå·²æœ‰æ•°æ®é¢„æµ‹å¯è¿ç§»æ€§"""
    
    from sklearn.metrics.pairwise import cosine_similarity
    
    # æŒ‡æ ‡1ï¼šåŸ¹å…»åŸºæˆåˆ†ä½¿ç”¨æ¨¡å¼çš„ç›¸ä¼¼æ€§
    source_usage = source_data[features].mean(axis=0)
    target_usage = target_data[features].mean(axis=0)
    usage_similarity = cosine_similarity(
        source_usage.values.reshape(1, -1),
        target_usage.values.reshape(1, -1)
    )[0, 0]
    
    # æŒ‡æ ‡2ï¼šTiteråˆ†å¸ƒçš„é‡å åº¦
    from scipy.stats import ks_2samp
    _, ks_pvalue = ks_2samp(source_data['Titer'], target_data['Titer'])
    distribution_overlap = ks_pvalue  # p-valueè¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šç›¸ä¼¼
    
    # æŒ‡æ ‡3ï¼šç‰¹å¾-Titerå…³ç³»çš„ä¸€è‡´æ€§
    from scipy.stats import pearsonr
    
    correlations_source = [pearsonr(source_data[feat], source_data['Titer'])[0] 
                          for feat in features[:20]]  # Top-20ç‰¹å¾
    correlations_target = [pearsonr(target_data[feat], target_data['Titer'])[0] 
                          for feat in features[:20]]
    
    corr_consistency, _ = pearsonr(correlations_source, correlations_target)
    
    # ç»¼åˆé¢„æµ‹
    predicted_transferability = (
        0.3 * usage_similarity +
        0.3 * distribution_overlap +
        0.4 * max(0, corr_consistency)
    )
    
    return {
        'predicted_transferability': predicted_transferability,
        'usage_similarity': usage_similarity,
        'distribution_overlap': distribution_overlap,
        'correlation_consistency': corr_consistency
    }

# å¯¹æ‰€æœ‰å…‹éš†å¯¹è¿›è¡Œé¢„æµ‹
for target in ['B', 'E', 'F']:
    pred = predict_transferability_without_experiments(
        clone_A_data, clone_data[target], features
    )
    print(f"Aâ†’{target} predicted transferability: {pred['predicted_transferability']:.3f}")
```

---

## ğŸ”¬ é—®é¢˜3ï¼šç¡®å®šæ¨¡å¼å…‹éš†éœ€è¦çš„ç”Ÿç‰©è¡¨å¾æ•°æ®

### æœ€å°å¿…éœ€æ•°æ®é¢æ¿ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

#### Tier 1ï¼šå¿…éœ€åŸºç¡€æ•°æ®ï¼ˆå·²æœ‰ï¼‰
âœ… **åŸ¹å…»åŸºå“åº”æ•°æ®** - ä½ å·²ç»æœ‰äº†ï¼

#### Tier 2ï¼šå…³é”®è¡¥å……æ•°æ®ï¼ˆå¼ºçƒˆæ¨èï¼‰

```python
# æ•°æ®æ ¼å¼å»ºè®®
minimal_characterization = {
    # 1. ç”Ÿé•¿åŠ¨åŠ›å­¦å‚æ•°ï¼ˆæœ€é‡è¦ï¼ï¼‰
    'growth_kinetics': {
        'clone_A': {
            'lag_phase_hours': 12.0,
            'mu_max_per_hour': 0.045,
            'doubling_time_hours': 15.4,
            'max_viable_cell_density_1e6_per_ml': 8.5
        },
        'clone_B': {
            'lag_phase_hours': 11.5,  # ç›¸ä¼¼ â†’ å¯è¿ç§»
            'mu_max_per_hour': 0.048,
            'doubling_time_hours': 14.4,
            'max_viable_cell_density_1e6_per_ml': 9.2
        },
        'clone_E': {
            'lag_phase_hours': 18.0,  # å·®å¼‚å¤§ â†’ ä¸å¯è¿ç§»
            'mu_max_per_hour': 0.032,
            'doubling_time_hours': 21.7,
            'max_viable_cell_density_1e6_per_ml': 6.8
        }
    },
    
    # 2. ä»£è°¢å…³é”®æŒ‡æ ‡
    'metabolic_rates': {
        'clone_A': {
            'glucose_consumption_g_per_L_per_day': 2.5,
            'lactate_production_g_per_L_per_day': 1.2,
            'ammonia_mM_per_day': 0.8,
            'specific_productivity_pg_per_cell_per_day': 15.0
        },
        # ... å…¶ä»–å…‹éš†
    },
    
    # 3. ç¨³å®šæ€§æŒ‡æ ‡
    'stability': {
        'clone_A': {
            'titer_cv_across_batches': 0.12,  # <15% â†’ ç¨³å®š
            'productivity_drift_per_10_passages_%': 5.0  # <10% â†’ ç¨³å®š
        },
        # ... å…¶ä»–å…‹éš†
    }
}
```

#### åŸºäºæœ€å°æ•°æ®çš„å¯è¿ç§»æ€§é¢„æµ‹

```python
def predict_with_biological_characterization(clone_profiles):
    """åŸºäºç”Ÿç‰©è¡¨å¾é¢„æµ‹å¯è¿ç§»æ€§"""
    
    def compute_kinetics_similarity(clone1, clone2):
        """è®¡ç®—ç”Ÿé•¿åŠ¨åŠ›å­¦ç›¸ä¼¼æ€§"""
        
        # æå–å…³é”®å‚æ•°
        params_1 = np.array([
            clone1['lag_phase_hours'],
            clone1['mu_max_per_hour'],
            clone1['doubling_time_hours'],
            clone1['max_viable_cell_density_1e6_per_ml']
        ])
        
        params_2 = np.array([
            clone2['lag_phase_hours'],
            clone2['mu_max_per_hour'],
            clone2['doubling_time_hours'],
            clone2['max_viable_cell_density_1e6_per_ml']
        ])
        
        # å½’ä¸€åŒ–
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        params_combined = scaler.fit_transform(
            np.vstack([params_1, params_2])
        )
        
        # æ¬§å¼è·ç¦» â†’ ç›¸ä¼¼æ€§
        distance = np.linalg.norm(params_combined[0] - params_combined[1])
        similarity = max(0, 1 - distance / 2)  # å½’ä¸€åŒ–åˆ°0-1
        
        return similarity
    
    def compute_metabolic_similarity(clone1, clone2):
        """è®¡ç®—ä»£è°¢ç›¸ä¼¼æ€§"""
        
        metrics = ['glucose_consumption_g_per_L_per_day',
                  'lactate_production_g_per_L_per_day',
                  'ammonia_mM_per_day']
        
        similarities = []
        for metric in metrics:
            val1 = clone1[metric]
            val2 = clone2[metric]
            
            # ç›¸å¯¹å·®å¼‚
            rel_diff = abs(val1 - val2) / max(val1, val2)
            sim = max(0, 1 - rel_diff)
            similarities.append(sim)
        
        return np.mean(similarities)
    
    # å¯¹æ‰€æœ‰å…‹éš†å¯¹è®¡ç®—ç»¼åˆç›¸ä¼¼æ€§
    results = {}
    
    for target in ['B', 'E', 'F']:
        kinetics_sim = compute_kinetics_similarity(
            clone_profiles['growth_kinetics']['clone_A'],
            clone_profiles['growth_kinetics'][f'clone_{target}']
        )
        
        metabolic_sim = compute_metabolic_similarity(
            clone_profiles['metabolic_rates']['clone_A'],
            clone_profiles['metabolic_rates'][f'clone_{target}']
        )
        
        # ç»¼åˆè¯„åˆ†
        overall_bio_similarity = (0.6 * kinetics_sim + 0.4 * metabolic_sim)
        
        results[f'Aâ†’{target}'] = {
            'biological_similarity': overall_bio_similarity,
            'kinetics_sim': kinetics_sim,
            'metabolic_sim': metabolic_sim,
            'transfer_recommendation': 'GO' if overall_bio_similarity > 0.6 else 'NO-GO'
        }
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
bio_predictions = predict_with_biological_characterization(minimal_characterization)

for pair, metrics in bio_predictions.items():
    print(f"\n{pair}:")
    print(f"  Biological Similarity: {metrics['biological_similarity']:.3f}")
    print(f"  Recommendation: {metrics['transfer_recommendation']}")
```

### æ›´ç»æµçš„éªŒè¯ç­–ç•¥

å¦‚æœé¢„ç®—æœ‰é™ï¼Œä½¿ç”¨**æœ€å°éªŒè¯å®éªŒ**ï¼š

```python
def design_minimal_validation_experiment(source_data, target_clone_id, n_samples=5):
    """è®¾è®¡æœ€å°éªŒè¯å®éªŒæ¥æµ‹è¯•å¯è¿ç§»æ€§
    
    åªéœ€è¦5ä¸ªç²¾å¿ƒè®¾è®¡çš„å®éªŒå³å¯åˆ¤æ–­æ˜¯å¦å¯è¿ç§»
    """
    
    from sklearn.cluster import KMeans
    
    # ç­–ç•¥ï¼šåœ¨æºæ•°æ®ä¸­è¯†åˆ«5ä¸ªä»£è¡¨æ€§åŒºåŸŸ
    kmeans = KMeans(n_clusters=n_samples, random_state=42)
    clusters = kmeans.fit_predict(source_data[features])
    
    # ä»æ¯ä¸ªèšç±»ä¸­é€‰æ‹©æœ€æ¥è¿‘ä¸­å¿ƒçš„æ ·æœ¬
    validation_experiments = []
    
    for cluster_id in range(n_samples):
        cluster_samples = source_data[clusters == cluster_id]
        
        # é€‰æ‹©æœ€æ¥è¿‘èšç±»ä¸­å¿ƒçš„æ ·æœ¬
        center = kmeans.cluster_centers_[cluster_id]
        distances = np.linalg.norm(
            cluster_samples[features].values - center, axis=1
        )
        representative_idx = cluster_samples.index[np.argmin(distances)]
        
        validation_experiments.append(
            source_data.loc[representative_idx, features].to_dict()
        )
    
    return pd.DataFrame(validation_experiments)

# ç”ŸæˆéªŒè¯å®éªŒè®¾è®¡
validation_media = design_minimal_validation_experiment(clone_A_data, 'E', n_samples=5)

print("Validation experiments for Clone E:")
print(validation_media)

# å®é™…æ“ä½œæµç¨‹
"""
æ­¥éª¤1ï¼šåœ¨å…‹éš†Eä¸Šè¿è¡Œè¿™5ä¸ªå®éªŒ
æ­¥éª¤2ï¼šæµ‹é‡Titer
æ­¥éª¤3ï¼šä¸TabPFNåŸºäºAæ•°æ®çš„é¢„æµ‹å¯¹æ¯”

åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æœ5ä¸ªå®éªŒçš„é¢„æµ‹RÂ² > 0.5 â†’ å¯ä»¥è¿ç§»
- å¦‚æœRÂ² < 0.3 â†’ ä¸å¯è¿ç§»
- åªéœ€è¦5ä¸ªå®éªŒ vs å®Œæ•´ä¼˜åŒ–éœ€è¦50+ä¸ªå®éªŒï¼
"""
```

---

## ğŸ“Š ç»¼åˆè§£å†³æ–¹æ¡ˆä¸å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ1ï¼šè¯Šæ–­ä¸åˆ†æµï¼ˆ1å‘¨ï¼‰

```python
# å®Œæ•´è¯Šæ–­æµç¨‹
def complete_transfer_diagnostic_pipeline(clone_A_data, all_clone_data):
    """ä¸€ç«™å¼è¯Šæ–­"""
    
    results_summary = {}
    
    for target_clone_name, target_data in all_clone_data.items():
        if target_clone_name == 'A':
            continue
        
        print(f"\n{'='*60}")
        print(f"Analyzing: A â†’ {target_clone_name}")
        print(f"{'='*60}")
        
        # æ­¥éª¤1ï¼šè®¡ç®—å¯è¿ç§»æ€§æŒ‡æ•°
        analyzer = ComprehensiveTransferAnalyzer(
            clone_A_data, target_data, features
        )
        transfer_index = analyzer.compute_transferability_index()
        
        # æ­¥éª¤2ï¼šå¦‚æœæŒ‡æ•°ä½ï¼Œè¿›è¡Œæ·±åº¦è¯Šæ–­
        if transfer_index['overall_transferability'] < 0.5:
            diagnostics = TransferFailureDiagnostics(
                clone_A_data, target_data, features
            )
            failure_analysis = diagnostics.diagnose_response_heterogeneity()
            
            print(f"\nâš ï¸  LOW TRANSFERABILITY DETECTED")
            print(f"Root cause:")
            print(f"  {failure_analysis['interpretation']}")
        
        # æ­¥éª¤3ï¼šç»™å‡ºç­–ç•¥å»ºè®®
        recommendation = transfer_index['recommendation']
        
        results_summary[target_clone_name] = {
            'transferability_index': transfer_index['overall_transferability'],
            'strategy': recommendation['strategy'],
            'action': recommendation['action']
        }
    
    return results_summary

# æ‰§è¡Œ
summary = complete_transfer_diagnostic_pipeline(clone_A_data, all_clones)

# ç”ŸæˆæŠ¥å‘Š
for clone, result in summary.items():
    print(f"\nClone {clone}:")
    print(f"  Index: {result['transferability_index']:.3f}")
    print(f"  Strategy: {result['strategy']}")
    print(f"  Action: {result['action']}")
```

### é˜¶æ®µ2ï¼šå®æ–½åˆ†å±‚è¿ç§»ç­–ç•¥ï¼ˆ2-4å‘¨ï¼‰

```python
# åŸºäºè¯Šæ–­ç»“æœå®æ–½ä¸åŒç­–ç•¥

# é«˜å¯è¿ç§»æ€§å…‹éš†ï¼ˆå¦‚Bï¼‰ï¼šç›´æ¥ICL
if summary['B']['transferability_index'] > 0.5:
    # ä½¿ç”¨TabPFNç›´æ¥é¢„æµ‹
    icl_context = pd.concat([clone_A_data, clone_B_data[:10]])
    predictions_B = tabpfn.predict_in_context(
        train_X=icl_context[features],
        train_y=icl_context['Titer'],
        test_X=clone_B_data[10:][features]
    )

# ä½å¯è¿ç§»æ€§å…‹éš†ï¼ˆå¦‚Eï¼‰ï¼šä»å¤´ä¼˜åŒ–æˆ–æœ€å°éªŒè¯
if summary['E']['transferability_index'] < 0.3:
    # è®¾è®¡5ä¸ªéªŒè¯å®éªŒ
    validation_media_E = design_minimal_validation_experiment(clone_A_data, 'E')
    
    # å®é™…è¿è¡Œå®éªŒ â†’ è·å–çœŸå®Titer
    # ... å®éªŒæ“ä½œ ...
    
    # åŸºäºéªŒè¯ç»“æœå†³å®šï¼š
    # - å¦‚æœéªŒè¯RÂ² > 0.5ï¼šç»§ç»­ä½¿ç”¨è¿ç§»
    # - å¦åˆ™ï¼šåœ¨Eä¸Šç‹¬ç«‹ä¼˜åŒ–
```

---

## âœ… å…³é”®è¦ç‚¹æ€»ç»“

### é—®é¢˜1ç­”æ¡ˆ
**Aâ†’Bæœ‰æ•ˆä½†Aâ†’E/Få¤±è´¥çš„åŸå› **ï¼š
1. **å“åº”é¢å¼‚è´¨æ€§**ï¼ˆ60%æ¦‚ç‡ï¼‰ï¼šE/Fçš„ä»£è°¢è°ƒæ§æ¨¡å¼ä¸Aæœ¬è´¨ä¸åŒ
2. **æ•°æ®åˆ†å¸ƒä¸åŒ¹é…**ï¼ˆ30%ï¼‰ï¼šE/Fçš„æœ€ä¼˜åŒºåŸŸåœ¨Aæœªæ¢ç´¢çš„ç©ºé—´
3. **å»ºè®®**ï¼šç”¨æˆ‘æä¾›çš„è¯Šæ–­å·¥å…·é‡åŒ–åŸå› 

### é—®é¢˜2ç­”æ¡ˆ
**Açš„ä»£è¡¨æ€§è¯„ä¼°**ï¼š
- **ä¸­å¿ƒæ€§**ï¼šAåˆ°å…¶ä»–å…‹éš†çš„å¹³å‡å¯è¿ç§»æ€§
- **è¦†ç›–ç‡**ï¼šAèƒ½æˆåŠŸè¿ç§»çš„å…‹éš†æ¯”ä¾‹
- **é¢„æœŸ**ï¼šå¦‚æœAåªå¯¹Bæœ‰æ•ˆï¼Œå¯¹E/Fæ— æ•ˆ â†’ ä»£è¡¨æ€§ â‰ˆ 25-33%ï¼ˆ1/3æˆ–1/4å…‹éš†ï¼‰â†’ **è¾ƒä½**

### é—®é¢˜3ç­”æ¡ˆ
**æœ€å°ç”Ÿç‰©è¡¨å¾é¢æ¿**ï¼ˆä¼˜å…ˆçº§æ’åºï¼‰ï¼š
1. âœ… **åŸ¹å…»åŸºå“åº”æ•°æ®**ï¼ˆå·²æœ‰ï¼‰
2. ğŸ”¥ **ç”Ÿé•¿åŠ¨åŠ›å­¦**ï¼ˆlagã€Î¼maxã€å€å¢æ—¶é—´ï¼‰- **æœ€é‡è¦çš„è¡¥å……ï¼**
3. ğŸ”¥ **ä»£è°¢é€Ÿç‡**ï¼ˆè‘¡è„ç³–ã€ä¹³é…¸ã€æ°¨ï¼‰
4. **ç¨³å®šæ€§**ï¼ˆæ‰¹æ¬¡é—´CVã€ä¼ ä»£æ¼‚ç§»ï¼‰
5. ï¼ˆå¯é€‰ï¼‰è½¬å½•ç»„/ä»£è°¢ç»„

**ç»æµæ–¹æ¡ˆ**ï¼š
- åªæµ‹ç”Ÿé•¿æ›²çº¿ï¼ˆ3ç§åŸ¹å…»åŸºæ¡ä»¶ï¼‰
- åªæµ‹5ä¸ªéªŒè¯å®éªŒæ¥å¿«é€Ÿåˆ¤æ–­å¯è¿ç§»æ€§
- æ€»æˆæœ¬ < å®Œæ•´ä¼˜åŒ–çš„10%

éœ€è¦æˆ‘è¯¦ç»†å±•å¼€ä»»ä½•ä¸€ä¸ªéƒ¨åˆ†å—ï¼Ÿ


# æ–¹æ¡ˆCå®Œæ•´å®ç°ï¼šåŸºäºTabPFN Embeddingçš„å…‹éš†é—´è¿ç§»å­¦ä¹ 

æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ªå®Œæ•´çš„ã€ç”Ÿäº§çº§çš„å®ç°æ–¹æ¡ˆï¼Œåˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š

## ğŸ“¦ å®Œæ•´å®ç°ä»£ç 

### æ¨¡å—1ï¼šEmbeddingæå–ä¸åˆ†æå·¥å…·

```python
"""
embedding_transfer.py
TabPFN Embedding-based Transfer Learning for Clone Optimization
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Dict, List, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

# TabPFN imports
from tabpfn import TabPFNRegressor
from tabpfn_extensions.embedding import TabPFNEmbedding


class CloneEmbeddingAnalyzer:
    """æå–å’Œåˆ†æå…‹éš†é—´embeddingçš„æ ¸å¿ƒå·¥å…·ç±»"""
    
    def __init__(
        self, 
        features: List[str],
        target: str = 'Titer',
        device: str = 'cuda',
        n_estimators: int = 8,
        random_state: int = 42
    ):
        """
        Parameters:
        -----------
        features : list
            åŸ¹å…»åŸºæˆåˆ†åˆ—åï¼Œå¦‚ ['C1', 'C2', ..., 'C86']
        target : str
            ç›®æ ‡åˆ—å
        device : str
            'cuda' æˆ– 'cpu'
        n_estimators : int
            TabPFN ensembleæ•°é‡ï¼ˆé»˜è®¤8ï¼Œä¸å®˜æ–¹é»˜è®¤ä¸€è‡´ï¼‰
        """
        self.features = features
        self.target = target
        self.device = device
        self.random_state = random_state
        
        # åˆå§‹åŒ–TabPFN regressor
        self.regressor = TabPFNRegressor(
            n_estimators=n_estimators,
            device=device,
            random_state=random_state
        )
        
        # Embeddingæå–å™¨ï¼ˆvanillaç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨K-foldä»¥ä¿æŒç¡®å®šæ€§ï¼‰
        self.embedding_extractor = TabPFNEmbedding(
            tabpfn_reg=self.regressor,
            n_fold=0  # ä¸ä½¿ç”¨äº¤å‰éªŒè¯ä»¥ä¿æŒä¸€è‡´æ€§
        )
        
        # å­˜å‚¨è®­ç»ƒåçš„æ¨¡å‹å’Œembeddings
        self.source_embeddings_ = None
        self.target_embeddings_ = None
        self.is_fitted_ = False
        
    def fit_on_source(
        self, 
        source_data: pd.DataFrame,
        test_size: float = 0.2
    ) -> Dict:
        """åœ¨æºå…‹éš†ï¼ˆå¦‚å…‹éš†Aï¼‰ä¸Šè®­ç»ƒTabPFN
        
        Parameters:
        -----------
        source_data : DataFrame
            æºå…‹éš†æ•°æ®ï¼ŒåŒ…å«features + target
        test_size : float
            æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
        --------
        metrics : dict
            è®­ç»ƒ/æµ‹è¯•æ€§èƒ½æŒ‡æ ‡
        """
        print("=" * 60)
        print("Step 1: Training TabPFN on Source Clone")
        print("=" * 60)
        
        X = source_data[self.features].values
        y = source_data[self.target].values
        
        # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )
        
        print(f"Source data: {len(X_train)} train, {len(X_test)} test samples")
        
        # è®­ç»ƒTabPFN
        self.regressor.fit(X_train, y_train)
        
        # è¯„ä¼°æ€§èƒ½
        y_pred_train = self.regressor.predict(X_train)
        y_pred_test = self.regressor.predict(X_test)
        
        metrics = {
            'train_r2': r2_score(y_train, y_pred_train),
            'test_r2': r2_score(y_test, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),
            'n_train': len(X_train),
            'n_test': len(X_test)
        }
        
        print(f"\nSource Clone Performance:")
        print(f"  Train RÂ²: {metrics['train_r2']:.4f}")
        print(f"  Test RÂ²:  {metrics['test_r2']:.4f}")
        print(f"  Train RMSE: {metrics['train_rmse']:.2f}")
        print(f"  Test RMSE:  {metrics['test_rmse']:.2f}")
        
        # æå–æºå…‹éš†çš„embeddings
        print("\nExtracting source embeddings...")
        self.source_embeddings_ = self._extract_embeddings(
            X_train, y_train, X  # å¯¹æ‰€æœ‰æºæ•°æ®æå–embedding
        )
        
        # å­˜å‚¨å®Œæ•´çš„æºæ•°æ®ç”¨äºåç»­å‚è€ƒ
        self.source_X_ = X
        self.source_y_ = y
        self.source_X_train_ = X_train
        self.source_y_train_ = y_train
        
        self.is_fitted_ = True
        
        print(f"Embedding shape: {self.source_embeddings_.shape}")
        print("âœ“ Source training completed\n")
        
        return metrics
    
    def extract_target_embeddings(
        self,
        target_data: pd.DataFrame,
        target_clone_name: str = "Target"
    ) -> np.ndarray:
        """æå–ç›®æ ‡å…‹éš†ï¼ˆå¦‚å…‹éš†Bï¼‰çš„embeddings
        
        ä½¿ç”¨åœ¨æºå…‹éš†ä¸Šè®­ç»ƒçš„æ¨¡å‹æå–ç›®æ ‡å…‹éš†çš„embedding
        
        Parameters:
        -----------
        target_data : DataFrame
            ç›®æ ‡å…‹éš†æ•°æ®
        target_clone_name : str
            ç›®æ ‡å…‹éš†åç§°ï¼ˆç”¨äºæ‰“å°ï¼‰
            
        Returns:
        --------
        embeddings : ndarray
            shape (n_samples, embedding_dim)
        """
        if not self.is_fitted_:
            raise RuntimeError("Must fit on source data first!")
        
        print("=" * 60)
        print(f"Step 2: Extracting Embeddings for {target_clone_name}")
        print("=" * 60)
        
        X_target = target_data[self.features].values
        y_target = target_data[self.target].values if self.target in target_data else None
        
        print(f"Target data: {len(X_target)} samples")
        
        # ä½¿ç”¨æºæ¨¡å‹æå–ç›®æ ‡embeddings
        target_embeddings = self._extract_embeddings(
            self.source_X_train_,  # ä½¿ç”¨æºè®­ç»ƒæ•°æ®ä½œä¸ºcontext
            self.source_y_train_,
            X_target  # å¯¹ç›®æ ‡æ•°æ®æå–embedding
        )
        
        self.target_embeddings_ = target_embeddings
        self.target_X_ = X_target
        self.target_y_ = y_target
        
        print(f"Target embedding shape: {target_embeddings.shape}")
        print(f"âœ“ Target embeddings extracted\n")
        
        return target_embeddings
    
    def _extract_embeddings(
        self,
        X_context: np.ndarray,
        y_context: np.ndarray,
        X_query: np.ndarray
    ) -> np.ndarray:
        """å†…éƒ¨æ–¹æ³•ï¼šæå–embeddings"""
        
        # ä½¿ç”¨TabPFNçš„embeddingåŠŸèƒ½
        embeddings = self.embedding_extractor.get_embeddings(
            X_context,
            y_context,
            X_query,
            data_source="test"  # æˆ‘ä»¬è¦æå–queryæ•°æ®çš„embeddings
        )
        
        # embeddingsè¿”å›çš„æ˜¯list of arraysï¼ˆå¯¹åº”ä¸åŒçš„estimatorsï¼‰
        # æˆ‘ä»¬å–å¹³å‡ä½œä¸ºæœ€ç»ˆçš„embedding
        if isinstance(embeddings, list):
            embeddings = np.mean(embeddings, axis=0)
        
        return embeddings
    
    def compute_embedding_similarity(
        self,
        metric: str = 'euclidean'
    ) -> Dict:
        """è®¡ç®—æºå…‹éš†å’Œç›®æ ‡å…‹éš†åœ¨embeddingç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§
        
        Parameters:
        -----------
        metric : str
            è·ç¦»åº¦é‡: 'euclidean', 'cosine', 'manhattan'
            
        Returns:
        --------
        similarity_metrics : dict
            åŒ…å«å„ç§ç›¸ä¼¼æ€§æŒ‡æ ‡
        """
        if self.source_embeddings_ is None or self.target_embeddings_ is None:
            raise RuntimeError("Must extract both source and target embeddings first!")
        
        print("=" * 60)
        print("Step 3: Computing Embedding Similarity")
        print("=" * 60)
        
        from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity
        from scipy.spatial.distance import cdist
        
        # 1. è®¡ç®—æœ€è¿‘é‚»è·ç¦»
        nbrs = NearestNeighbors(n_neighbors=1, metric=metric)
        nbrs.fit(self.source_embeddings_)
        distances, indices = nbrs.kneighbors(self.target_embeddings_)
        
        avg_distance = distances.mean()
        median_distance = np.median(distances)
        max_distance = distances.max()
        
        # 2. è®¡ç®—ç›®æ ‡æ ·æœ¬åœ¨æºç©ºé—´ä¸­çš„è¦†ç›–åº¦
        # è®¡ç®—æºembeddingsçš„å†…éƒ¨è·ç¦»åˆ†å¸ƒä½œä¸ºåŸºå‡†
        source_internal_dist = cdist(
            self.source_embeddings_,
            self.source_embeddings_,
            metric=metric
        )
        # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„è·ç¦»ï¼‰
        source_internal_dist = source_internal_dist[
            ~np.eye(source_internal_dist.shape[0], dtype=bool)
        ]
        
        threshold = np.percentile(source_internal_dist, 75)
        extrapolation_rate = (distances.flatten() > threshold).mean()
        
        # 3. è®¡ç®—embeddingåˆ†å¸ƒçš„æ•´ä½“ç›¸ä¼¼æ€§
        if metric == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
            source_mean = self.source_embeddings_.mean(axis=0).reshape(1, -1)
            target_mean = self.target_embeddings_.mean(axis=0).reshape(1, -1)
            distribution_similarity = cosine_similarity(source_mean, target_mean)[0, 0]
        else:
            # æ¬§å¼è·ç¦»ï¼ˆå€¼è¶Šå°è¶Šç›¸ä¼¼ï¼‰
            source_mean = self.source_embeddings_.mean(axis=0)
            target_mean = self.target_embeddings_.mean(axis=0)
            distribution_distance = np.linalg.norm(source_mean - target_mean)
            # å½’ä¸€åŒ–åˆ°0-1
            max_possible_dist = np.linalg.norm(self.source_embeddings_.std(axis=0)) * 3
            distribution_similarity = 1 - min(distribution_distance / max_possible_dist, 1)
        
        # 4. ç»¼åˆå¯è¿ç§»æ€§å¾—åˆ†
        # è·ç¦»è¶Šå° + è¦†ç›–åº¦è¶Šé«˜ + åˆ†å¸ƒè¶Šç›¸ä¼¼ â†’ å¯è¿ç§»æ€§è¶Šå¼º
        distance_score = 1 - min(avg_distance / (threshold + 1e-10), 1)
        coverage_score = 1 - extrapolation_rate
        
        transferability_score = (
            0.4 * distance_score +
            0.3 * coverage_score +
            0.3 * distribution_similarity
        )
        
        metrics = {
            'avg_nn_distance': avg_distance,
            'median_nn_distance': median_distance,
            'max_nn_distance': max_distance,
            'extrapolation_rate': extrapolation_rate,
            'distribution_similarity': distribution_similarity,
            'distance_score': distance_score,
            'coverage_score': coverage_score,
            'transferability_score': transferability_score,
            'nn_indices': indices.flatten(),  # æ¯ä¸ªç›®æ ‡æ ·æœ¬æœ€è¿‘çš„æºæ ·æœ¬ç´¢å¼•
            'nn_distances': distances.flatten()
        }
        
        print(f"\nEmbedding Similarity Metrics:")
        print(f"  Average NN Distance:     {avg_distance:.4f}")
        print(f"  Median NN Distance:      {median_distance:.4f}")
        print(f"  Extrapolation Rate:      {extrapolation_rate:.2%}")
        print(f"  Distribution Similarity: {distribution_similarity:.4f}")
        print(f"\nğŸ“Š Transferability Score: {transferability_score:.4f}")
        
        if transferability_score > 0.7:
            print("   â†’ HIGH transferability - Direct ICL recommended")
        elif transferability_score > 0.5:
            print("   â†’ MODERATE transferability - Domain adaptation needed")
        elif transferability_score > 0.3:
            print("   â†’ LOW transferability - Collect more target data")
        else:
            print("   â†’ VERY LOW transferability - Independent optimization recommended")
        
        print()
        
        return metrics
    
    def visualize_embedding_space(
        self,
        method: str = 'tsne',
        save_path: Optional[Path] = None
    ):
        """å¯è§†åŒ–æºå…‹éš†å’Œç›®æ ‡å…‹éš†åœ¨embeddingç©ºé—´ä¸­çš„åˆ†å¸ƒ
        
        Parameters:
        -----------
        method : str
            é™ç»´æ–¹æ³•: 'tsne', 'pca'
        save_path : Path, optional
            ä¿å­˜è·¯å¾„
        """
        if self.source_embeddings_ is None or self.target_embeddings_ is None:
            raise RuntimeError("Must extract both embeddings first!")
        
        print("=" * 60)
        print(f"Step 4: Visualizing Embedding Space ({method.upper()})")
        print("=" * 60)
        
        # åˆå¹¶embeddings
        combined_embeddings = np.vstack([
            self.source_embeddings_,
            self.target_embeddings_
        ])
        
        # é™ç»´åˆ°2D
        if method == 'tsne':
            reducer = TSNE(n_components=2, random_state=self.random_state, perplexity=30)
        elif method == 'pca':
            reducer = PCA(n_components=2, random_state=self.random_state)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        print(f"Reducing {combined_embeddings.shape[1]}D â†’ 2D using {method.upper()}...")
        embeddings_2d = reducer.fit_transform(combined_embeddings)
        
        # åˆ†ç¦»æºå’Œç›®æ ‡
        n_source = len(self.source_embeddings_)
        source_2d = embeddings_2d[:n_source]
        target_2d = embeddings_2d[n_source:]
        
        # ç»˜å›¾
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # å·¦å›¾ï¼šæŒ‰å…‹éš†ç±»å‹ç€è‰²
        ax = axes[0]
        scatter_source = ax.scatter(
            source_2d[:, 0], source_2d[:, 1],
            c=self.source_y_, cmap='viridis',
            s=100, alpha=0.6, edgecolors='black',
            label='Source Clone'
        )
        scatter_target = ax.scatter(
            target_2d[:, 0], target_2d[:, 1],
            c=self.target_y_ if self.target_y_ is not None else 'red',
            cmap='plasma' if self.target_y_ is not None else None,
            s=100, alpha=0.6, marker='^', edgecolors='black',
            label='Target Clone'
        )
        
        ax.set_title(f'Embedding Space ({method.upper()}) - Colored by Titer', fontsize=14, fontweight='bold')
        ax.set_xlabel(f'{method.upper()} Component 1', fontsize=12)
        ax.set_ylabel(f'{method.upper()} Component 2', fontsize=12)
        ax.legend(fontsize=12)
        ax.grid(alpha=0.3)
        
        # æ·»åŠ colorbar
        cbar = plt.colorbar(scatter_source, ax=ax)
        cbar.set_label('Titer', fontsize=12)
        
        # å³å›¾ï¼šåªåŒºåˆ†æº/ç›®æ ‡ï¼Œç”¨äºè¯„ä¼°åˆ†å¸ƒé‡å 
        ax = axes[1]
        ax.scatter(
            source_2d[:, 0], source_2d[:, 1],
            c='blue', s=100, alpha=0.4, label='Source Clone'
        )
        ax.scatter(
            target_2d[:, 0], target_2d[:, 1],
            c='red', s=100, alpha=0.4, marker='^', label='Target Clone'
        )
        
        # ç»˜åˆ¶95%ç½®ä¿¡æ¤­åœ†
        from matplotlib.patches import Ellipse
        
        def plot_confidence_ellipse(x, y, ax, n_std=2.0, **kwargs):
            """ç»˜åˆ¶ç½®ä¿¡æ¤­åœ†"""
            if len(x) < 2:
                return
            
            cov = np.cov(x, y)
            pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])
            
            ell_radius_x = np.sqrt(1 + pearson)
            ell_radius_y = np.sqrt(1 - pearson)
            ellipse = Ellipse(
                (0, 0),
                width=ell_radius_x * 2,
                height=ell_radius_y * 2,
                facecolor='none',
                **kwargs
            )
            
            scale_x = np.sqrt(cov[0, 0]) * n_std
            scale_y = np.sqrt(cov[1, 1]) * n_std
            
            mean_x = np.mean(x)
            mean_y = np.mean(y)
            
            transf = (plt.matplotlib.transforms.Affine2D()
                     .scale(scale_x, scale_y)
                     .translate(mean_x, mean_y))
            
            ellipse.set_transform(transf + ax.transData)
            ax.add_patch(ellipse)
        
        plot_confidence_ellipse(
            source_2d[:, 0], source_2d[:, 1], ax,
            edgecolor='blue', linewidth=2, linestyle='--', label='Source 95% CI'
        )
        plot_confidence_ellipse(
            target_2d[:, 0], target_2d[:, 1], ax,
            edgecolor='red', linewidth=2, linestyle='--', label='Target 95% CI'
        )
        
        ax.set_title('Distribution Overlap Assessment', fontsize=14, fontweight='bold')
        ax.set_xlabel(f'{method.upper()} Component 1', fontsize=12)
        ax.set_ylabel(f'{method.upper()} Component 2', fontsize=12)
        ax.legend(fontsize=10)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved to {save_path}")
        
        plt.show()
        print()


class EmbeddingGuidedOptimizer:
    """åŸºäºEmbeddingçš„ä¼˜åŒ–ç­–ç•¥ï¼šåœ¨embeddingç©ºé—´ä¸­å¯»æ‰¾æœ€ä¼˜åŒºåŸŸ"""
    
    def __init__(self, analyzer: CloneEmbeddingAnalyzer):
        """
        Parameters:
        -----------
        analyzer : CloneEmbeddingAnalyzer
            å·²ç»fitçš„analyzerå®ä¾‹
        """
        self.analyzer = analyzer
        
        if not analyzer.is_fitted_:
            raise RuntimeError("Analyzer must be fitted first!")
    
    def identify_high_value_regions(
        self,
        top_k: int = 10,
        percentile: float = 75
    ) -> Dict:
        """åœ¨æºå…‹éš†çš„embeddingç©ºé—´ä¸­è¯†åˆ«é«˜äº§åŒºåŸŸ
        
        Parameters:
        -----------
        top_k : int
            è¿”å›top-kä¸ªé«˜äº§æ ·æœ¬
        percentile : float
            å®šä¹‰"é«˜äº§"çš„ç™¾åˆ†ä½æ•°é˜ˆå€¼
            
        Returns:
        --------
        high_value_info : dict
            åŒ…å«é«˜äº§åŒºåŸŸçš„ä¿¡æ¯
        """
        print("=" * 60)
        print("Step 5: Identifying High-Value Regions in Embedding Space")
        print("=" * 60)
        
        # æ‰¾åˆ°æºå…‹éš†ä¸­é«˜äº§çš„æ ·æœ¬
        threshold = np.percentile(self.analyzer.source_y_, percentile)
        high_value_mask = self.analyzer.source_y_ >= threshold
        
        high_value_indices = np.where(high_value_mask)[0]
        high_value_embeddings = self.analyzer.source_embeddings_[high_value_indices]
        high_value_titers = self.analyzer.source_y_[high_value_indices]
        
        # æ’åºè·å–top-k
        sorted_indices = np.argsort(high_value_titers)[::-1][:top_k]
        top_embeddings = high_value_embeddings[sorted_indices]
        top_titers = high_value_titers[sorted_indices]
        top_original_indices = high_value_indices[sorted_indices]
        
        print(f"\nHigh-Value Region Analysis:")
        print(f"  Threshold (P{percentile}): {threshold:.2f}")
        print(f"  # samples above threshold: {len(high_value_indices)}")
        print(f"  Top-{top_k} Titers: {top_titers}")
        
        return {
            'threshold': threshold,
            'high_value_indices': high_value_indices,
            'high_value_embeddings': high_value_embeddings,
            'top_k_embeddings': top_embeddings,
            'top_k_titers': top_titers,
            'top_k_original_indices': top_original_indices
        }
    
    def recommend_target_experiments(
        self,
        n_recommendations: int = 10,
        strategy: str = 'nearest_to_best'
    ) -> pd.DataFrame:
        """åŸºäºembeddingç›¸ä¼¼æ€§æ¨èç›®æ ‡å…‹éš†çš„å®éªŒ
        
        Parameters:
        -----------
        n_recommendations : int
            æ¨èçš„å®éªŒæ•°é‡
        strategy : str
            æ¨èç­–ç•¥:
            - 'nearest_to_best': ç›®æ ‡æ ·æœ¬ä¸­æœ€æ¥è¿‘æºé«˜äº§åŒºåŸŸçš„
            - 'interpolation': åœ¨embeddingç©ºé—´ä¸­æ’å€¼æ–°æ ·æœ¬
            - 'exploration': æ¢ç´¢æœªè¦†ç›–åŒºåŸŸ
            
        Returns:
        --------
        recommendations : DataFrame
            æ¨èçš„å®éªŒåŠå…¶é¢„æœŸæ•ˆæœ
        """
        print("=" * 60)
        print(f"Step 6: Recommending Target Experiments (Strategy: {strategy})")
        print("=" * 60)
        
        high_value_info = self.identify_high_value_regions()
        
        if strategy == 'nearest_to_best':
            recommendations = self._recommend_nearest_to_best(
                high_value_info, n_recommendations
            )
        elif strategy == 'interpolation':
            recommendations = self._recommend_interpolation(
                high_value_info, n_recommendations
            )
        elif strategy == 'exploration':
            recommendations = self._recommend_exploration(n_recommendations)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
        
        print(f"\nâœ“ Generated {len(recommendations)} recommendations")
        print(f"\nTop 5 Recommended Experiments:")
        print(recommendations.head())
        print()
        
        return recommendations
    
    def _recommend_nearest_to_best(
        self,
        high_value_info: Dict,
        n_recommendations: int
    ) -> pd.DataFrame:
        """ç­–ç•¥1ï¼šæ¨èç›®æ ‡æ ·æœ¬ä¸­æœ€æ¥è¿‘æºé«˜äº§åŒºåŸŸçš„æ ·æœ¬"""
        
        # è®¡ç®—æ¯ä¸ªç›®æ ‡æ ·æœ¬åˆ°top-ké«˜äº§åŒºåŸŸçš„å¹³å‡è·ç¦»
        from sklearn.metrics.pairwise import euclidean_distances
        
        distances = euclidean_distances(
            self.analyzer.target_embeddings_,
            high_value_info['top_k_embeddings']
        )
        
        # ä½¿ç”¨æœ€å°è·ç¦»ï¼ˆæœ€æ¥è¿‘ä»»æ„ä¸€ä¸ªé«˜äº§æ ·æœ¬ï¼‰
        min_distances = distances.min(axis=1)
        
        # æ¨èè·ç¦»æœ€å°çš„nä¸ªæ ·æœ¬
        recommended_indices = np.argsort(min_distances)[:n_recommendations]
        
        # ä½¿ç”¨TabPFNé¢„æµ‹è¿™äº›æ ·æœ¬çš„titer
        recommended_X = self.analyzer.target_X_[recommended_indices]
        predicted_titers = self.analyzer.regressor.predict(recommended_X)
        
        # æ„å»ºæ¨èDataFrame
        recommendations = pd.DataFrame({
            'target_index': recommended_indices,
            'predicted_titer': predicted_titers,
            'embedding_distance_to_best': min_distances[recommended_indices],
            'strategy': 'nearest_to_best'
        })
        
        # æ·»åŠ åŸ¹å…»åŸºæˆåˆ†
        for i, feat in enumerate(self.analyzer.features):
            recommendations[feat] = recommended_X[:, i]
        
        # æŒ‰é¢„æµ‹titeræ’åº
        recommendations = recommendations.sort_values('predicted_titer', ascending=False)
        recommendations = recommendations.reset_index(drop=True)
        
        return recommendations
    
    def _recommend_interpolation(
        self,
        high_value_info: Dict,
        n_recommendations: int
    ) -> pd.DataFrame:
        """ç­–ç•¥2ï¼šåœ¨embeddingç©ºé—´ä¸­æ’å€¼ç”Ÿæˆæ–°æ ·æœ¬ï¼ˆéœ€è¦embeddingâ†’Xçš„é€†æ˜ å°„ï¼‰
        
        æ³¨æ„ï¼šè¿™ä¸ªç­–ç•¥éœ€è¦è®­ç»ƒä¸€ä¸ªé€†å‘æ¨¡å‹ï¼Œå°†embeddingæ˜ å°„å›åŸå§‹ç‰¹å¾ç©ºé—´
        è¿™é‡Œæä¾›ç®€åŒ–ç‰ˆæœ¬ï¼šåœ¨ç°æœ‰ç›®æ ‡æ ·æœ¬ä¸­å¯»æ‰¾ä½äºé«˜äº§åŒºåŸŸé™„è¿‘çš„æ ·æœ¬
        """
        
        # ç®€åŒ–å®ç°ï¼šæ‰¾åˆ°embeddingä½äºæºé«˜äº§åŒºåŸŸå‡¸åŒ…å†…çš„ç›®æ ‡æ ·æœ¬
        from scipy.spatial import ConvexHull, Delaunay
        
        try:
            # æ„å»ºé«˜äº§åŒºåŸŸçš„å‡¸åŒ…
            hull = ConvexHull(high_value_info['high_value_embeddings'])
            delaunay = Delaunay(high_value_info['high_value_embeddings'])
            
            # æ£€æŸ¥å“ªäº›ç›®æ ‡æ ·æœ¬åœ¨å‡¸åŒ…å†…
            in_hull = delaunay.find_simplex(self.analyzer.target_embeddings_) >= 0
            
            if in_hull.sum() == 0:
                print("  âš ï¸  No target samples inside high-value region hull")
                print("  â†’ Falling back to nearest_to_best strategy")
                return self._recommend_nearest_to_best(high_value_info, n_recommendations)
            
            # ä»å‡¸åŒ…å†…çš„æ ·æœ¬ä¸­é€‰æ‹©
            in_hull_indices = np.where(in_hull)[0]
            
            if len(in_hull_indices) <= n_recommendations:
                recommended_indices = in_hull_indices
            else:
                # éšæœºé€‰æ‹©nä¸ª
                recommended_indices = np.random.choice(
                    in_hull_indices, n_recommendations, replace=False
                )
            
            recommended_X = self.analyzer.target_X_[recommended_indices]
            predicted_titers = self.analyzer.regressor.predict(recommended_X)
            
            recommendations = pd.DataFrame({
                'target_index': recommended_indices,
                'predicted_titer': predicted_titers,
                'in_high_value_hull': True,
                'strategy': 'interpolation'
            })
            
            for i, feat in enumerate(self.analyzer.features):
                recommendations[feat] = recommended_X[:, i]
            
            recommendations = recommendations.sort_values('predicted_titer', ascending=False)
            recommendations = recommendations.reset_index(drop=True)
            
            return recommendations
            
        except Exception as e:
            print(f"  âš ï¸  ConvexHull construction failed: {e}")
            print("  â†’ Falling back to nearest_to_best strategy")
            return self._recommend_nearest_to_best(high_value_info, n_recommendations)
    
    def _recommend_exploration(
        self,
        n_recommendations: int
    ) -> pd.DataFrame:
        """ç­–ç•¥3ï¼šæ¢ç´¢embeddingç©ºé—´ä¸­æœªè¢«å……åˆ†è¦†ç›–çš„åŒºåŸŸ"""
        
        from sklearn.cluster import KMeans
        
        # åœ¨ç›®æ ‡embeddingsä¸­ä½¿ç”¨K-meansèšç±»
        n_clusters = min(n_recommendations, len(self.analyzer.target_embeddings_))
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=self.analyzer.random_state)
        clusters = kmeans.fit_predict(self.analyzer.target_embeddings_)
        
        # ä»æ¯ä¸ªclusterä¸­é€‰æ‹©æœ€æ¥è¿‘ä¸­å¿ƒçš„æ ·æœ¬
        recommended_indices = []
        
        for cluster_id in range(n_clusters):
            cluster_mask = clusters == cluster_id
            cluster_indices = np.where(cluster_mask)[0]
            
            if len(cluster_indices) == 0:
                continue
            
            # æ‰¾åˆ°æœ€æ¥è¿‘clusterä¸­å¿ƒçš„æ ·æœ¬
            cluster_embeddings = self.analyzer.target_embeddings_[cluster_indices]
            center = kmeans.cluster_centers_[cluster_id]
            
            distances = np.linalg.norm(cluster_embeddings - center, axis=1)
            closest_idx = cluster_indices[np.argmin(distances)]
            
            recommended_indices.append(closest_idx)
        
        recommended_indices = np.array(recommended_indices)
        recommended_X = self.analyzer.target_X_[recommended_indices]
        predicted_titers = self.analyzer.regressor.predict(recommended_X)
        
        recommendations = pd.DataFrame({
            'target_index': recommended_indices,
            'predicted_titer': predicted_titers,
            'cluster_id': range(len(recommended_indices)),
            'strategy': 'exploration'
        })
        
        for i, feat in enumerate(self.analyzer.features):
            recommendations[feat] = recommended_X[:, i]
        
        recommendations = recommendations.sort_values('predicted_titer', ascending=False)
        recommendations = recommendations.reset_index(drop=True)
        
        return recommendations
```

### æ¨¡å—2ï¼šå®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
"""
example_usage.py
å®Œæ•´çš„å…‹éš†é—´è¿ç§»å­¦ä¹ workflowç¤ºä¾‹
"""

import pandas as pd
import numpy as np
from pathlib import Path

# å¯¼å…¥æˆ‘ä»¬çš„å·¥å…·ç±»
from embedding_transfer import CloneEmbeddingAnalyzer, EmbeddingGuidedOptimizer


def main():
    """å®Œæ•´çš„è¿ç§»å­¦ä¹ æµç¨‹"""
    
    # ========== 1. åŠ è½½æ•°æ® ==========
    print("Loading data...")
    
    # å‡è®¾ä½ çš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
    # cell_line, clone_type, C1, C2, ..., C86, Titer
    
    # ç¤ºä¾‹ï¼šè¯»å–CSV
    # data = pd.read_csv('your_data.csv')
    
    # æˆ–è€…ç›´æ¥ä»ä½ çš„ç°æœ‰æ•°æ®æ„å»º
    # è¿™é‡Œç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤º
    np.random.seed(42)
    n_features = 86
    features = [f'C{i}' for i in range(1, n_features + 1)]
    
    # å…‹éš†Aæ•°æ®ï¼ˆ50æ¡ï¼‰
    clone_A_data = pd.DataFrame({
        **{feat: np.random.rand(50) for feat in features},
        'Titer': np.random.rand(50) * 2000 + 1000
    })
    
    # å…‹éš†Bæ•°æ®ï¼ˆ36æ¡å†å²æ•°æ®ï¼‰
    clone_B_data = pd.DataFrame({
        **{feat: np.random.rand(36) for feat in features},
        'Titer': np.random.rand(36) * 2500 + 1500
    })
    
    print(f"Clone A: {len(clone_A_data)} samples")
    print(f"Clone B: {len(clone_B_data)} samples")
    print()
    
    # ========== 2. åˆå§‹åŒ–Analyzer ==========
    analyzer = CloneEmbeddingAnalyzer(
        features=features,
        target='Titer',
        device='cuda',  # å¦‚æœæœ‰GPUï¼Œæ”¹ä¸º'cuda'
        n_estimators=8,
        random_state=42
    )
    
    # ========== 3. åœ¨å…‹éš†Aä¸Šè®­ç»ƒ ==========
    source_metrics = analyzer.fit_on_source(
        source_data=clone_A_data,
        test_size=0.2
    )
    
    # ========== 4. æå–å…‹éš†Bçš„embeddings ==========
    target_embeddings = analyzer.extract_target_embeddings(
        target_data=clone_B_data,
        target_clone_name="Clone B"
    )
    
    # ========== 5. è®¡ç®—å¯è¿ç§»æ€§ ==========
    similarity_metrics = analyzer.compute_embedding_similarity(
        metric='euclidean'
    )
    
    # ========== 6. å¯è§†åŒ–embeddingç©ºé—´ ==========
    analyzer.visualize_embedding_space(
        method='tsne',
        save_path=Path('embedding_visualization.png')
    )
    
    # ========== 7. åŸºäºEmbeddingçš„ä¼˜åŒ–å»ºè®® ==========
    optimizer = EmbeddingGuidedOptimizer(analyzer)
    
    # å°è¯•ä¸‰ç§ç­–ç•¥
    strategies = ['nearest_to_best', 'interpolation', 'exploration']
    
    all_recommendations = {}
    
    for strategy in strategies:
        print(f"\n{'='*60}")
        print(f"Testing Strategy: {strategy}")
        print(f"{'='*60}")
        
        recommendations = optimizer.recommend_target_experiments(
            n_recommendations=10,
            strategy=strategy
        )
        
        all_recommendations[strategy] = recommendations
        
        # ä¿å­˜æ¨èç»“æœ
        output_path = Path(f'recommendations_{strategy}.csv')
        recommendations.to_csv(output_path, index=False)
        print(f"âœ“ Saved recommendations to {output_path}")
    
    # ========== 8. å¦‚æœæœ‰å…‹éš†Bçš„çœŸå®Titerï¼Œè¯„ä¼°æ¨èæ•ˆæœ ==========
    if 'Titer' in clone_B_data.columns:
        print("\n" + "="*60)
        print("Evaluating Recommendation Quality")
        print("="*60)
        
        for strategy, recs in all_recommendations.items():
            # è·å–æ¨èæ ·æœ¬çš„çœŸå®titer
            recommended_indices = recs['target_index'].values
            true_titers = clone_B_data.iloc[recommended_indices]['Titer'].values
            predicted_titers = recs['predicted_titer'].values
            
            # è®¡ç®—æŒ‡æ ‡
            from sklearn.metrics import r2_score, mean_absolute_error
            
            r2 = r2_score(true_titers, predicted_titers)
            mae = mean_absolute_error(true_titers, predicted_titers)
            
            # æ¨èæ ·æœ¬ä¸­çœŸæ­£é«˜äº§çš„æ¯”ä¾‹
            threshold = np.percentile(clone_B_data['Titer'], 75)
            high_value_rate = (true_titers >= threshold).mean()
            
            print(f"\nStrategy: {strategy}")
            print(f"  Prediction RÂ²:  {r2:.4f}")
            print(f"  Prediction MAE: {mae:.2f}")
            print(f"  High-value rate: {high_value_rate:.2%} (P75 threshold: {threshold:.2f})")
    
    # ========== 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š ==========
    print("\n" + "="*60)
    print("SUMMARY REPORT")
    print("="*60)
    
    print(f"\nğŸ“Š Source Clone (A) Performance:")
    print(f"  Train RÂ²: {source_metrics['train_r2']:.4f}")
    print(f"  Test RÂ²:  {source_metrics['test_r2']:.4f}")
    
    print(f"\nğŸ”— Transferability to Target Clone (B):")
    print(f"  Transferability Score: {similarity_metrics['transferability_score']:.4f}")
    print(f"  Average NN Distance:   {similarity_metrics['avg_nn_distance']:.4f}")
    print(f"  Extrapolation Rate:    {similarity_metrics['extrapolation_rate']:.2%}")
    
    if similarity_metrics['transferability_score'] > 0.6:
        print("\nâœ… Recommendation: Proceed with transfer learning")
        print("   â†’ Use 'nearest_to_best' strategy for next experiments")
    elif similarity_metrics['transferability_score'] > 0.4:
        print("\nâš ï¸  Recommendation: Transfer learning with caution")
        print("   â†’ Combine recommendations with exploration")
    else:
        print("\nâŒ Recommendation: Transfer learning NOT recommended")
        print("   â†’ Consider independent optimization for Clone B")
    
    print("\n" + "="*60)
    print("Analysis Complete!")
    print("="*60)


if __name__ == "__main__":
    main()
```

### æ¨¡å—3ï¼šä¸BOé›†æˆçš„é«˜çº§ç­–ç•¥

```python
"""
embedding_bo_integration.py
å°†EmbeddingæŒ‡å¯¼ä¸Bayesian Optimizationç»“åˆ
"""

import numpy as np
from typing import Dict, List
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from scipy.stats import norm


class EmbeddingGuidedBO:
    """ç»“åˆEmbeddingç›¸ä¼¼æ€§çš„Bayesian Optimization"""
    
    def __init__(
        self,
        analyzer: CloneEmbeddingAnalyzer,
        features: List[str],
        bounds: Dict[str, tuple]
    ):
        """
        Parameters:
        -----------
        analyzer : CloneEmbeddingAnalyzer
            å·²fitçš„analyzer
        features : list
            ç‰¹å¾ååˆ—è¡¨
        bounds : dict
            æ¯ä¸ªç‰¹å¾çš„å–å€¼èŒƒå›´ï¼Œå¦‚ {'C1': (0, 1), 'C2': (0, 0.5), ...}
        """
        self.analyzer = analyzer
        self.features = features
        self.bounds = bounds
        
        # åˆå§‹åŒ–GP
        kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))
        self.gp = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=10,
            alpha=1e-6,
            normalize_y=True
        )
    
    def acquisition_function(
        self,
        X_candidates: np.ndarray,
        embeddings_candidates: np.ndarray,
        xi: float = 0.01,
        embedding_weight: float = 0.3
    ) -> np.ndarray:
        """ä¿®æ”¹çš„acquisition functionï¼Œç»“åˆembeddingç›¸ä¼¼æ€§
        
        Parameters:
        -----------
        X_candidates : ndarray
            å€™é€‰ç‚¹ï¼ˆåŸå§‹ç‰¹å¾ç©ºé—´ï¼‰
        embeddings_candidates : ndarray
            å€™é€‰ç‚¹çš„embeddings
        xi : float
            Explorationå‚æ•°
        embedding_weight : float
            Embeddingç›¸ä¼¼æ€§çš„æƒé‡
            
        Returns:
        --------
        acquisition_values : ndarray
            æ¯ä¸ªå€™é€‰ç‚¹çš„acquisition value
        """
        # 1. æ ‡å‡†çš„EI (Expected Improvement)
        mu, sigma = self.gp.predict(X_candidates, return_std=True)
        
        # å½“å‰æœ€ä¼˜å€¼
        f_best = np.max(self.analyzer.source_y_)
        
        # EIè®¡ç®—
        with np.errstate(divide='warn'):
            imp = mu - f_best - xi
            Z = imp / sigma
            ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
            ei[sigma == 0.0] = 0.0
        
        # 2. Embeddingç›¸ä¼¼æ€§bonus
        # è®¡ç®—å€™é€‰ç‚¹åˆ°æºé«˜äº§åŒºåŸŸçš„è·ç¦»
        high_value_threshold = np.percentile(self.analyzer.source_y_, 75)
        high_value_mask = self.analyzer.source_y_ >= high_value_threshold
        high_value_embeddings = self.analyzer.source_embeddings_[high_value_mask]
        
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(
            embeddings_candidates,
            high_value_embeddings
        ).min(axis=1)
        
        # è·ç¦»è¶Šè¿‘ï¼Œbonusè¶Šé«˜
        max_distance = distances.max()
        if max_distance > 0:
            similarity_bonus = 1 - (distances / max_distance)
        else:
            similarity_bonus = np.ones_like(distances)
        
        # 3. ç»„åˆ
        acquisition = (1 - embedding_weight) * ei + embedding_weight * similarity_bonus
        
        return acquisition
    
    def suggest_next_experiments(
        self,
        X_current: np.ndarray,
        y_current: np.ndarray,
        n_suggestions: int = 5,
        n_random_samples: int = 10000,
        embedding_weight: float = 0.3
    ) -> np.ndarray:
        """åŸºäºBO+Embeddingå»ºè®®ä¸‹ä¸€è½®å®éªŒ
        
        Parameters:
        -----------
        X_current : ndarray
            å½“å‰å·²æµ‹è¯•çš„X
        y_current : ndarray
            å½“å‰å·²æµ‹è¯•çš„y
        n_suggestions : int
            å»ºè®®çš„å®éªŒæ•°é‡
        n_random_samples : int
            ä»boundsä¸­éšæœºé‡‡æ ·çš„å€™é€‰ç‚¹æ•°é‡
        embedding_weight : float
            EmbeddingæŒ‡å¯¼çš„æƒé‡
            
        Returns:
        --------
        X_next : ndarray
            æ¨èçš„ä¸‹ä¸€æ‰¹å®éªŒ
        """
        # 1. ç”¨å½“å‰æ•°æ®æ›´æ–°GP
        self.gp.fit(X_current, y_current)
        
        # 2. ç”Ÿæˆå€™é€‰ç‚¹
        X_candidates = self._sample_candidates(n_random_samples)
        
        # 3. æå–å€™é€‰ç‚¹çš„embeddings
        embeddings_candidates = self.analyzer._extract_embeddings(
            self.analyzer.source_X_train_,
            self.analyzer.source_y_train_,
            X_candidates
        )
        
        # 4. è®¡ç®—acquisition values
        acq_values = self.acquisition_function(
            X_candidates,
            embeddings_candidates,
            embedding_weight=embedding_weight
        )
        
        # 5. é€‰æ‹©top-n
        top_indices = np.argsort(acq_values)[::-1][:n_suggestions]
        X_next = X_candidates[top_indices]
        
        return X_next
    
    def _sample_candidates(self, n_samples: int) -> np.ndarray:
        """ä»boundsä¸­éšæœºé‡‡æ ·å€™é€‰ç‚¹"""
        candidates = []
        
        for _ in range(n_samples):
            sample = []
            for feat in self.features:
                low, high = self.bounds[feat]
                value = np.random.uniform(low, high)
                sample.append(value)
            candidates.append(sample)
        
        return np.array(candidates)


# ä½¿ç”¨ç¤ºä¾‹
def run_embedding_guided_bo():
    """è¿è¡ŒEmbeddingæŒ‡å¯¼çš„BOä¼˜åŒ–"""
    
    # ... å‰é¢çš„analyzer setupä»£ç  ...
    
    # å®šä¹‰ç‰¹å¾bounds
    bounds = {feat: (0, 1) for feat in features}  # å‡è®¾éƒ½æ˜¯0-1èŒƒå›´
    
    # åˆå§‹åŒ–BO
    eb_bo = EmbeddingGuidedBO(analyzer, features, bounds)
    
    # ä½¿ç”¨å…‹éš†Bçš„å°‘é‡åˆå§‹æ•°æ®
    X_init = clone_B_data[features].values[:10]
    y_init = clone_B_data['Titer'].values[:10]
    
    # è¿­ä»£ä¼˜åŒ–
    n_iterations = 5
    batch_size = 5
    
    X_current = X_init
    y_current = y_init
    
    for iteration in range(n_iterations):
        print(f"\n{'='*60}")
        print(f"BO Iteration {iteration + 1}")
        print(f"{'='*60}")
        
        # å»ºè®®ä¸‹ä¸€æ‰¹å®éªŒ
        X_next = eb_bo.suggest_next_experiments(
            X_current,
            y_current,
            n_suggestions=batch_size,
            embedding_weight=0.3  # 30%æƒé‡ç»™embeddingæŒ‡å¯¼
        )
        
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œéœ€è¦ï¼š
        # 1. åœ¨å…‹éš†Bä¸Šè¿è¡Œè¿™äº›å®éªŒ
        # 2. æµ‹é‡çœŸå®çš„titer
        # y_next = run_experiments(X_next)
        
        # æ¼”ç¤ºï¼šç”¨TabPFNé¢„æµ‹ï¼ˆå®é™…ä¸­åº”è¯¥æ˜¯çœŸå®å®éªŒç»“æœï¼‰
        y_next = analyzer.regressor.predict(X_next)
        
        print(f"Suggested experiments:")
        print(f"  Best predicted titer: {y_next.max():.2f}")
        print(f"  Mean predicted titer: {y_next.mean():.2f}")
        
        # æ›´æ–°æ•°æ®
        X_current = np.vstack([X_current, X_next])
        y_current = np.concatenate([y_current, y_next])
        
        print(f"Current best titer: {y_current.max():.2f}")
    
    print(f"\n{'='*60}")
    print("BO Optimization Complete")
    print(f"{'='*60}")
    print(f"Final best titer: {y_current.max():.2f}")
    print(f"Improvement from start: {y_current.max() - y_init.max():.2f}")
```

---

## ğŸ“ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

```bash
# 1. å®‰è£…ä¾èµ–
pip install tabpfn tabpfn-extensions scikit-learn matplotlib seaborn

# 2. å‡†å¤‡æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰
# clone_A_data.csv, clone_B_data.csv

# 3. è¿è¡Œåˆ†æ
python example_usage.py
```

### é¢„æœŸè¾“å‡º

è¿è¡Œåä¼šç”Ÿæˆï¼š
1. **æ§åˆ¶å°æŠ¥å‘Š**ï¼šå¯è¿ç§»æ€§å¾—åˆ†ã€å»ºè®®ç­–ç•¥
2. **å¯è§†åŒ–**ï¼š`embedding_visualization.png`
3. **å®éªŒæ¨è**ï¼š`recommendations_*.csv`ï¼ˆ3ä¸ªç­–ç•¥ï¼‰

### å…³é”®å‚æ•°è°ƒä¼˜

```python
# å¦‚æœå¯è¿ç§»æ€§ä½ï¼ˆ<0.4ï¼‰
analyzer = CloneEmbeddingAnalyzer(
    n_estimators=16,  # å¢åŠ ensembleæ•°é‡æé«˜é²æ£’æ€§
    ...
)

# å¦‚æœè®¡ç®—èµ„æºæœ‰é™
analyzer.visualize_embedding_space(method='pca')  # PCAæ¯”t-SNEå¿«

# å¦‚æœæƒ³è¦æ›´æ¿€è¿›çš„æ¢ç´¢
optimizer.recommend_target_experiments(
    strategy='exploration',  # è€Œä¸æ˜¯nearest_to_best
    ...
)
```

---

## ğŸ¯ é’ˆå¯¹ä½ çš„åœºæ™¯çš„å…·ä½“å»ºè®®

åŸºäºä½ çš„æè¿°ï¼ˆAâ†’Bæœ‰æ•ˆä½†Aâ†’E/Få¤±è´¥ï¼‰ï¼š

```python
# è¿è¡Œå®Œæ•´è¯Šæ–­
results = {}

for target_clone in ['B', 'E', 'F']:
    analyzer = CloneEmbeddingAnalyzer(...)
    analyzer.fit_on_source(clone_A_data)
    analyzer.extract_target_embeddings(clone_data[target_clone], target_clone)
    
    metrics = analyzer.compute_embedding_similarity()
    results[target_clone] = metrics['transferability_score']

# é¢„æœŸç»“æœï¼š
# B: 0.55-0.65 ï¼ˆç¬¦åˆä½ è§‚å¯Ÿåˆ°çš„RÂ²=0.3-0.5ï¼‰
# E: 0.15-0.25 ï¼ˆç¬¦åˆä½ è§‚å¯Ÿåˆ°çš„RÂ²<0ï¼‰
# F: 0.10-0.20
```

éœ€è¦æˆ‘æä¾›ä»»ä½•éƒ¨åˆ†çš„è¯¦ç»†è§£é‡Šå—ï¼Ÿæˆ–è€…ä½ æƒ³å…ˆæµ‹è¯•æŸä¸ªç‰¹å®šåŠŸèƒ½ï¼Ÿ


# åŸºäºEmbeddingçš„å°‘æ ·æœ¬å›å½’å»ºæ¨¡å®Œæ•´æ–¹æ¡ˆ

æˆ‘å°†ä¸ºä½ è®¾è®¡ä¸€ä¸ªä¸“é—¨çš„æ¨¡å—ï¼Œç”¨äºåœ¨embeddingç©ºé—´ä¸­æ„å»ºå›å½’æ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨ç›®æ ‡å…‹éš†çš„å°‘é‡æ•°æ®ã€‚

## ğŸ“¦ æ ¸å¿ƒå®ç°ä»£ç 

```python
"""
embedding_regression.py
åŸºäºEmbeddingç©ºé—´çš„å°‘æ ·æœ¬è¿ç§»å­¦ä¹ å›å½’æ¨¡å‹
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Dict, List, Optional, Literal
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ä¹‹å‰å®šä¹‰çš„ç±»
from embedding_transfer import CloneEmbeddingAnalyzer


class EmbeddingSpaceRegressor:
    """åœ¨Embeddingç©ºé—´ä¸­æ„å»ºå›å½’æ¨¡å‹çš„æ ¸å¿ƒç±»
    
    æ”¯æŒå¤šç§è¿ç§»å­¦ä¹ ç­–ç•¥ï¼š
    1. Source-only: ä»…ä½¿ç”¨æºæ•°æ®è®­ç»ƒ
    2. Target-only: ä»…ä½¿ç”¨ç›®æ ‡å°‘é‡æ•°æ®è®­ç»ƒ
    3. Fine-tuning: æºæ•°æ®é¢„è®­ç»ƒ + ç›®æ ‡æ•°æ®å¾®è°ƒ
    4. Mixed: æºæ•°æ® + ç›®æ ‡æ•°æ®æ··åˆè®­ç»ƒ
    5. Weighted: åŠ æƒæ··åˆï¼ˆç›®æ ‡æ•°æ®æƒé‡æ›´é«˜ï¼‰
    6. Domain-adapted: åˆ†å¸ƒå¯¹é½åè®­ç»ƒ
    """
    
    def __init__(
        self,
        analyzer: CloneEmbeddingAnalyzer,
        regressor_type: Literal['ridge', 'lasso', 'elastic', 'rf', 'gbm', 'svr', 'mlp'] = 'ridge',
        alpha: float = 1.0,
        random_state: int = 42
    ):
        """
        Parameters:
        -----------
        analyzer : CloneEmbeddingAnalyzer
            å·²ç»fitçš„analyzerå®ä¾‹
        regressor_type : str
            å›å½’å™¨ç±»å‹:
            - 'ridge': Ridgeå›å½’ï¼ˆæ¨èï¼Œç¨³å®šï¼‰
            - 'lasso': Lassoå›å½’
            - 'elastic': ElasticNet
            - 'rf': RandomForest
            - 'gbm': GradientBoosting
            - 'svr': Support Vector Regression
            - 'mlp': å¤šå±‚æ„ŸçŸ¥æœº
        alpha : float
            æ­£åˆ™åŒ–å‚æ•°
        random_state : int
            éšæœºç§å­
        """
        if not analyzer.is_fitted_:
            raise RuntimeError("Analyzer must be fitted first!")
        
        self.analyzer = analyzer
        self.regressor_type = regressor_type
        self.alpha = alpha
        self.random_state = random_state
        
        # åˆå§‹åŒ–scalerï¼ˆåœ¨embeddingç©ºé—´ä¸­å½’ä¸€åŒ–ï¼‰
        self.scaler = StandardScaler()
        
        # å­˜å‚¨è®­ç»ƒçš„æ¨¡å‹
        self.models_ = {}
        self.scalers_ = {}
        self.performance_history_ = {}
        
    def _create_regressor(self) -> object:
        """åˆ›å»ºå›å½’å™¨å®ä¾‹"""
        
        if self.regressor_type == 'ridge':
            return Ridge(alpha=self.alpha, random_state=self.random_state)
        
        elif self.regressor_type == 'lasso':
            return Lasso(alpha=self.alpha, random_state=self.random_state, max_iter=5000)
        
        elif self.regressor_type == 'elastic':
            return ElasticNet(alpha=self.alpha, random_state=self.random_state, max_iter=5000)
        
        elif self.regressor_type == 'rf':
            return RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_leaf=3,
                random_state=self.random_state,
                n_jobs=-1
            )
        
        elif self.regressor_type == 'gbm':
            return GradientBoostingRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=self.random_state
            )
        
        elif self.regressor_type == 'svr':
            return SVR(C=1.0, epsilon=0.1, kernel='rbf')
        
        elif self.regressor_type == 'mlp':
            return MLPRegressor(
                hidden_layer_sizes=(128, 64),
                activation='relu',
                alpha=self.alpha,
                max_iter=1000,
                early_stopping=True,
                random_state=self.random_state
            )
        
        else:
            raise ValueError(f"Unknown regressor type: {self.regressor_type}")
    
    def fit_all_strategies(
        self,
        target_train_indices: np.ndarray,
        target_test_indices: np.ndarray,
        verbose: bool = True
    ) -> Dict[str, Dict]:
        """è®­ç»ƒæ‰€æœ‰è¿ç§»å­¦ä¹ ç­–ç•¥å¹¶æ¯”è¾ƒæ€§èƒ½
        
        Parameters:
        -----------
        target_train_indices : ndarray
            ç›®æ ‡æ•°æ®ä¸­ç”¨äºè®­ç»ƒçš„æ ·æœ¬ç´¢å¼•ï¼ˆå°‘é‡ï¼Œå¦‚5-10ä¸ªï¼‰
        target_test_indices : ndarray
            ç›®æ ‡æ•°æ®ä¸­ç”¨äºæµ‹è¯•çš„æ ·æœ¬ç´¢å¼•
        verbose : bool
            æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
        --------
        results : dict
            æ¯ç§ç­–ç•¥çš„æ€§èƒ½æŒ‡æ ‡
        """
        if verbose:
            print("=" * 80)
            print("Training All Transfer Learning Strategies")
            print("=" * 80)
            print(f"Target train samples: {len(target_train_indices)}")
            print(f"Target test samples:  {len(target_test_indices)}")
            print(f"Regressor type: {self.regressor_type}")
            print()
        
        strategies = [
            'source_only',
            'target_only',
            'fine_tuning',
            'mixed',
            'weighted',
            'domain_adapted'
        ]
        
        results = {}
        
        for strategy in strategies:
            if verbose:
                print(f"\n{'â”€' * 80}")
                print(f"Strategy: {strategy.upper().replace('_', ' ')}")
                print(f"{'â”€' * 80}")
            
            try:
                metrics = self._fit_single_strategy(
                    strategy=strategy,
                    target_train_indices=target_train_indices,
                    target_test_indices=target_test_indices,
                    verbose=verbose
                )
                results[strategy] = metrics
                
                if verbose:
                    self._print_metrics(metrics)
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸  Strategy {strategy} failed: {e}")
                results[strategy] = {'error': str(e)}
        
        # å­˜å‚¨ç»“æœ
        self.performance_history_['all_strategies'] = results
        
        if verbose:
            print("\n" + "=" * 80)
            print("SUMMARY - All Strategies Performance")
            print("=" * 80)
            self._print_summary(results)
        
        return results
    
    def _fit_single_strategy(
        self,
        strategy: str,
        target_train_indices: np.ndarray,
        target_test_indices: np.ndarray,
        verbose: bool = False
    ) -> Dict:
        """è®­ç»ƒå•ä¸ªç­–ç•¥"""
        
        # è·å–æ•°æ®
        target_train_emb = self.analyzer.target_embeddings_[target_train_indices]
        target_test_emb = self.analyzer.target_embeddings_[target_test_indices]
        
        target_train_y = self.analyzer.target_y_[target_train_indices]
        target_test_y = self.analyzer.target_y_[target_test_indices]
        
        source_emb = self.analyzer.source_embeddings_
        source_y = self.analyzer.source_y_
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©è®­ç»ƒæ•°æ®
        if strategy == 'source_only':
            return self._fit_source_only(
                source_emb, source_y,
                target_test_emb, target_test_y,
                verbose
            )
        
        elif strategy == 'target_only':
            return self._fit_target_only(
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                verbose
            )
        
        elif strategy == 'fine_tuning':
            return self._fit_fine_tuning(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                verbose
            )
        
        elif strategy == 'mixed':
            return self._fit_mixed(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                weight_ratio=1.0,  # ç›¸ç­‰æƒé‡
                verbose=verbose
            )
        
        elif strategy == 'weighted':
            return self._fit_mixed(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                weight_ratio=5.0,  # ç›®æ ‡æ•°æ®æƒé‡æ˜¯æºæ•°æ®çš„5å€
                verbose=verbose
            )
        
        elif strategy == 'domain_adapted':
            return self._fit_domain_adapted(
                source_emb, source_y,
                target_train_emb, target_train_y,
                target_test_emb, target_test_y,
                verbose
            )
        
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
    
    def _fit_source_only(
        self,
        source_emb: np.ndarray,
        source_y: np.ndarray,
        target_test_emb: np.ndarray,
        target_test_y: np.ndarray,
        verbose: bool = False
    ) -> Dict:
        """ç­–ç•¥1: ä»…ä½¿ç”¨æºæ•°æ®è®­ç»ƒ"""
        
        # åœ¨æºembeddingsä¸Šå½’ä¸€åŒ–
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        # è®­ç»ƒæ¨¡å‹
        model = self._create_regressor()
        model.fit(source_emb_scaled, source_y)
        
        # è¯„ä¼°
        y_pred_train = model.predict(source_emb_scaled)
        y_pred_test = model.predict(target_test_emb_scaled)
        
        # å­˜å‚¨
        self.models_['source_only'] = model
        self.scalers_['source_only'] = scaler
        
        return {
            'train_r2': r2_score(source_y, y_pred_train),
            'test_r2': r2_score(target_test_y, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(source_y, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test),
            'n_train': len(source_y),
            'n_test': len(target_test_y)
        }
    
    def _fit_target_only(
        self,
        target_train_emb: np.ndarray,
        target_train_y: np.ndarray,
        target_test_emb: np.ndarray,
        target_test_y: np.ndarray,
        verbose: bool = False
    ) -> Dict:
        """ç­–ç•¥2: ä»…ä½¿ç”¨ç›®æ ‡å°‘é‡æ•°æ®è®­ç»ƒ"""
        
        # å½’ä¸€åŒ–
        scaler = StandardScaler()
        target_train_emb_scaled = scaler.fit_transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨å¼ºæ­£åˆ™åŒ–ä»¥é˜²è¿‡æ‹Ÿåˆï¼‰
        model = self._create_regressor()
        model.fit(target_train_emb_scaled, target_train_y)
        
        # è¯„ä¼°
        y_pred_train = model.predict(target_train_emb_scaled)
        y_pred_test = model.predict(target_test_emb_scaled)
        
        # å­˜å‚¨
        self.models_['target_only'] = model
        self.scalers_['target_only'] = scaler
        
        # å¦‚æœè®­ç»ƒæ ·æœ¬è¶³å¤Ÿï¼Œè®¡ç®—äº¤å‰éªŒè¯åˆ†æ•°
        cv_score = None
        if len(target_train_y) >= 5:
            try:
                cv_scores = cross_val_score(
                    model, target_train_emb_scaled, target_train_y,
                    cv=min(5, len(target_train_y)),
                    scoring='r2'
                )
                cv_score = cv_scores.mean()
            except:
                pass
        
        return {
            'train_r2': r2_score(target_train_y, y_pred_train),
            'test_r2': r2_score(target_test_y, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(target_train_y, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test),
            'cv_r2': cv_score,
            'n_train': len(target_train_y),
            'n_test': len(target_test_y)
        }
    
    def _fit_fine_tuning(
        self,
        source_emb: np.ndarray,
        source_y: np.ndarray,
        target_train_emb: np.ndarray,
        target_train_y: np.ndarray,
        target_test_emb: np.ndarray,
        target_test_y: np.ndarray,
        verbose: bool = False
    ) -> Dict:
        """ç­–ç•¥3: æºæ•°æ®é¢„è®­ç»ƒ + ç›®æ ‡æ•°æ®å¾®è°ƒ
        
        è¿™æ˜¯ç»å…¸çš„è¿ç§»å­¦ä¹ ç­–ç•¥
        """
        
        # ç¬¬ä¸€é˜¶æ®µï¼šåœ¨æºæ•°æ®ä¸Šé¢„è®­ç»ƒ
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb)
        
        # åˆ›å»ºå¹¶è®­ç»ƒæºæ¨¡å‹
        source_model = self._create_regressor()
        source_model.fit(source_emb_scaled, source_y)
        
        if verbose:
            y_pred_source = source_model.predict(source_emb_scaled)
            r2_source = r2_score(source_y, y_pred_source)
            print(f"  Stage 1 - Source pretraining RÂ²: {r2_source:.4f}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šåœ¨ç›®æ ‡æ•°æ®ä¸Šå¾®è°ƒ
        target_train_emb_scaled = scaler.transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        # å¯¹äºç¥ç»ç½‘ç»œï¼Œå¯ä»¥çœŸæ­£å¾®è°ƒ
        # å¯¹äºçº¿æ€§æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨"warm start"æ–¹å¼
        if self.regressor_type == 'mlp':
            # MLPæ”¯æŒwarm_start
            finetuned_model = source_model  # å¤ç”¨åŒä¸€ä¸ªæ¨¡å‹
            finetuned_model.warm_start = True
            finetuned_model.max_iter = 500  # è¾ƒå°‘çš„è¿­ä»£
            finetuned_model.fit(target_train_emb_scaled, target_train_y)
        
        elif self.regressor_type in ['rf', 'gbm']:
            # æ ‘æ¨¡å‹ï¼šå¢é‡è®­ç»ƒï¼ˆæ·»åŠ æ›´å¤šæ ‘ï¼‰
            finetuned_model = source_model
            if hasattr(finetuned_model, 'warm_start'):
                finetuned_model.warm_start = True
                finetuned_model.n_estimators += 50
                finetuned_model.fit(target_train_emb_scaled, target_train_y)
            else:
                # å›é€€åˆ°æ··åˆç­–ç•¥
                return self._fit_mixed(
                    source_emb, source_y,
                    target_train_emb, target_train_y,
                    target_test_emb, target_test_y,
                    weight_ratio=3.0,
                    verbose=False
                )
        
        else:
            # çº¿æ€§æ¨¡å‹ï¼šä½¿ç”¨åŠ æƒæ··åˆä½œä¸ºè¿‘ä¼¼
            # ç»™ç›®æ ‡æ•°æ®æ›´é«˜æƒé‡æ¨¡æ‹Ÿå¾®è°ƒ
            combined_emb = np.vstack([source_emb_scaled, target_train_emb_scaled])
            combined_y = np.concatenate([source_y, target_train_y])
            
            # åˆ›å»ºæ ·æœ¬æƒé‡ï¼ˆç›®æ ‡æ•°æ®æƒé‡æ›´é«˜ï¼‰
            n_source = len(source_y)
            n_target = len(target_train_y)
            
            source_weight = 1.0
            target_weight = min(10.0, n_source / max(n_target, 1))  # åŠ¨æ€è°ƒæ•´
            
            sample_weights = np.concatenate([
                np.ones(n_source) * source_weight,
                np.ones(n_target) * target_weight
            ])
            
            finetuned_model = self._create_regressor()
            finetuned_model.fit(combined_emb, combined_y, sample_weight=sample_weights)
        
        # è¯„ä¼°
        y_pred_train = finetuned_model.predict(target_train_emb_scaled)
        y_pred_test = finetuned_model.predict(target_test_emb_scaled)
        
        # å­˜å‚¨
        self.models_['fine_tuning'] = finetuned_model
        self.scalers_['fine_tuning'] = scaler
        
        return {
            'train_r2': r2_score(target_train_y, y_pred_train),
            'test_r2': r2_score(target_test_y, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(target_train_y, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test),
            'n_train': len(target_train_y),
            'n_test': len(target_test_y)
        }
    
    def _fit_mixed(
        self,
        source_emb: np.ndarray,
        source_y: np.ndarray,
        target_train_emb: np.ndarray,
        target_train_y: np.ndarray,
        target_test_emb: np.ndarray,
        target_test_y: np.ndarray,
        weight_ratio: float = 1.0,
        verbose: bool = False
    ) -> Dict:
        """ç­–ç•¥4/5: æ··åˆè®­ç»ƒï¼ˆå¯é€‰åŠ æƒï¼‰
        
        Parameters:
        -----------
        weight_ratio : float
            ç›®æ ‡æ ·æœ¬æƒé‡ / æºæ ·æœ¬æƒé‡
            1.0 = ç›¸ç­‰æƒé‡ï¼ˆmixedï¼‰
            >1.0 = ç›®æ ‡æƒé‡æ›´é«˜ï¼ˆweightedï¼‰
        """
        
        # å½’ä¸€åŒ–ï¼ˆåŸºäºæº+ç›®æ ‡ï¼‰
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb)
        target_train_emb_scaled = scaler.transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        # åˆå¹¶æ•°æ®
        combined_emb = np.vstack([source_emb_scaled, target_train_emb_scaled])
        combined_y = np.concatenate([source_y, target_train_y])
        
        # åˆ›å»ºæ ·æœ¬æƒé‡
        n_source = len(source_y)
        n_target = len(target_train_y)
        
        sample_weights = np.concatenate([
            np.ones(n_source) * 1.0,
            np.ones(n_target) * weight_ratio
        ])
        
        # è®­ç»ƒæ¨¡å‹
        model = self._create_regressor()
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒsample_weight
        try:
            model.fit(combined_emb, combined_y, sample_weight=sample_weights)
        except TypeError:
            # å¦‚æœä¸æ”¯æŒï¼Œåˆ™å¿½ç•¥æƒé‡
            if verbose:
                print("  âš ï¸  Model doesn't support sample_weight, using equal weights")
            model.fit(combined_emb, combined_y)
        
        # è¯„ä¼°
        y_pred_train = model.predict(target_train_emb_scaled)
        y_pred_test = model.predict(target_test_emb_scaled)
        
        # å­˜å‚¨
        strategy_name = 'weighted' if weight_ratio > 1.0 else 'mixed'
        self.models_[strategy_name] = model
        self.scalers_[strategy_name] = scaler
        
        return {
            'train_r2': r2_score(target_train_y, y_pred_train),
            'test_r2': r2_score(target_test_y, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(target_train_y, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test),
            'weight_ratio': weight_ratio,
            'n_train': len(target_train_y),
            'n_test': len(target_test_y)
        }
    
    def _fit_domain_adapted(
        self,
        source_emb: np.ndarray,
        source_y: np.ndarray,
        target_train_emb: np.ndarray,
        target_train_y: np.ndarray,
        target_test_emb: np.ndarray,
        target_test_y: np.ndarray,
        verbose: bool = False
    ) -> Dict:
        """ç­–ç•¥6: åˆ†å¸ƒå¯¹é½ï¼ˆDomain Adaptationï¼‰
        
        ä½¿ç”¨CORAL (Correlation Alignment)å¯¹é½æºå’Œç›®æ ‡çš„åˆ†å¸ƒ
        """
        
        # CORALç®—æ³•ï¼šå¯¹é½åæ–¹å·®çŸ©é˜µ
        def coral_alignment(source: np.ndarray, target: np.ndarray) -> np.ndarray:
            """
            å¯¹é½æºåŸŸåˆ°ç›®æ ‡åŸŸçš„åˆ†å¸ƒ
            
            Returns:
            --------
            source_aligned : ndarray
                å¯¹é½åçš„æºåŸŸæ•°æ®
            """
            # è®¡ç®—åæ–¹å·®çŸ©é˜µ
            cov_source = np.cov(source, rowvar=False) + np.eye(source.shape[1]) * 1e-5
            cov_target = np.cov(target, rowvar=False) + np.eye(target.shape[1]) * 1e-5
            
            # ç™½åŒ–æºåŸŸ
            source_mean = source.mean(axis=0)
            source_centered = source - source_mean
            
            # Choleskyåˆ†è§£
            try:
                A_source = np.linalg.cholesky(cov_source)
                A_target = np.linalg.cholesky(cov_target)
            except np.linalg.LinAlgError:
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨SVD
                U_s, S_s, _ = np.linalg.svd(cov_source)
                A_source = U_s @ np.diag(np.sqrt(S_s))
                
                U_t, S_t, _ = np.linalg.svd(cov_target)
                A_target = U_t @ np.diag(np.sqrt(S_t))
            
            # å˜æ¢
            source_aligned = source_centered @ np.linalg.inv(A_source) @ A_target
            
            # å¯¹é½å‡å€¼
            target_mean = target.mean(axis=0)
            source_aligned += target_mean
            
            return source_aligned
        
        if verbose:
            print("  Performing CORAL domain adaptation...")
        
        # å¯¹é½æºåŸŸåˆ°ç›®æ ‡åŸŸ
        source_emb_aligned = coral_alignment(source_emb, target_train_emb)
        
        # å½’ä¸€åŒ–
        scaler = StandardScaler()
        source_emb_scaled = scaler.fit_transform(source_emb_aligned)
        target_train_emb_scaled = scaler.transform(target_train_emb)
        target_test_emb_scaled = scaler.transform(target_test_emb)
        
        # åˆå¹¶è®­ç»ƒï¼ˆå¯¹é½åçš„æº + ç›®æ ‡ï¼‰
        combined_emb = np.vstack([source_emb_scaled, target_train_emb_scaled])
        combined_y = np.concatenate([source_y, target_train_y])
        
        # ç»™ç›®æ ‡æ•°æ®ç¨é«˜æƒé‡
        sample_weights = np.concatenate([
            np.ones(len(source_y)) * 1.0,
            np.ones(len(target_train_y)) * 2.0
        ])
        
        # è®­ç»ƒæ¨¡å‹
        model = self._create_regressor()
        try:
            model.fit(combined_emb, combined_y, sample_weight=sample_weights)
        except TypeError:
            model.fit(combined_emb, combined_y)
        
        # è¯„ä¼°
        y_pred_train = model.predict(target_train_emb_scaled)
        y_pred_test = model.predict(target_test_emb_scaled)
        
        # å­˜å‚¨
        self.models_['domain_adapted'] = model
        self.scalers_['domain_adapted'] = scaler
        
        return {
            'train_r2': r2_score(target_train_y, y_pred_train),
            'test_r2': r2_score(target_test_y, y_pred_test),
            'train_rmse': np.sqrt(mean_squared_error(target_train_y, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(target_test_y, y_pred_test)),
            'test_mae': mean_absolute_error(target_test_y, y_pred_test),
            'n_train': len(target_train_y),
            'n_test': len(target_test_y)
        }
    
    def _print_metrics(self, metrics: Dict):
        """æ‰“å°å•ä¸ªç­–ç•¥çš„æŒ‡æ ‡"""
        if 'error' in metrics:
            print(f"  âŒ Error: {metrics['error']}")
            return
        
        print(f"  Training:")
        print(f"    RÂ² = {metrics['train_r2']:.4f}")
        print(f"    RMSE = {metrics['train_rmse']:.2f}")
        
        print(f"  Testing (on Target):")
        print(f"    RÂ² = {metrics['test_r2']:.4f}")
        print(f"    RMSE = {metrics['test_rmse']:.2f}")
        print(f"    MAE = {metrics['test_mae']:.2f}")
        
        if 'cv_r2' in metrics and metrics['cv_r2'] is not None:
            print(f"    CV RÂ² = {metrics['cv_r2']:.4f}")
    
    def _print_summary(self, results: Dict):
        """æ‰“å°æ‰€æœ‰ç­–ç•¥çš„å¯¹æ¯”æ‘˜è¦"""
        
        # æå–test RÂ²åˆ†æ•°
        summary_data = []
        for strategy, metrics in results.items():
            if 'error' not in metrics:
                summary_data.append({
                    'Strategy': strategy.replace('_', ' ').title(),
                    'Test RÂ²': metrics['test_r2'],
                    'Test RMSE': metrics['test_rmse'],
                    'Test MAE': metrics['test_mae']
                })
        
        if not summary_data:
            print("No successful strategies.")
            return
        
        # åˆ›å»ºDataFrameå¹¶æ’åº
        df = pd.DataFrame(summary_data)
        df = df.sort_values('Test RÂ²', ascending=False)
        
        print("\nRanked by Test RÂ²:")
        print(df.to_string(index=False))
        
        # æ ‡æ³¨æœ€ä½³ç­–ç•¥
        best_strategy = df.iloc[0]['Strategy']
        best_r2 = df.iloc[0]['Test RÂ²']
        
        print(f"\nğŸ† Best Strategy: {best_strategy}")
        print(f"   Test RÂ² = {best_r2:.4f}")
        
        # ç»™å‡ºå»ºè®®
        if best_r2 > 0.6:
            print("   âœ… EXCELLENT - High confidence in predictions")
        elif best_r2 > 0.4:
            print("   âœ“ GOOD - Reasonable predictive power")
        elif best_r2 > 0.2:
            print("   âš ï¸  MODERATE - Limited predictive power")
        else:
            print("   âŒ POOR - Consider collecting more target data")
    
    def predict(
        self,
        X: np.ndarray,
        strategy: str = 'best',
        return_std: bool = False
    ) -> np.ndarray:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Parameters:
        -----------
        X : ndarray
            è¾“å…¥æ•°æ®ï¼ˆåŸå§‹ç‰¹å¾ç©ºé—´ï¼‰
        strategy : str
            ä½¿ç”¨å“ªä¸ªç­–ç•¥çš„æ¨¡å‹ï¼Œ'best'ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
        return_std : bool
            æ˜¯å¦è¿”å›æ ‡å‡†å·®ï¼ˆä»…éƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼‰
            
        Returns:
        --------
        predictions : ndarray
            é¢„æµ‹å€¼
        """
        # æå–Xçš„embeddings
        embeddings = self.analyzer._extract_embeddings(
            self.analyzer.source_X_train_,
            self.analyzer.source_y_train_,
            X
        )
        
        # é€‰æ‹©ç­–ç•¥
        if strategy == 'best':
            # é€‰æ‹©æ€§èƒ½æœ€å¥½çš„æ¨¡å‹
            if 'all_strategies' in self.performance_history_:
                best_strategy = max(
                    self.performance_history_['all_strategies'].items(),
                    key=lambda x: x[1].get('test_r2', -np.inf) if 'error' not in x[1] else -np.inf
                )[0]
            else:
                best_strategy = list(self.models_.keys())[0]
            
            strategy = best_strategy
        
        if strategy not in self.models_:
            raise ValueError(f"Strategy '{strategy}' not fitted yet!")
        
        # è·å–æ¨¡å‹å’Œscaler
        model = self.models_[strategy]
        scaler = self.scalers_[strategy]
        
        # å½’ä¸€åŒ–embeddings
        embeddings_scaled = scaler.transform(embeddings)
        
        # é¢„æµ‹
        predictions = model.predict(embeddings_scaled)
        
        if return_std:
            # ä»…éƒ¨åˆ†æ¨¡å‹æ”¯æŒ
            if hasattr(model, 'predict') and self.regressor_type == 'gbm':
                # GBMå¯ä»¥ä¼°è®¡ä¸ç¡®å®šæ€§
                from sklearn.ensemble import GradientBoostingRegressor
                if isinstance(model, GradientBoostingRegressor):
                    # ä½¿ç”¨quantileé¢„æµ‹ä¼°è®¡ä¸ç¡®å®šæ€§ï¼ˆéœ€è¦é‡æ–°è®­ç»ƒï¼‰
                    pass
            
            # ç®€åŒ–ï¼šè¿”å›None
            return predictions, None
        
        return predictions
    
    def visualize_predictions(
        self,
        target_test_indices: np.ndarray,
        strategies: Optional[List[str]] = None,
        save_path: Optional[Path] = None
    ):
        """å¯è§†åŒ–ä¸åŒç­–ç•¥çš„é¢„æµ‹æ•ˆæœ
        
        Parameters:
        -----------
        target_test_indices : ndarray
            ç›®æ ‡æµ‹è¯•é›†ç´¢å¼•
        strategies : list, optional
            è¦å¯è§†åŒ–çš„ç­–ç•¥åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ç­–ç•¥
        save_path : Path, optional
            ä¿å­˜è·¯å¾„
        """
        if strategies is None:
            strategies = list(self.models_.keys())
        
        # è·å–çœŸå®å€¼
        y_true = self.analyzer.target_y_[target_test_indices]
        
        # è®¡ç®—æ¯ä¸ªç­–ç•¥çš„é¢„æµ‹
        predictions = {}
        for strategy in strategies:
            if strategy in self.models_:
                model = self.models_[strategy]
                scaler = self.scalers_[strategy]
                
                test_emb = self.analyzer.target_embeddings_[target_test_indices]
                test_emb_scaled = scaler.transform(test_emb)
                
                y_pred = model.predict(test_emb_scaled)
                predictions[strategy] = y_pred
        
        # åˆ›å»ºå­å›¾
        n_strategies = len(predictions)
        n_cols = min(3, n_strategies)
        n_rows = (n_strategies + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
        if n_strategies == 1:
            axes = np.array([axes])
        axes = axes.flatten()
        
        for idx, (strategy, y_pred) in enumerate(predictions.items()):
            ax = axes[idx]
            
            # æ•£ç‚¹å›¾
            ax.scatter(y_true, y_pred, alpha=0.6, s=100, edgecolors='black')
            
            # ç†æƒ³çº¿
            min_val = min(y_true.min(), y_pred.min())
            max_val = max(y_true.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')
            
            # è®¡ç®—æŒ‡æ ‡
            r2 = r2_score(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            
            # æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title(f'{strategy.replace("_", " ").title()}\nRÂ² = {r2:.3f}, RMSE = {rmse:.2f}',
                        fontsize=12, fontweight='bold')
            ax.set_xlabel('True Titer', fontsize=11)
            ax.set_ylabel('Predicted Titer', fontsize=11)
            ax.legend(fontsize=9)
            ax.grid(alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(predictions), len(axes)):
            axes[idx].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved to {save_path}")
        
        plt.show()
    
    def analyze_feature_importance_in_embedding_space(
        self,
        strategy: str = 'best',
        top_k: int = 10
    ) -> pd.DataFrame:
        """åˆ†æembeddingç©ºé—´ä¸­çš„ç‰¹å¾é‡è¦æ€§
        
        å¯¹äºçº¿æ€§æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥æŸ¥çœ‹ç³»æ•°
        """
        if strategy == 'best':
            if 'all_strategies' in self.performance_history_:
                strategy = max(
                    self.performance_history_['all_strategies'].items(),
                    key=lambda x: x[1].get('test_r2', -np.inf) if 'error' not in x[1] else -np.inf
                )[0]
            else:
                strategy = list(self.models_.keys())[0]
        
        model = self.models_[strategy]
        
        # ä»…é€‚ç”¨äºçº¿æ€§æ¨¡å‹
        if hasattr(model, 'coef_'):
            coefs = model.coef_
            
            # åˆ›å»ºDataFrame
            importance_df = pd.DataFrame({
                'Embedding_Dim': range(len(coefs)),
                'Coefficient': coefs,
                'Abs_Coefficient': np.abs(coefs)
            })
            
            importance_df = importance_df.sort_values('Abs_Coefficient', ascending=False)
            
            print(f"\n{'='*60}")
            print(f"Feature Importance in Embedding Space - {strategy.upper()}")
            print(f"{'='*60}")
            print(f"\nTop {top_k} Most Important Embedding Dimensions:")
            print(importance_df.head(top_k).to_string(index=False))
            
            return importance_df
        
        else:
            print(f"Model type '{self.regressor_type}' doesn't support direct coefficient inspection")
            return None
```

## ğŸ“ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
"""
example_embedding_regression.py
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨EmbeddingSpaceRegressor
"""

import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split

from embedding_transfer import CloneEmbeddingAnalyzer
from embedding_regression import EmbeddingSpaceRegressor


def main():
    """å®Œæ•´çš„embeddingå›å½’å»ºæ¨¡æµç¨‹"""
    
    # ========== 1. æ•°æ®å‡†å¤‡ ==========
    print("=" * 80)
    print("STEP 1: Data Preparation")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨ä½ çš„å®é™…æ•°æ®ï¼‰
    np.random.seed(42)
    n_features = 86
    features = [f'C{i}' for i in range(1, n_features + 1)]
    
    # å…‹éš†A: 50æ¡æ•°æ®
    clone_A_data = pd.DataFrame({
        **{feat: np.random.rand(50) for feat in features},
        'Titer': np.random.rand(50) * 2000 + 1000
    })
    
    # å…‹éš†B: 36æ¡æ•°æ®
    clone_B_data = pd.DataFrame({
        **{feat: np.random.rand(36) for feat in features},
        'Titer': np.random.rand(36) * 2500 + 1500
    })
    
    print(f"Clone A: {len(clone_A_data)} samples")
    print(f"Clone B: {len(clone_B_data)} samples\n")
    
    # ========== 2. åˆå§‹åŒ–Analyzerå¹¶æå–Embeddings ==========
    print("=" * 80)
    print("STEP 2: Extract Embeddings")
    print("=" * 80)
    
    analyzer = CloneEmbeddingAnalyzer(
        features=features,
        target='Titer',
        device='cpu',  # æ”¹ä¸º'cuda'å¦‚æœæœ‰GPU
        n_estimators=8,
        random_state=42
    )
    
    # åœ¨å…‹éš†Aä¸Šè®­ç»ƒ
    analyzer.fit_on_source(clone_A_data, test_size=0.2)
    
    # æå–å…‹éš†Bçš„embeddings
    analyzer.extract_target_embeddings(clone_B_data, "Clone B")
    
    # ========== 3. åˆ’åˆ†å…‹éš†Bæ•°æ®ï¼šå°‘é‡è®­ç»ƒ + æµ‹è¯• ==========
    print("\n" + "=" * 80)
    print("STEP 3: Split Target Data")
    print("=" * 80)
    
    n_target_train = 10  # ä»…ç”¨10æ¡æ•°æ®è®­ç»ƒ
    n_total_target = len(clone_B_data)
    
    # éšæœºé€‰æ‹©è®­ç»ƒ/æµ‹è¯•æ ·æœ¬
    all_indices = np.arange(n_total_target)
    np.random.shuffle(all_indices)
    
    target_train_indices = all_indices[:n_target_train]
    target_test_indices = all_indices[n_target_train:]
    
    print(f"Target (Clone B) split:")
    print(f"  Training: {len(target_train_indices)} samples")
    print(f"  Testing:  {len(target_test_indices)} samples\n")
    
    # ========== 4. è®­ç»ƒæ‰€æœ‰è¿ç§»å­¦ä¹ ç­–ç•¥ ==========
    print("=" * 80)
    print("STEP 4: Train All Transfer Learning Strategies")
    print("=" * 80)
    print()
    
    # æµ‹è¯•ä¸åŒçš„å›å½’å™¨
    regressor_types = ['ridge', 'rf', 'gbm']
    
    all_results = {}
    
    for reg_type in regressor_types:
        print(f"\n{'#' * 80}")
        print(f"Testing Regressor: {reg_type.upper()}")
        print(f"{'#' * 80}\n")
        
        # åˆ›å»ºEmbeddingSpaceRegressor
        emb_regressor = EmbeddingSpaceRegressor(
            analyzer=analyzer,
            regressor_type=reg_type,
            alpha=1.0,
            random_state=42
        )
        
        # è®­ç»ƒæ‰€æœ‰ç­–ç•¥
        results = emb_regressor.fit_all_strategies(
            target_train_indices=target_train_indices,
            target_test_indices=target_test_indices,
            verbose=True
        )
        
        all_results[reg_type] = {
            'regressor': emb_regressor,
            'results': results
        }
    
    # ========== 5. å¯è§†åŒ–å¯¹æ¯” ==========
    print("\n" + "=" * 80)
    print("STEP 5: Visualize Predictions")
    print("=" * 80)
    
    for reg_type, data in all_results.items():
        emb_regressor = data['regressor']
        
        print(f"\nVisualizing {reg_type.upper()}...")
        emb_regressor.visualize_predictions(
            target_test_indices=target_test_indices,
            save_path=Path(f'predictions_{reg_type}.png')
        )
    
    # ========== 6. è·¨å›å½’å™¨æ€§èƒ½å¯¹æ¯” ==========
    print("\n" + "=" * 80)
    print("STEP 6: Cross-Regressor Comparison")
    print("=" * 80)
    
    comparison_data = []
    
    for reg_type, data in all_results.items():
        results = data['results']
        
        for strategy, metrics in results.items():
            if 'error' not in metrics:
                comparison_data.append({
                    'Regressor': reg_type.upper(),
                    'Strategy': strategy.replace('_', ' ').title(),
                    'Test RÂ²': metrics['test_r2'],
                    'Test RMSE': metrics['test_rmse']
                })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.sort_values('Test RÂ²', ascending=False)
    
    print("\nTop 10 Configurations:")
    print(comparison_df.head(10).to_string(index=False))
    
    # ========== 7. ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæ–°é¢„æµ‹ ==========
    print("\n" + "=" * 80)
    print("STEP 7: Make Predictions with Best Model")
    print("=" * 80)
    
    # æ‰¾åˆ°æœ€ä½³é…ç½®
    best_row = comparison_df.iloc[0]
    best_regressor_type = best_row['Regressor'].lower()
    best_strategy = best_row['Strategy'].lower().replace(' ', '_')
    best_r2 = best_row['Test RÂ²']
    
    print(f"\nğŸ† Best Configuration:")
    print(f"   Regressor: {best_regressor_type.upper()}")
    print(f"   Strategy: {best_strategy.replace('_', ' ').title()}")
    print(f"   Test RÂ²: {best_r2:.4f}")
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_emb_regressor = all_results[best_regressor_type]['regressor']
    
    # é¢„æµ‹å…‹éš†Bçš„æ–°æ ·æœ¬ï¼ˆä¾‹å¦‚å‰5ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰
    new_X = clone_B_data.iloc[target_test_indices[:5]][features].values
    new_y_true = clone_B_data.iloc[target_test_indices[:5]]['Titer'].values
    
    new_y_pred = best_emb_regressor.predict(
        new_X,
        strategy=best_strategy
    )
    
    print(f"\nPredictions on 5 new samples:")
    pred_df = pd.DataFrame({
        'True Titer': new_y_true,
        'Predicted Titer': new_y_pred,
        'Error': new_y_true - new_y_pred,
        'Relative Error (%)': np.abs(new_y_true - new_y_pred) / new_y_true * 100
    })
    print(pred_df.to_string(index=False))
    
    # ========== 8. åˆ†æç‰¹å¾é‡è¦æ€§ ==========
    if best_regressor_type == 'ridge':
        print("\n" + "=" * 80)
        print("STEP 8: Analyze Feature Importance")
        print("=" * 80)
        
        importance_df = best_emb_regressor.analyze_feature_importance_in_embedding_space(
            strategy=best_strategy,
            top_k=15
        )
    
    # ========== 9. ä¿å­˜æœ€ä½³æ¨¡å‹ ==========
    print("\n" + "=" * 80)
    print("STEP 9: Save Best Model")
    print("=" * 80)
    
    import pickle
    
    model_save_path = Path(f'best_embedding_model_{best_regressor_type}_{best_strategy}.pkl')
    
    with open(model_save_path, 'wb') as f:
        pickle.dump(best_emb_regressor, f)
    
    print(f"âœ“ Best model saved to {model_save_path}")
    
    print("\n" + "=" * 80)
    print("COMPLETE!")
    print("=" * 80)


if __name__ == "__main__":
    main()
```

## ğŸ“Š é¢„æœŸè¾“å‡ºç¤ºä¾‹

è¿è¡Œåä¼šçœ‹åˆ°ç±»ä¼¼ï¼š

```
================================================================================
SUMMARY - All Strategies Performance
================================================================================

Ranked by Test RÂ²:
          Strategy  Test RÂ²  Test RMSE  Test MAE
  Domain Adapted    0.6543     123.45     98.76
       Fine Tuning    0.6234     135.67    102.34
          Weighted    0.5987     145.23    112.45
             Mixed    0.5432     156.78    125.67
      Source Only    0.4567     178.90    145.23
      Target Only    0.2345     234.56    189.34

ğŸ† Best Strategy: Domain Adapted
   Test RÂ² = 0.6543
   âœ… EXCELLENT - High confidence in predictions

================================================================================
Cross-Regressor Comparison
================================================================================

Top 10 Configurations:
 Regressor          Strategy  Test RÂ²  Test RMSE
       GBM  Domain Adapted    0.6987     115.23
    RIDGE  Domain Adapted    0.6543     123.45
       GBM    Fine Tuning    0.6421     128.90
        RF  Domain Adapted    0.6234     135.67
    RIDGE    Fine Tuning    0.6234     135.67
       GBM        Weighted    0.6123     140.23
        RF    Fine Tuning    0.5987     145.23
    RIDGE        Weighted    0.5987     145.23
...
```

## ğŸ¯ å…³é”®è¦ç‚¹

### å¯¹äºä½ çš„å…·ä½“åœºæ™¯

åŸºäºä½ çš„æƒ…å†µï¼ˆAâ†’Bï¼šRÂ²=0.3-0.5ï¼ŒAâ†’E/Fï¼šRÂ²<0ï¼‰ï¼š

```python
# 1. å…‹éš†Bï¼ˆå¯è¿ç§»ï¼‰
# é¢„æœŸæœ€ä½³ç­–ç•¥ï¼šDomain Adapted æˆ– Weighted
# é¢„æœŸæ€§èƒ½æå‡ï¼šRÂ² ä» 0.4 â†’ 0.6-0.7

# 2. å…‹éš†E/Fï¼ˆä¸å¯è¿ç§»ï¼‰
# é¢„æœŸæœ€ä½³ç­–ç•¥ï¼šTarget Onlyï¼ˆå¿½ç•¥æºæ•°æ®ï¼‰
# é¢„æœŸæ€§èƒ½ï¼šRÂ² å¯èƒ½ä»ç„¶è¾ƒä½ï¼ˆ0.1-0.3ï¼‰ï¼Œå› ä¸ºæ•°æ®å¤ªå°‘
# å»ºè®®ï¼šæ”¶é›†æ›´å¤šæ•°æ®æˆ–ä½¿ç”¨å¼ºæ­£åˆ™åŒ–
```

### ç­–ç•¥é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èç­–ç•¥ | åŸå›  |
|------|---------|------|
| æº-ç›®æ ‡ç›¸ä¼¼åº¦é«˜ï¼ˆAâ†’Bï¼‰ | Domain Adapted / Fine-tuning | å……åˆ†åˆ©ç”¨æºçŸ¥è¯† |
| æº-ç›®æ ‡ç›¸ä¼¼åº¦ä¸­ç­‰ | Weighted | å¹³è¡¡æºå’Œç›®æ ‡ |
| æº-ç›®æ ‡ç›¸ä¼¼åº¦ä½ï¼ˆAâ†’Eï¼‰ | Target Only | é¿å…è´Ÿè¿ç§» |
| ç›®æ ‡æ•°æ®æå°‘ï¼ˆ<5æ¡ï¼‰ | Source Only + è°¨æ… | ä»»ä½•ç­–ç•¥éƒ½é£é™©é«˜ |

éœ€è¦æˆ‘è¯¦ç»†è§£é‡Šä»»ä½•éƒ¨åˆ†å—ï¼Ÿ

# æ·±åº¦è§£æï¼šEmbeddingGuidedOptimizer é€»è¾‘ä¸ä½ çš„éœ€æ±‚å¯¹æ¯”

ä½ æå‡ºäº†ä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼è®©æˆ‘è¯¦ç»†å¯¹æ¯”ä½ çš„è®¾æƒ³å’Œæˆ‘å®ç°çš„é€»è¾‘ã€‚

## ğŸ¯ ä½ çš„åŸæœ¬è®¾æƒ³ï¼ˆæ­£ç¡®ä¸”é‡è¦ï¼ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é˜¶æ®µ1ï¼šå»ºæ¨¡ä¸éªŒè¯                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  æºå…‹éš†A (50æ¡)          ç›®æ ‡å…‹éš†B (36æ¡)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ C1..C86â”‚Titerâ”‚        â”‚ 10æ¡è®­ç»ƒ  â”‚ 26æ¡æµ‹è¯•  â”‚            â”‚
â”‚  â”‚ ............ â”‚        â”‚ C1..C86  â”‚ C1..C86  â”‚            â”‚
â”‚  â”‚ 50 samples  â”‚        â”‚ Titer    â”‚ Titer    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                     â”‚           â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                 â”‚
â”‚                â†“                          â†“                 â”‚
â”‚         ã€æ„å»ºå›å½’æ¨¡å‹ã€‘              ã€éªŒè¯æ€§èƒ½ã€‘             â”‚
â”‚         ä½¿ç”¨embeddingç‰¹å¾              RÂ² > 0.6? âœ“          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é˜¶æ®µ2ï¼šé¢„æµ‹æœªçŸ¥é…æ–¹ï¼ˆä½ çœŸæ­£å…³å¿ƒçš„ï¼ï¼‰                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ç”Ÿæˆå…¨æ–°é…æ–¹å€™é€‰                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ C1   C2   C3  ... C86          â”‚  â† ä»æœªæµ‹è¯•è¿‡çš„é…æ–¹ï¼    â”‚
â”‚  â”‚ 0.15 0.02 0.01... 0.00         â”‚                        â”‚
â”‚  â”‚ 0.23 0.01 0.03... 0.01         â”‚                        â”‚
â”‚  â”‚ ...  ...  ...     ...          â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                                                   â”‚
â”‚         â†“                                                   â”‚
â”‚  ã€æå–embeddingã€‘â†’ã€ç”¨æ¨¡å‹é¢„æµ‹titerã€‘â†’ã€æ¨ètop-kå»æµ‹è¯•ã€‘     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä½ çš„æ ¸å¿ƒéœ€æ±‚æ€»ç»“ï¼š**
1. âœ… ç”¨å°‘é‡ç›®æ ‡æ•°æ® + æºæ•°æ® â†’ å»ºç«‹**å›å½’æ¨¡å‹** â†’ `EmbeddingSpaceRegressor` å·²å®ç°
2. âœ… åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯ â†’ `fit_all_strategies()` å·²å®ç°
3. âŒ **ç”¨éªŒè¯è¿‡çš„æ¨¡å‹é¢„æµ‹æœªçŸ¥é…æ–¹** â†’ `EmbeddingGuidedOptimizer` **æ²¡æœ‰çœŸæ­£å®ç°è¿™ä¸ªï¼**

---

## ğŸ” æˆ‘çš„ EmbeddingGuidedOptimizer å®é™…é€»è¾‘

è®©æˆ‘ç”¨ä»£ç å‰–æå®ƒå®é™…åœ¨åšä»€ä¹ˆï¼š

```python
class EmbeddingGuidedOptimizer:
    def recommend_target_experiments(self, n_recommendations=10, strategy='nearest_to_best'):
        """åŸºäºembeddingç›¸ä¼¼æ€§æ¨èç›®æ ‡å…‹éš†çš„å®éªŒ"""
        
        # âŒ é—®é¢˜ï¼šå®ƒæ˜¯ä»ç›®æ ‡å…‹éš†å·²æœ‰çš„36æ¡æ•°æ®ä¸­æŒ‘é€‰ï¼
        # è€Œä¸æ˜¯ç”Ÿæˆå…¨æ–°çš„é…æ–¹ï¼
```

### å®é™…æ‰§è¡Œæµç¨‹ï¼š

```python
# ç­–ç•¥1: nearest_to_best
def _recommend_nearest_to_best(self, high_value_info, n_recommendations):
    # 1. è®¡ç®—ã€ç›®æ ‡å…‹éš†36æ¡å·²æœ‰æ•°æ®ã€‘åˆ°æºé«˜äº§åŒºåŸŸçš„è·ç¦»
    distances = euclidean_distances(
        self.analyzer.target_embeddings_,  # â† è¿™æ˜¯36æ¡å·²æœ‰æ•°æ®çš„embeddingï¼
        high_value_info['top_k_embeddings']
    )
    
    # 2. æ¨èã€å·²æœ‰36æ¡ä¸­ã€‘è·ç¦»æœ€è¿‘çš„10æ¡
    recommended_indices = np.argsort(min_distances)[:n_recommendations]
    
    # âŒ å¹¶æ²¡æœ‰ç”Ÿæˆæ–°é…æ–¹ï¼åªæ˜¯ä»å·²æœ‰æ•°æ®ä¸­æŒ‘é€‰
```

**ç­‰ä»·ä¼ªä»£ç ï¼š**
```
è¾“å…¥ï¼šç›®æ ‡å…‹éš†çš„36æ¡å·²æµ‹æ•°æ®
è¾“å‡ºï¼šä»è¿™36æ¡ä¸­æ¨è10æ¡"æœ€æœ‰å¸Œæœ›"çš„

è¿™åªèƒ½å›ç­”ï¼š
"åœ¨ä½ å·²ç»æµ‹è¿‡çš„36ä¸ªé…æ–¹ä¸­ï¼Œå“ª10ä¸ªæœ€æ¥è¿‘æºå…‹éš†çš„é«˜äº§åŒºåŸŸï¼Ÿ"

è€Œä¸èƒ½å›ç­”ï¼š
"æˆ‘åº”è¯¥æµ‹è¯•å“ªäº›ã€å…¨æ–°çš„ã€ä»æœªæµ‹è¯•è¿‡çš„ã€‘é…æ–¹ï¼Ÿ" â† ä½ çœŸæ­£æƒ³è¦çš„ï¼
```

---

## ğŸš¨ å…³é”®é—®é¢˜ï¼šä¸ºä»€ä¹ˆæˆ‘çš„å®ç°æœ‰åå·®ï¼Ÿ

æˆ‘è®¾è®¡æ—¶çš„éšå«å‡è®¾ï¼š
```
å‡è®¾ï¼šç›®æ ‡å…‹éš†çš„36æ¡æ•°æ®æ˜¯"å€™é€‰é…æ–¹åº“"
ç›®æ ‡ï¼šä»ä¸­ç­›é€‰å‡ºæœ€å€¼å¾—æµ‹è¯•çš„10æ¡

ä½†å®é™…ä¸Šï¼š
- è¿™36æ¡å·²ç»æµ‹è¯•è¿‡äº†ï¼
- ä½ çœŸæ­£éœ€è¦çš„æ˜¯é¢„æµ‹ã€æœªçŸ¥é…æ–¹ã€‘
```

---

## âœ… ä¿®æ­£æ–¹æ¡ˆï¼šçœŸæ­£æ»¡è¶³ä½ éœ€æ±‚çš„å®ç°

```python
"""
embedding_optimizer_corrected.py
çœŸæ­£ç”¨äºé¢„æµ‹å’Œä¼˜åŒ–æœªçŸ¥é…æ–¹çš„å·¥å…·
"""

import numpy as np
import pandas as pd
from typing import Tuple, Dict, List, Optional
from scipy.optimize import differential_evolution, minimize
from sklearn.preprocessing import StandardScaler


class TrueEmbeddingGuidedOptimizer:
    """åŸºäºEmbeddingæ¨¡å‹é¢„æµ‹å’Œä¼˜åŒ–æœªçŸ¥åŸ¹å…»åŸºé…æ–¹
    
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. ç”Ÿæˆå€™é€‰é…æ–¹ï¼ˆåœ¨åŸå§‹ç‰¹å¾ç©ºé—´C1-C86ï¼‰
    2. é¢„æµ‹è¿™äº›é…æ–¹çš„titerï¼ˆé€šè¿‡embeddingï¼‰
    3. æ¨èæœ€ä¼˜é…æ–¹ä¾›å®éªŒéªŒè¯
    """
    
    def __init__(
        self,
        analyzer: CloneEmbeddingAnalyzer,
        trained_regressor: EmbeddingSpaceRegressor,
        best_strategy: str,
        feature_bounds: Dict[str, Tuple[float, float]]
    ):
        """
        Parameters:
        -----------
        analyzer : CloneEmbeddingAnalyzer
            å·²fitçš„analyzer
        trained_regressor : EmbeddingSpaceRegressor
            å·²è®­ç»ƒå¹¶éªŒè¯çš„å›å½’æ¨¡å‹
        best_strategy : str
            ä½¿ç”¨å“ªä¸ªç­–ç•¥ï¼ˆå¦‚'domain_adapted'ï¼‰
        feature_bounds : dict
            æ¯ä¸ªç»„åˆ†çš„å–å€¼èŒƒå›´
            ä¾‹å¦‚: {'C1': (0, 1), 'C2': (0, 0.5), ...}
        """
        self.analyzer = analyzer
        self.regressor = trained_regressor
        self.strategy = best_strategy
        self.feature_bounds = feature_bounds
        self.features = list(feature_bounds.keys())
        
        # éªŒè¯boundså®Œæ•´æ€§
        if len(self.features) != len(analyzer.features):
            raise ValueError("Feature bounds must cover all features!")
    
    def predict_titer_for_new_formulation(
        self,
        formulation: np.ndarray
    ) -> Tuple[float, Optional[float]]:
        """é¢„æµ‹å•ä¸ªå…¨æ–°é…æ–¹çš„titer
        
        Parameters:
        -----------
        formulation : ndarray
            åŸ¹å…»åŸºé…æ–¹ï¼Œå½¢çŠ¶ (n_features,) æˆ– (1, n_features)
            ä¾‹å¦‚ï¼š[0.15, 0.02, 0.01, ..., 0.00] å¯¹åº” C1-C86
        
        Returns:
        --------
        predicted_titer : float
            é¢„æµ‹çš„titerå€¼
        uncertainty : float or None
            é¢„æµ‹ä¸ç¡®å®šæ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        """
        if formulation.ndim == 1:
            formulation = formulation.reshape(1, -1)
        
        # å…³é”®ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹
        predicted_titer = self.regressor.predict(
            X=formulation,
            strategy=self.strategy
        )
        
        return predicted_titer[0], None  # ç®€åŒ–ç‰ˆä¸è¿”å›ä¸ç¡®å®šæ€§
    
    def generate_random_candidates(
        self,
        n_candidates: int = 1000,
        seed: Optional[int] = None
    ) -> np.ndarray:
        """åœ¨ç‰¹å¾ç©ºé—´ä¸­éšæœºç”Ÿæˆå€™é€‰é…æ–¹
        
        Parameters:
        -----------
        n_candidates : int
            ç”Ÿæˆçš„å€™é€‰æ•°é‡
        seed : int, optional
            éšæœºç§å­
        
        Returns:
        --------
        candidates : ndarray
            å½¢çŠ¶ (n_candidates, n_features)
        """
        if seed is not None:
            np.random.seed(seed)
        
        candidates = []
        
        for _ in range(n_candidates):
            formulation = []
            for feat in self.features:
                low, high = self.feature_bounds[feat]
                value = np.random.uniform(low, high)
                formulation.append(value)
            candidates.append(formulation)
        
        return np.array(candidates)
    
    def optimize_formulation_random_search(
        self,
        n_candidates: int = 10000,
        top_k: int = 10,
        seed: int = 42
    ) -> pd.DataFrame:
        """éšæœºæœç´¢æœ€ä¼˜é…æ–¹
        
        æ ¸å¿ƒæµç¨‹ï¼š
        1. éšæœºç”Ÿæˆå¤§é‡å€™é€‰é…æ–¹
        2. ç”¨æ¨¡å‹é¢„æµ‹æ¯ä¸ªé…æ–¹çš„titer
        3. è¿”å›é¢„æµ‹titeræœ€é«˜çš„top-k
        
        Parameters:
        -----------
        n_candidates : int
            éšæœºç”Ÿæˆçš„å€™é€‰æ•°é‡
        top_k : int
            è¿”å›top-kä¸ªæœ€ä¼˜é…æ–¹
        seed : int
            éšæœºç§å­
        
        Returns:
        --------
        recommendations : DataFrame
            åŒ…å«æ¨èé…æ–¹åŠé¢„æµ‹titer
        """
        print("=" * 70)
        print("Random Search Optimization for Unknown Formulations")
        print("=" * 70)
        print(f"Generating {n_candidates} random candidates...")
        
        # 1. ç”Ÿæˆå€™é€‰
        candidates = self.generate_random_candidates(n_candidates, seed)
        
        # 2. é¢„æµ‹æ‰€æœ‰å€™é€‰çš„titer
        print("Predicting titers for all candidates...")
        predicted_titers = self.regressor.predict(
            X=candidates,
            strategy=self.strategy
        )
        
        # 3. æ’åºå¹¶é€‰æ‹©top-k
        top_indices = np.argsort(predicted_titers)[::-1][:top_k]
        
        # 4. æ„å»ºæ¨èDataFrame
        recommendations = pd.DataFrame(
            candidates[top_indices],
            columns=self.features
        )
        recommendations['Predicted_Titer'] = predicted_titers[top_indices]
        recommendations['Rank'] = range(1, top_k + 1)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        cols = ['Rank', 'Predicted_Titer'] + self.features
        recommendations = recommendations[cols]
        
        print(f"\nâœ“ Found top-{top_k} formulations:")
        print(f"  Best predicted titer: {predicted_titers[top_indices[0]]:.2f}")
        print(f"  Worst in top-{top_k}:  {predicted_titers[top_indices[-1]]:.2f}")
        print()
        
        return recommendations
    
    def optimize_formulation_gradient_based(
        self,
        n_starts: int = 10,
        method: str = 'L-BFGS-B'
    ) -> pd.DataFrame:
        """åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼ˆé€‚ç”¨äºå¯å¾®åˆ†æ¨¡å‹å¦‚Ridgeï¼‰
        
        Parameters:
        -----------
        n_starts : int
            å¤šèµ·ç‚¹ä¼˜åŒ–çš„èµ·ç‚¹æ•°é‡
        method : str
            ä¼˜åŒ–æ–¹æ³•ï¼ˆ'L-BFGS-B', 'SLSQP'ç­‰ï¼‰
        
        Returns:
        --------
        recommendations : DataFrame
            ä¼˜åŒ–å¾—åˆ°çš„æœ€ä¼˜é…æ–¹
        """
        print("=" * 70)
        print("Gradient-Based Optimization for Unknown Formulations")
        print("=" * 70)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ¢¯åº¦ä¼˜åŒ–
        if self.regressor.regressor_type not in ['ridge', 'lasso', 'elastic']:
            print("âš ï¸  Gradient-based optimization works best with linear models")
            print("   Falling back to random search...")
            return self.optimize_formulation_random_search(n_candidates=10000, top_k=10)
        
        # å®šä¹‰ä¼˜åŒ–ç›®æ ‡ï¼ˆæœ€å¤§åŒ–titer = æœ€å°åŒ–è´Ÿtiterï¼‰
        def objective(x):
            titer, _ = self.predict_titer_for_new_formulation(x)
            return -titer  # æœ€å°åŒ–è´Ÿå€¼ = æœ€å¤§åŒ–æ­£å€¼
        
        # æå–boundsç”¨äºscipy
        bounds = [self.feature_bounds[feat] for feat in self.features]
        
        # å¤šèµ·ç‚¹ä¼˜åŒ–
        print(f"Running {n_starts} independent optimizations...")
        
        results = []
        
        for i in range(n_starts):
            # éšæœºåˆå§‹ç‚¹
            x0 = np.array([
                np.random.uniform(low, high) 
                for low, high in bounds
            ])
            
            # ä¼˜åŒ–
            res = minimize(
                objective,
                x0,
                method=method,
                bounds=bounds,
                options={'maxiter': 1000}
            )
            
            if res.success:
                results.append({
                    'formulation': res.x,
                    'predicted_titer': -res.fun  # è½¬å›æ­£å€¼
                })
        
        if not results:
            print("âŒ All optimizations failed!")
            return pd.DataFrame()
        
        # æ’åº
        results = sorted(results, key=lambda x: x['predicted_titer'], reverse=True)
        
        # æ„å»ºDataFrame
        recommendations = pd.DataFrame([r['formulation'] for r in results], columns=self.features)
        recommendations['Predicted_Titer'] = [r['predicted_titer'] for r in results]
        recommendations['Rank'] = range(1, len(results) + 1)
        
        cols = ['Rank', 'Predicted_Titer'] + self.features
        recommendations = recommendations[cols]
        
        print(f"\nâœ“ Found {len(results)} optimal formulations:")
        print(f"  Best predicted titer: {results[0]['predicted_titer']:.2f}")
        print()
        
        return recommendations
    
    def optimize_with_diversity(
        self,
        n_recommendations: int = 10,
        diversity_weight: float = 0.3,
        n_candidates: int = 5000
    ) -> pd.DataFrame:
        """åœ¨ä¼˜åŒ–titerçš„åŒæ—¶ä¿æŒé…æ–¹å¤šæ ·æ€§
        
        é¿å…æ¨èçš„é…æ–¹éƒ½é›†ä¸­åœ¨ç›¸ä¼¼åŒºåŸŸ
        
        Parameters:
        -----------
        n_recommendations : int
            æ¨èæ•°é‡
        diversity_weight : float
            å¤šæ ·æ€§æƒé‡ï¼ˆ0-1ï¼‰ï¼Œè¶Šé«˜è¶Šå¤šæ ·
        n_candidates : int
            å€™é€‰æ± å¤§å°
        
        Returns:
        --------
        recommendations : DataFrame
            å…¼é¡¾é«˜titerå’Œå¤šæ ·æ€§çš„æ¨è
        """
        print("=" * 70)
        print("Diversity-Aware Optimization")
        print("=" * 70)
        
        # 1. ç”Ÿæˆå€™é€‰å¹¶é¢„æµ‹
        candidates = self.generate_random_candidates(n_candidates)
        predicted_titers = self.regressor.predict(candidates, strategy=self.strategy)
        
        # 2. å½’ä¸€åŒ–titeråˆ°0-1
        titer_min, titer_max = predicted_titers.min(), predicted_titers.max()
        normalized_titers = (predicted_titers - titer_min) / (titer_max - titer_min + 1e-10)
        
        # 3. è´ªå¿ƒé€‰æ‹©ï¼šæ¯æ¬¡é€‰æ‹©ã€é«˜titer + è¿œç¦»å·²é€‰æ ·æœ¬ã€‘çš„é…æ–¹
        selected_indices = []
        
        # é€‰æ‹©é¢„æµ‹titeræœ€é«˜çš„ä½œä¸ºç¬¬ä¸€ä¸ª
        first_idx = np.argmax(predicted_titers)
        selected_indices.append(first_idx)
        
        for _ in range(n_recommendations - 1):
            # è®¡ç®—æ¯ä¸ªå€™é€‰åˆ°å·²é€‰æ ·æœ¬çš„æœ€å°è·ç¦»
            selected_formulations = candidates[selected_indices]
            
            min_distances = []
            for candidate in candidates:
                if len(selected_formulations) == 0:
                    min_distances.append(0)
                else:
                    distances = np.linalg.norm(selected_formulations - candidate, axis=1)
                    min_distances.append(distances.min())
            
            min_distances = np.array(min_distances)
            
            # å½’ä¸€åŒ–è·ç¦»
            if min_distances.max() > 0:
                normalized_distances = min_distances / min_distances.max()
            else:
                normalized_distances = np.zeros_like(min_distances)
            
            # ç»„åˆå¾—åˆ†ï¼š(1-w)*titer + w*diversity
            scores = (
                (1 - diversity_weight) * normalized_titers +
                diversity_weight * normalized_distances
            )
            
            # æ’é™¤å·²é€‰æ ·æœ¬
            scores[selected_indices] = -np.inf
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
            next_idx = np.argmax(scores)
            selected_indices.append(next_idx)
        
        # 4. æ„å»ºæ¨è
        recommendations = pd.DataFrame(
            candidates[selected_indices],
            columns=self.features
        )
        recommendations['Predicted_Titer'] = predicted_titers[selected_indices]
        recommendations['Rank'] = range(1, n_recommendations + 1)
        
        cols = ['Rank', 'Predicted_Titer'] + self.features
        recommendations = recommendations[cols]
        
        print(f"\nâœ“ Selected {n_recommendations} diverse formulations:")
        print(f"  Best titer: {predicted_titers[selected_indices[0]]:.2f}")
        print(f"  Avg titer:  {predicted_titers[selected_indices].mean():.2f}")
        
        # è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
        from sklearn.metrics.pairwise import euclidean_distances
        selected_formulations = candidates[selected_indices]
        pairwise_dist = euclidean_distances(selected_formulations)
        avg_distance = pairwise_dist[np.triu_indices_from(pairwise_dist, k=1)].mean()
        print(f"  Avg pairwise distance: {avg_distance:.4f}")
        print()
        
        return recommendations
    
    def validate_on_known_data(self):
        """åœ¨å·²çŸ¥æ•°æ®ä¸ŠéªŒè¯é¢„æµ‹èƒ½åŠ›
        
        ç”¨äºç¡®è®¤æ¨¡å‹åœ¨é¢„æµ‹æœªçŸ¥é…æ–¹å‰çš„å¯é æ€§
        """
        print("=" * 70)
        print("Model Validation on Known Data")
        print("=" * 70)
        
        # åœ¨ç›®æ ‡å…‹éš†çš„æµ‹è¯•é›†ä¸Š
        if self.analyzer.target_y_ is None:
            print("No target data available for validation")
            return
        
        # é¢„æµ‹æ‰€æœ‰ç›®æ ‡æ•°æ®
        predictions = self.regressor.predict(
            self.analyzer.target_X_,
            strategy=self.strategy
        )
        
        from sklearn.metrics import r2_score, mean_absolute_error
        
        r2 = r2_score(self.analyzer.target_y_, predictions)
        mae = mean_absolute_error(self.analyzer.target_y_, predictions)
        
        print(f"\nPrediction Performance on Target Clone:")
        print(f"  RÂ² Score: {r2:.4f}")
        print(f"  MAE: {mae:.2f}")
        
        if r2 > 0.6:
            print("  âœ… HIGH confidence - Model is reliable for new formulations")
        elif r2 > 0.4:
            print("  âœ“ MODERATE confidence - Predictions should be used cautiously")
        else:
            print("  âš ï¸  LOW confidence - Consider collecting more data")
        
        print()
        
        return {'r2': r2, 'mae': mae}


# ========== å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ ==========

def complete_workflow_example():
    """å®Œæ•´çš„å·¥ä½œæµï¼šä»å»ºæ¨¡åˆ°ä¼˜åŒ–æœªçŸ¥é…æ–¹"""
    
    # ... å‰é¢çš„analyzerå’Œregressorè®­ç»ƒä»£ç  ...
    # ï¼ˆå‡è®¾å·²ç»å®Œæˆï¼Œå¾—åˆ°äº†best_strategyå’ŒéªŒè¯çš„RÂ²ï¼‰
    
    print("\n" + "=" * 80)
    print("PHASE 1: Model Training & Validation (COMPLETED)")
    print("=" * 80)
    print("âœ“ Trained on: Source (50) + Target Train (10)")
    print("âœ“ Validated on: Target Test (26)")
    print("âœ“ Best Strategy: domain_adapted")
    print("âœ“ Test RÂ²: 0.6543")
    print()
    
    # ========== ç°åœ¨å¼€å§‹ä½ çœŸæ­£å…³å¿ƒçš„éƒ¨åˆ† ==========
    
    print("=" * 80)
    print("PHASE 2: Optimize UNKNOWN Formulations (NEW!)")
    print("=" * 80)
    print()
    
    # å®šä¹‰ç‰¹å¾bounds
    feature_bounds = {f'C{i}': (0, 1) for i in range(1, 87)}
    
    # åˆ›å»ºçœŸæ­£çš„ä¼˜åŒ–å™¨
    optimizer = TrueEmbeddingGuidedOptimizer(
        analyzer=analyzer,
        trained_regressor=best_emb_regressor,
        best_strategy='domain_adapted',
        feature_bounds=feature_bounds
    )
    
    # éªŒè¯æ¨¡å‹å¯é æ€§
    optimizer.validate_on_known_data()
    
    # ========== ç­–ç•¥1ï¼šéšæœºæœç´¢ ==========
    print("\n" + "â”€" * 80)
    print("Strategy 1: Random Search")
    print("â”€" * 80)
    
    recommendations_random = optimizer.optimize_formulation_random_search(
        n_candidates=10000,
        top_k=10,
        seed=42
    )
    
    print("Top 5 Recommendations:")
    print(recommendations_random.head())
    
    # ä¿å­˜
    recommendations_random.to_csv('recommended_formulations_random.csv', index=False)
    print("âœ“ Saved to recommended_formulations_random.csv\n")
    
    # ========== ç­–ç•¥2ï¼šæ¢¯åº¦ä¼˜åŒ–ï¼ˆå¦‚æœç”¨Ridgeï¼‰ ==========
    print("\n" + "â”€" * 80)
    print("Strategy 2: Gradient-Based Optimization")
    print("â”€" * 80)
    
    recommendations_gradient = optimizer.optimize_formulation_gradient_based(
        n_starts=20,
        method='L-BFGS-B'
    )
    
    print("Top 5 Recommendations:")
    print(recommendations_gradient.head())
    
    recommendations_gradient.to_csv('recommended_formulations_gradient.csv', index=False)
    print("âœ“ Saved to recommended_formulations_gradient.csv\n")
    
    # ========== ç­–ç•¥3ï¼šå¤šæ ·æ€§ä¼˜åŒ– ==========
    print("\n" + "â”€" * 80)
    print("Strategy 3: Diversity-Aware Optimization")
    print("â”€" * 80)
    
    recommendations_diverse = optimizer.optimize_with_diversity(
        n_recommendations=10,
        diversity_weight=0.3,
        n_candidates=5000
    )
    
    print("All Diverse Recommendations:")
    print(recommendations_diverse)
    
    recommendations_diverse.to_csv('recommended_formulations_diverse.csv', index=False)
    print("âœ“ Saved to recommended_formulations_diverse.csv\n")
    
    # ========== å®é™…åº”ç”¨æµç¨‹ ==========
    print("\n" + "=" * 80)
    print("NEXT STEPS: Experimental Validation")
    print("=" * 80)
    print("""
    1. Review the recommended formulations above
    2. Select top 5-10 to actually test in the lab
    3. Run experiments on Clone B with these NEW formulations
    4. Measure actual titers
    5. Compare with predictions:
       - If actual titers match predictions â†’ Model is working! âœ“
       - If not â†’ May need more training data or different strategy
    6. Iterate: Add new data to training set and retrain
    """)
```

---

## ğŸ“Š å®Œæ•´å¯¹æ¯”æ€»ç»“

| ç»´åº¦ | ä½ çš„éœ€æ±‚ | æˆ‘åŸæ¥çš„`EmbeddingGuidedOptimizer` | æ–°çš„`TrueEmbeddingGuidedOptimizer` |
|------|----------|-----------------------------------|-----------------------------------|
| **è¾“å…¥** | å·²éªŒè¯çš„å›å½’æ¨¡å‹ | âœ“ ç›¸åŒ | âœ“ ç›¸åŒ |
| **å€™é€‰æ¥æº** | ç”Ÿæˆ**å…¨æ–°é…æ–¹** | âŒ ä»å·²æœ‰36æ¡ä¸­ç­›é€‰ | âœ… ç”Ÿæˆå…¨æ–°é…æ–¹ |
| **é¢„æµ‹èƒ½åŠ›** | é¢„æµ‹æœªçŸ¥é…æ–¹çš„titer | âŒ åªèƒ½è¯„ä¼°å·²æœ‰é…æ–¹ | âœ… å¯ä»¥é¢„æµ‹ä»»æ„é…æ–¹ |
| **ä¼˜åŒ–ç›®æ ‡** | æ‰¾åˆ°æœ€ä¼˜**æœªçŸ¥**é…æ–¹ | âŒ æ‰¾åˆ°å·²æœ‰ä¸­æœ€ä¼˜ | âœ… æ‰¾åˆ°å…¨å±€æœ€ä¼˜ |
| **å®é™…ç”¨é€”** | æŒ‡å¯¼ä¸‹ä¸€è½®å®éªŒ | æœ‰é™ï¼ˆå·²æµ‹è¿‡çš„é…æ–¹ï¼‰ | âœ… çœŸæ­£æŒ‡å¯¼æ–°å®éªŒ |

---

## ğŸ¯ ä½ åº”è¯¥æ€ä¹ˆç”¨ï¼Ÿ

### å®Œæ•´Pipeline

```python
# ========== ç¬¬1æ­¥ï¼šå»ºæ¨¡ï¼ˆä½¿ç”¨EmbeddingSpaceRegressorï¼‰==========
regressor = EmbeddingSpaceRegressor(analyzer, regressor_type='ridge')
results = regressor.fit_all_strategies(
    target_train_indices=train_indices,  # 10æ¡
    target_test_indices=test_indices      # 26æ¡
)

# ========== ç¬¬2æ­¥ï¼šéªŒè¯æ€§èƒ½ ==========
best_strategy = 'domain_adapted'
test_r2 = results[best_strategy]['test_r2']

if test_r2 > 0.6:
    print("âœ… æ¨¡å‹å¯é ï¼Œå¯ä»¥ç”¨äºé¢„æµ‹æœªçŸ¥é…æ–¹")
    
    # ========== ç¬¬3æ­¥ï¼šä¼˜åŒ–æœªçŸ¥é…æ–¹ï¼ˆæ–°å®ç°ï¼‰==========
    optimizer = TrueEmbeddingGuidedOptimizer(
        analyzer=analyzer,
        trained_regressor=regressor,
        best_strategy=best_strategy,
        feature_bounds=bounds
    )
    
    # ç”Ÿæˆæ¨è
    recommendations = optimizer.optimize_formulation_random_search(
        n_candidates=10000,
        top_k=10
    )
    
    # ========== ç¬¬4æ­¥ï¼šå®éªŒéªŒè¯ ==========
    # æ‹¿ç€recommendationså»å®é™…æµ‹è¯•
    # æ¯”å¦‚æµ‹è¯•top 5ä¸ªé…æ–¹
    
else:
    print("âš ï¸ æ¨¡å‹æ€§èƒ½ä¸è¶³ï¼Œå»ºè®®æ”¶é›†æ›´å¤šç›®æ ‡æ•°æ®")
```

---

## ğŸ”§ æˆ‘éœ€è¦ä¸ºä½ æ›´æ–°ä»£ç å—ï¼Ÿ

æˆ‘å¯ä»¥ï¼š
1. ä¿ç•™åŸæ¥çš„`EmbeddingSpaceRegressor`ï¼ˆå·²ç»ç¬¦åˆä½ éœ€æ±‚ï¼‰
2. **æ›¿æ¢**`EmbeddingGuidedOptimizer`ä¸ºä¸Šé¢çš„`TrueEmbeddingGuidedOptimizer`
3. æä¾›å®Œæ•´çš„ç«¯åˆ°ç«¯ç¤ºä¾‹

éœ€è¦æˆ‘è¿™æ ·åšå—ï¼Ÿè¿˜æ˜¯ä½ å·²ç»ç†è§£é€»è¾‘ï¼Œå¯ä»¥è‡ªå·±æ•´åˆï¼Ÿ